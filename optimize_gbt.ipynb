{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunathanael/Optimize/blob/nate/optimize_gbt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('./'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "papermill": {
          "duration": 0.048708,
          "end_time": "2023-10-29T20:12:29.326306",
          "exception": false,
          "start_time": "2023-10-29T20:12:29.277598",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-11-04T06:45:02.825502Z",
          "iopub.execute_input": "2023-11-04T06:45:02.825887Z",
          "iopub.status.idle": "2023-11-04T06:45:02.851196Z",
          "shell.execute_reply.started": "2023-11-04T06:45:02.825857Z",
          "shell.execute_reply": "2023-11-04T06:45:02.850250Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp98HUY8reL1",
        "outputId": "d0f9d2b4-e757-4267-c2a6-2cb9a9a8e6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./.config/.last_update_check.json\n",
            "./.config/active_config\n",
            "./.config/gce\n",
            "./.config/config_sentinel\n",
            "./.config/.last_opt_in_prompt.yaml\n",
            "./.config/default_configs.db\n",
            "./.config/.last_survey_prompt.yaml\n",
            "./.config/configurations/config_default\n",
            "./.config/logs/2023.09.21/13.49.10.454054.log\n",
            "./.config/logs/2023.09.21/13.49.01.549218.log\n",
            "./.config/logs/2023.09.21/13.48.45.393225.log\n",
            "./.config/logs/2023.09.21/13.48.20.358335.log\n",
            "./.config/logs/2023.09.21/13.48.54.318162.log\n",
            "./.config/logs/2023.09.21/13.49.11.211030.log\n",
            "./sample_data/README.md\n",
            "./sample_data/anscombe.json\n",
            "./sample_data/mnist_train_small.csv\n",
            "./sample_data/california_housing_test.csv\n",
            "./sample_data/california_housing_train.csv\n",
            "./sample_data/mnist_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lib import\n",
        "import numpy as np\n",
        "from numpy import inf\n",
        "import pandas as pd\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "TRAINING = True"
      ],
      "metadata": {
        "papermill": {
          "duration": 13038.093127,
          "end_time": "2023-10-29T23:49:47.421754",
          "exception": false,
          "start_time": "2023-10-29T20:12:29.328627",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-11-04T06:45:02.852710Z",
          "iopub.execute_input": "2023-11-04T06:45:02.853037Z",
          "iopub.status.idle": "2023-11-04T06:45:03.368653Z",
          "shell.execute_reply.started": "2023-11-04T06:45:02.853010Z",
          "shell.execute_reply": "2023-11-04T06:45:03.367869Z"
        },
        "trusted": true,
        "id": "3gx6hloQreL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_features(df):\n",
        "    features = ['seconds_in_bucket', 'imbalance_buy_sell_flag',\n",
        "               'imbalance_size', 'matched_size', 'bid_size', 'ask_size',\n",
        "                'reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap',\n",
        "                'imb_s1', 'imb_s2'\n",
        "               ]\n",
        "\n",
        "    df['imb_s1'] = df.eval('(bid_size-ask_size)/(bid_size+ask_size)')\n",
        "    df['imb_s2'] = df.eval('(imbalance_size-matched_size)/(matched_size+imbalance_size)')\n",
        "\n",
        "    prices = ['reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n",
        "\n",
        "    for i,a in enumerate(prices):\n",
        "        for j,b in enumerate(prices):\n",
        "            if i>j:\n",
        "                df[f'{a}_{b}_imb'] = df.eval(f'({a}-{b})/({a}+{b})')\n",
        "                features.append(f'{a}_{b}_imb')\n",
        "\n",
        "    for i,a in enumerate(prices):\n",
        "        for j,b in enumerate(prices):\n",
        "            for k,c in enumerate(prices):\n",
        "                if i>j and j>k:\n",
        "                    max_ = df[[a,b,c]].max(axis=1)\n",
        "                    min_ = df[[a,b,c]].min(axis=1)\n",
        "                    mid_ = df[[a,b,c]].sum(axis=1)-min_-max_\n",
        "\n",
        "                    df[f'{a}_{b}_{c}_imb2'] = (max_-mid_)/(mid_-min_)\n",
        "                    features.append(f'{a}_{b}_{c}_imb2')\n",
        "\n",
        "    return df[features]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T06:45:03.369930Z",
          "iopub.execute_input": "2023-11-04T06:45:03.370305Z",
          "iopub.status.idle": "2023-11-04T06:45:03.381827Z",
          "shell.execute_reply.started": "2023-11-04T06:45:03.370271Z",
          "shell.execute_reply": "2023-11-04T06:45:03.380909Z"
        },
        "trusted": true,
        "id": "olcae27lreL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAINING:\n",
        "    # load dataset\n",
        "    df_train = pd.read_csv('./train.csv')\n",
        "    df_features = generate_features(df_train)\n",
        "\n",
        "    # data is defaulty normalized\n",
        "    X = np.float32(df_features.values)\n",
        "    Y = np.float32(df_train['target'].values)\n",
        "\n",
        "    X = X[np.isfinite(Y)]\n",
        "    Y = Y[np.isfinite(Y)]\n",
        "\n",
        "    index = np.arange(len(X)) #array for indexing\n",
        "\n",
        "    max_value = np.finfo(X.dtype).max #max float value allowed by numpy, perhaps lower? to prevent NAN during fit\n",
        "    min_value = np.finfo(X.dtype).min\n",
        "#     X[X==inf] = max_value\n",
        "#     X[~np.isfinite(X)] = min_value\n",
        "    X[X==inf] = 3.40282e+14\n",
        "    X[~np.isfinite(X)] = -3.40282e+14"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T06:45:50.497088Z",
          "iopub.execute_input": "2023-11-04T06:45:50.497478Z",
          "iopub.status.idle": "2023-11-04T06:47:39.343418Z",
          "shell.execute_reply.started": "2023-11-04T06:45:50.497447Z",
          "shell.execute_reply": "2023-11-04T06:47:39.341907Z"
        },
        "trusted": true,
        "id": "CRmFb8ZkreL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "\n",
        "# test to train ratio\n",
        "N_fold = 5\n",
        "\n",
        "def train(tuned_model):\n",
        "    if TRAINING:\n",
        "          tuned_model.fit(X[index%N_fold!=0][0:40000, : ], Y[index%N_fold!=0][0:40000], verbose=2)\n",
        "\n",
        "#         model = tfdf.keras.GradientBoostedTreesModel()\n",
        "#         model.fit(X[index%N_fold!=i], Y[index%N_fold!=i],\n",
        "#                     eval_set=[(X[index%N_fold==i], Y[index%N_fold==i])],\n",
        "#                     verbose=10,\n",
        "#                     early_stopping_rounds=100\n",
        "#                     )\n",
        "\n",
        "tuner = tfdf.tuner.RandomSearch(num_trials=100, use_predefined_hps=True)\n",
        "tuned_model = tfdf.keras.GradientBoostedTreesModel(task = tfdf.keras.Task.REGRESSION, tuner=tuner, verbose=1)\n",
        "train(tuned_model)\n",
        "tuned_model.compile(metrics=[\"mae\"])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T06:47:43.792340Z",
          "iopub.execute_input": "2023-11-04T06:47:43.793097Z"
        },
        "trusted": true,
        "id": "SlEprcLnreL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272f81fd-4156-4fe7-ebc7-dcb177b87830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpgcwqak7x as temporary training directory\n",
            "Reading training dataset...\n",
            "Training tensor examples:\n",
            "Features: Tensor(\"data:0\", shape=(32, 49), dtype=float32)\n",
            "Label: Tensor(\"data_1:0\", shape=(32,), dtype=float32)\n",
            "Weights: None\n",
            "Normalized tensor features:\n",
            " {'data:0.0': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice:0' shape=(32,) dtype=float32>), 'data:0.1': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_1:0' shape=(32,) dtype=float32>), 'data:0.2': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_2:0' shape=(32,) dtype=float32>), 'data:0.3': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_3:0' shape=(32,) dtype=float32>), 'data:0.4': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_4:0' shape=(32,) dtype=float32>), 'data:0.5': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_5:0' shape=(32,) dtype=float32>), 'data:0.6': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_6:0' shape=(32,) dtype=float32>), 'data:0.7': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_7:0' shape=(32,) dtype=float32>), 'data:0.8': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_8:0' shape=(32,) dtype=float32>), 'data:0.9': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_9:0' shape=(32,) dtype=float32>), 'data:0.10': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_10:0' shape=(32,) dtype=float32>), 'data:0.11': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_11:0' shape=(32,) dtype=float32>), 'data:0.12': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_12:0' shape=(32,) dtype=float32>), 'data:0.13': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_13:0' shape=(32,) dtype=float32>), 'data:0.14': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_14:0' shape=(32,) dtype=float32>), 'data:0.15': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_15:0' shape=(32,) dtype=float32>), 'data:0.16': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_16:0' shape=(32,) dtype=float32>), 'data:0.17': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_17:0' shape=(32,) dtype=float32>), 'data:0.18': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_18:0' shape=(32,) dtype=float32>), 'data:0.19': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_19:0' shape=(32,) dtype=float32>), 'data:0.20': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_20:0' shape=(32,) dtype=float32>), 'data:0.21': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_21:0' shape=(32,) dtype=float32>), 'data:0.22': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_22:0' shape=(32,) dtype=float32>), 'data:0.23': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_23:0' shape=(32,) dtype=float32>), 'data:0.24': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_24:0' shape=(32,) dtype=float32>), 'data:0.25': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_25:0' shape=(32,) dtype=float32>), 'data:0.26': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_26:0' shape=(32,) dtype=float32>), 'data:0.27': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_27:0' shape=(32,) dtype=float32>), 'data:0.28': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_28:0' shape=(32,) dtype=float32>), 'data:0.29': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_29:0' shape=(32,) dtype=float32>), 'data:0.30': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_30:0' shape=(32,) dtype=float32>), 'data:0.31': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_31:0' shape=(32,) dtype=float32>), 'data:0.32': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_32:0' shape=(32,) dtype=float32>), 'data:0.33': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_33:0' shape=(32,) dtype=float32>), 'data:0.34': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_34:0' shape=(32,) dtype=float32>), 'data:0.35': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_35:0' shape=(32,) dtype=float32>), 'data:0.36': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_36:0' shape=(32,) dtype=float32>), 'data:0.37': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_37:0' shape=(32,) dtype=float32>), 'data:0.38': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_38:0' shape=(32,) dtype=float32>), 'data:0.39': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_39:0' shape=(32,) dtype=float32>), 'data:0.40': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_40:0' shape=(32,) dtype=float32>), 'data:0.41': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_41:0' shape=(32,) dtype=float32>), 'data:0.42': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_42:0' shape=(32,) dtype=float32>), 'data:0.43': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_43:0' shape=(32,) dtype=float32>), 'data:0.44': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_44:0' shape=(32,) dtype=float32>), 'data:0.45': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_45:0' shape=(32,) dtype=float32>), 'data:0.46': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_46:0' shape=(32,) dtype=float32>), 'data:0.47': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_47:0' shape=(32,) dtype=float32>), 'data:0.48': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_48:0' shape=(32,) dtype=float32>)}\n",
            "Training dataset read in 0:00:12.131574. Found 40000 examples.\n",
            "Training model...\n",
            "Standard output detected as not visible to the user e.g. running in a notebook. Creating a training log redirection. If training gets stuck, try calling tfdf.keras.set_training_logs_redirection(False).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO 23-11-04 17:31:58.4746 UTC kernel.cc:773] Start Yggdrasil model training\n",
            "[INFO 23-11-04 17:31:58.4746 UTC kernel.cc:774] Collect training examples\n",
            "[INFO 23-11-04 17:31:58.4746 UTC kernel.cc:787] Dataspec guide:\n",
            "column_guides {\n",
            "  column_name_pattern: \"^__LABEL$\"\n",
            "  type: NUMERICAL\n",
            "}\n",
            "default_column_guide {\n",
            "  categorial {\n",
            "    max_vocab_count: 2000\n",
            "  }\n",
            "  discretized_numerical {\n",
            "    maximum_num_bins: 255\n",
            "  }\n",
            "}\n",
            "ignore_columns_without_guides: false\n",
            "detect_numerical_as_discretized_numerical: false\n",
            "\n",
            "[INFO 23-11-04 17:31:58.4763 UTC kernel.cc:393] Number of batches: 1250\n",
            "[INFO 23-11-04 17:31:58.4764 UTC kernel.cc:394] Number of examples: 40000\n",
            "[INFO 23-11-04 17:31:58.4874 UTC kernel.cc:794] Training dataset:\n",
            "Number of records: 40000\n",
            "Number of columns: 50\n",
            "\n",
            "Number of columns by type:\n",
            "\tNUMERICAL: 50 (100%)\n",
            "\n",
            "Columns:\n",
            "\n",
            "NUMERICAL: 50 (100%)\n",
            "\t0: \"__LABEL\" NUMERICAL mean:-0.257429 min:-130.83 max:98.8495 sd:8.6433\n",
            "\t1: \"data:0.0\" NUMERICAL mean:259.308 min:0 max:540 sd:155.18\n",
            "\t2: \"data:0.1\" NUMERICAL mean:0.116325 min:-1 max:1 sd:0.849584\n",
            "\t3: \"data:0.10\" NUMERICAL mean:0.999856 min:0.983962 max:1.01416 sd:0.00178254\n",
            "\t4: \"data:0.11\" NUMERICAL mean:1.00009 min:0.984214 max:1.01493 sd:0.00179244\n",
            "\t5: \"data:0.12\" NUMERICAL mean:-0.00602022 min:-0.99975 max:0.999337 sd:0.588747\n",
            "\t6: \"data:0.13\" NUMERICAL mean:-0.709815 min:-1 max:0.804611 sd:0.353707\n",
            "\t7: \"data:0.14\" NUMERICAL mean:-1.97993e+14 min:-3.40282e+14 max:0.0503286 sd:1.67846e+14\n",
            "\t8: \"data:0.15\" NUMERICAL mean:-1.95186e+14 min:-3.40282e+14 max:0.0475426 sd:1.68288e+14\n",
            "\t9: \"data:0.16\" NUMERICAL mean:-1.97993e+14 min:-3.40282e+14 max:0.999145 sd:1.67846e+14\n",
            "\t10: \"data:0.17\" NUMERICAL mean:0.000109012 min:-0.00167711 max:0.00351385 sd:0.000173477\n",
            "\t11: \"data:0.18\" NUMERICAL mean:-1.97993e+14 min:-3.40282e+14 max:0.999175 sd:1.67846e+14\n",
            "\t12: \"data:0.19\" NUMERICAL mean:-1.95186e+14 min:-3.40282e+14 max:0.0471637 sd:1.68288e+14\n",
            "\t13: \"data:0.2\" NUMERICAL mean:3.82438e+06 min:0 max:3.71957e+08 sd:1.26551e+07\n",
            "\t14: \"data:0.20\" NUMERICAL mean:-0.000125945 min:-0.00404984 max:0.00184788 sd:0.000191448\n",
            "\t15: \"data:0.21\" NUMERICAL mean:-1.97993e+14 min:-3.40282e+14 max:0.999175 sd:1.67846e+14\n",
            "\t16: \"data:0.22\" NUMERICAL mean:-1.95186e+14 min:-3.40282e+14 max:0.0469662 sd:1.68288e+14\n",
            "\t17: \"data:0.23\" NUMERICAL mean:-0.000234957 min:-0.00428707 max:-7.0026e-06 sd:0.000247559\n",
            "\t18: \"data:0.24\" NUMERICAL mean:-9.69941e-06 min:-0.00239369 max:0.00212584 sd:0.000164285\n",
            "\t19: \"data:0.25\" NUMERICAL mean:-1.97993e+14 min:-3.40282e+14 max:0.999175 sd:1.67846e+14\n",
            "\t20: \"data:0.26\" NUMERICAL mean:-1.95186e+14 min:-3.40282e+14 max:0.0471198 sd:1.68288e+14\n",
            "\t21: \"data:0.27\" NUMERICAL mean:-0.000118711 min:-0.00310919 max:0 sd:0.000167504\n",
            "\t22: \"data:0.28\" NUMERICAL mean:0.000116246 min:0 max:0.00311167 sd:0.000159493\n",
            "\t23: \"data:0.29\" NUMERICAL mean:-2.01295e+13 min:-3.94677e+14 max:4.2196e+14 sd:8.75794e+13\n",
            "\t24: \"data:0.3\" NUMERICAL mean:2.55734e+07 min:29435.3 max:6.3552e+08 sd:5.60655e+07\n",
            "\t25: \"data:0.30\" NUMERICAL mean:1.29002e+13 min:-8.97423e+14 max:8.01659e+14 sd:9.75435e+13\n",
            "\t26: \"data:0.31\" NUMERICAL mean:1.31115e+13 min:-3.40282e+14 max:3.40282e+14 sd:9.81206e+13\n",
            "\t27: \"data:0.32\" NUMERICAL mean:6.07373e+12 min:-4.05954e+14 max:3.97497e+14 sd:8.31698e+13\n",
            "\t28: \"data:0.33\" NUMERICAL mean:-4.65354e+12 min:-3.40282e+14 max:3.40282e+14 sd:4.33256e+13\n",
            "\t29: \"data:0.34\" NUMERICAL mean:-4.95449e+12 min:-3.40282e+14 max:3.73015e+14 sd:4.68306e+13\n",
            "\t30: \"data:0.35\" NUMERICAL mean:-3.94445e+12 min:-4.20285e+14 max:3.93263e+14 sd:4.69037e+13\n",
            "\t31: \"data:0.36\" NUMERICAL mean:2.42858e+13 min:-2.71027e+13 max:3.40282e+14 sd:8.76154e+13\n",
            "\t32: \"data:0.37\" NUMERICAL mean:4.24831e+12 min:-1.94375e+13 max:3.40282e+14 sd:3.78098e+13\n",
            "\t33: \"data:0.38\" NUMERICAL mean:4.90029e+12 min:-2.14461e+13 max:3.40282e+14 sd:4.05428e+13\n",
            "\t34: \"data:0.39\" NUMERICAL mean:7.46487e+12 min:-3.40282e+14 max:3.40282e+14 sd:5.04984e+13\n",
            "\t35: \"data:0.4\" NUMERICAL mean:56575.2 min:34.85 max:4.23874e+06 sd:96520.1\n",
            "\t36: \"data:0.40\" NUMERICAL mean:8.02535e+12 min:-3.40282e+14 max:3.40282e+14 sd:5.2243e+13\n",
            "\t37: \"data:0.41\" NUMERICAL mean:8.48392e+12 min:-3.96479e+14 max:3.90138e+14 sd:5.38285e+13\n",
            "\t38: \"data:0.42\" NUMERICAL mean:1.36361e+11 min:-3.40282e+14 max:3.40282e+14 sd:1.25022e+13\n",
            "\t39: \"data:0.43\" NUMERICAL mean:1.17707e+11 min:-3.40282e+14 max:3.40282e+14 sd:7.61594e+12\n",
            "\t40: \"data:0.44\" NUMERICAL mean:8.58524e+10 min:-3.40282e+14 max:3.40282e+14 sd:6.8107e+12\n",
            "\t41: \"data:0.45\" NUMERICAL mean:2.46458e+13 min:-3.40282e+14 max:3.40282e+14 sd:8.87234e+13\n",
            "\t42: \"data:0.46\" NUMERICAL mean:4.19415e+12 min:-3.40282e+14 max:3.40282e+14 sd:3.78595e+13\n",
            "\t43: \"data:0.47\" NUMERICAL mean:4.87029e+12 min:-3.40282e+14 max:3.40282e+14 sd:4.06884e+13\n",
            "\t44: \"data:0.48\" NUMERICAL mean:3.40284e+11 min:-3.06695e+12 max:3.40282e+14 sd:1.07554e+13\n",
            "\t45: \"data:0.5\" NUMERICAL mean:58801.6 min:37.4 max:7.13542e+06 sd:121082\n",
            "\t46: \"data:0.6\" NUMERICAL mean:1.00011 min:0.98485 max:1.01549 sd:0.00181534\n",
            "\t47: \"data:0.7\" NUMERICAL mean:-1.97993e+14 min:-3.40282e+14 max:1.11186 sd:1.67846e+14\n",
            "\t48: \"data:0.8\" NUMERICAL mean:-1.95186e+14 min:-3.40282e+14 max:1.10251 sd:1.68288e+14\n",
            "\t49: \"data:0.9\" NUMERICAL mean:1.00033 min:0.985223 max:1.01579 sd:0.00180873\n",
            "\n",
            "Terminology:\n",
            "\tnas: Number of non-available (i.e. missing) values.\n",
            "\tood: Out of dictionary.\n",
            "\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n",
            "\ttokenized: The attribute value is obtained through tokenization.\n",
            "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
            "\tvocab-size: Number of unique values.\n",
            "\n",
            "[INFO 23-11-04 17:31:58.4877 UTC kernel.cc:810] Configure learner\n",
            "[WARNING 23-11-04 17:31:58.4879 UTC gradient_boosted_trees.cc:1830] \"goss_alpha\" set but \"sampling_method\" not equal to \"GOSS\".\n",
            "[WARNING 23-11-04 17:31:58.4879 UTC gradient_boosted_trees.cc:1841] \"goss_beta\" set but \"sampling_method\" not equal to \"GOSS\".\n",
            "[WARNING 23-11-04 17:31:58.4879 UTC gradient_boosted_trees.cc:1855] \"selective_gradient_boosting_ratio\" set but \"sampling_method\" not equal to \"SELGB\".\n",
            "[INFO 23-11-04 17:31:58.4880 UTC kernel.cc:824] Training config:\n",
            "learner: \"HYPERPARAMETER_OPTIMIZER\"\n",
            "features: \"^data:0\\\\.0$\"\n",
            "features: \"^data:0\\\\.1$\"\n",
            "features: \"^data:0\\\\.10$\"\n",
            "features: \"^data:0\\\\.11$\"\n",
            "features: \"^data:0\\\\.12$\"\n",
            "features: \"^data:0\\\\.13$\"\n",
            "features: \"^data:0\\\\.14$\"\n",
            "features: \"^data:0\\\\.15$\"\n",
            "features: \"^data:0\\\\.16$\"\n",
            "features: \"^data:0\\\\.17$\"\n",
            "features: \"^data:0\\\\.18$\"\n",
            "features: \"^data:0\\\\.19$\"\n",
            "features: \"^data:0\\\\.2$\"\n",
            "features: \"^data:0\\\\.20$\"\n",
            "features: \"^data:0\\\\.21$\"\n",
            "features: \"^data:0\\\\.22$\"\n",
            "features: \"^data:0\\\\.23$\"\n",
            "features: \"^data:0\\\\.24$\"\n",
            "features: \"^data:0\\\\.25$\"\n",
            "features: \"^data:0\\\\.26$\"\n",
            "features: \"^data:0\\\\.27$\"\n",
            "features: \"^data:0\\\\.28$\"\n",
            "features: \"^data:0\\\\.29$\"\n",
            "features: \"^data:0\\\\.3$\"\n",
            "features: \"^data:0\\\\.30$\"\n",
            "features: \"^data:0\\\\.31$\"\n",
            "features: \"^data:0\\\\.32$\"\n",
            "features: \"^data:0\\\\.33$\"\n",
            "features: \"^data:0\\\\.34$\"\n",
            "features: \"^data:0\\\\.35$\"\n",
            "features: \"^data:0\\\\.36$\"\n",
            "features: \"^data:0\\\\.37$\"\n",
            "features: \"^data:0\\\\.38$\"\n",
            "features: \"^data:0\\\\.39$\"\n",
            "features: \"^data:0\\\\.4$\"\n",
            "features: \"^data:0\\\\.40$\"\n",
            "features: \"^data:0\\\\.41$\"\n",
            "features: \"^data:0\\\\.42$\"\n",
            "features: \"^data:0\\\\.43$\"\n",
            "features: \"^data:0\\\\.44$\"\n",
            "features: \"^data:0\\\\.45$\"\n",
            "features: \"^data:0\\\\.46$\"\n",
            "features: \"^data:0\\\\.47$\"\n",
            "features: \"^data:0\\\\.48$\"\n",
            "features: \"^data:0\\\\.5$\"\n",
            "features: \"^data:0\\\\.6$\"\n",
            "features: \"^data:0\\\\.7$\"\n",
            "features: \"^data:0\\\\.8$\"\n",
            "features: \"^data:0\\\\.9$\"\n",
            "label: \"^__LABEL$\"\n",
            "task: REGRESSION\n",
            "metadata {\n",
            "  framework: \"TF Keras\"\n",
            "}\n",
            "[yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.hyperparameters_optimizer_config] {\n",
            "  base_learner {\n",
            "    learner: \"GRADIENT_BOOSTED_TREES\"\n",
            "    features: \"^data:0\\\\.0$\"\n",
            "    features: \"^data:0\\\\.1$\"\n",
            "    features: \"^data:0\\\\.10$\"\n",
            "    features: \"^data:0\\\\.11$\"\n",
            "    features: \"^data:0\\\\.12$\"\n",
            "    features: \"^data:0\\\\.13$\"\n",
            "    features: \"^data:0\\\\.14$\"\n",
            "    features: \"^data:0\\\\.15$\"\n",
            "    features: \"^data:0\\\\.16$\"\n",
            "    features: \"^data:0\\\\.17$\"\n",
            "    features: \"^data:0\\\\.18$\"\n",
            "    features: \"^data:0\\\\.19$\"\n",
            "    features: \"^data:0\\\\.2$\"\n",
            "    features: \"^data:0\\\\.20$\"\n",
            "    features: \"^data:0\\\\.21$\"\n",
            "    features: \"^data:0\\\\.22$\"\n",
            "    features: \"^data:0\\\\.23$\"\n",
            "    features: \"^data:0\\\\.24$\"\n",
            "    features: \"^data:0\\\\.25$\"\n",
            "    features: \"^data:0\\\\.26$\"\n",
            "    features: \"^data:0\\\\.27$\"\n",
            "    features: \"^data:0\\\\.28$\"\n",
            "    features: \"^data:0\\\\.29$\"\n",
            "    features: \"^data:0\\\\.3$\"\n",
            "    features: \"^data:0\\\\.30$\"\n",
            "    features: \"^data:0\\\\.31$\"\n",
            "    features: \"^data:0\\\\.32$\"\n",
            "    features: \"^data:0\\\\.33$\"\n",
            "    features: \"^data:0\\\\.34$\"\n",
            "    features: \"^data:0\\\\.35$\"\n",
            "    features: \"^data:0\\\\.36$\"\n",
            "    features: \"^data:0\\\\.37$\"\n",
            "    features: \"^data:0\\\\.38$\"\n",
            "    features: \"^data:0\\\\.39$\"\n",
            "    features: \"^data:0\\\\.4$\"\n",
            "    features: \"^data:0\\\\.40$\"\n",
            "    features: \"^data:0\\\\.41$\"\n",
            "    features: \"^data:0\\\\.42$\"\n",
            "    features: \"^data:0\\\\.43$\"\n",
            "    features: \"^data:0\\\\.44$\"\n",
            "    features: \"^data:0\\\\.45$\"\n",
            "    features: \"^data:0\\\\.46$\"\n",
            "    features: \"^data:0\\\\.47$\"\n",
            "    features: \"^data:0\\\\.48$\"\n",
            "    features: \"^data:0\\\\.5$\"\n",
            "    features: \"^data:0\\\\.6$\"\n",
            "    features: \"^data:0\\\\.7$\"\n",
            "    features: \"^data:0\\\\.8$\"\n",
            "    features: \"^data:0\\\\.9$\"\n",
            "    label: \"^__LABEL$\"\n",
            "    task: REGRESSION\n",
            "    random_seed: 123456\n",
            "    pure_serving_model: false\n",
            "    [yggdrasil_decision_forests.model.gradient_boosted_trees.proto.gradient_boosted_trees_config] {\n",
            "      num_trees: 300\n",
            "      decision_tree {\n",
            "        max_depth: 6\n",
            "        min_examples: 5\n",
            "        in_split_min_examples_check: true\n",
            "        keep_non_leaf_label_distribution: true\n",
            "        num_candidate_attributes: -1\n",
            "        missing_value_policy: GLOBAL_IMPUTATION\n",
            "        allow_na_conditions: false\n",
            "        categorical_set_greedy_forward {\n",
            "          sampling: 0.1\n",
            "          max_num_items: -1\n",
            "          min_item_frequency: 1\n",
            "        }\n",
            "        growing_strategy_local {\n",
            "        }\n",
            "        categorical {\n",
            "          cart {\n",
            "          }\n",
            "        }\n",
            "        axis_aligned_split {\n",
            "        }\n",
            "        internal {\n",
            "          sorting_strategy: PRESORTED\n",
            "        }\n",
            "        uplift {\n",
            "          min_examples_in_treatment: 5\n",
            "          split_score: KULLBACK_LEIBLER\n",
            "        }\n",
            "      }\n",
            "      shrinkage: 0.1\n",
            "      loss: DEFAULT\n",
            "      validation_set_ratio: 0.1\n",
            "      validation_interval_in_trees: 1\n",
            "      early_stopping: VALIDATION_LOSS_INCREASE\n",
            "      early_stopping_num_trees_look_ahead: 30\n",
            "      l2_regularization: 0\n",
            "      lambda_loss: 1\n",
            "      mart {\n",
            "      }\n",
            "      adapt_subsample_for_maximum_training_duration: false\n",
            "      l1_regularization: 0\n",
            "      use_hessian_gain: false\n",
            "      l2_regularization_categorical: 1\n",
            "      stochastic_gradient_boosting {\n",
            "        ratio: 1\n",
            "      }\n",
            "      apply_link_function: true\n",
            "      compute_permutation_variable_importance: false\n",
            "      binary_focal_loss_options {\n",
            "        misprediction_exponent: 2\n",
            "        positive_sample_coefficient: 0.5\n",
            "      }\n",
            "      early_stopping_initial_iteration: 10\n",
            "    }\n",
            "  }\n",
            "  optimizer {\n",
            "    optimizer_key: \"RANDOM\"\n",
            "    [yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.random] {\n",
            "      num_trials: 100\n",
            "    }\n",
            "  }\n",
            "  base_learner_deployment {\n",
            "    num_threads: 1\n",
            "  }\n",
            "  predefined_search_space {\n",
            "  }\n",
            "}\n",
            "\n",
            "[INFO 23-11-04 17:31:58.4890 UTC kernel.cc:827] Deployment config:\n",
            "cache_path: \"/tmp/tmpgcwqak7x/working_cache\"\n",
            "num_threads: 12\n",
            "try_resume_training: true\n",
            "\n",
            "[INFO 23-11-04 17:31:58.5415 UTC kernel.cc:889] Train model\n",
            "[INFO 23-11-04 17:31:58.5419 UTC hyperparameters_optimizer.cc:209] Hyperparameter search space:\n",
            "fields {\n",
            "  name: \"split_axis\"\n",
            "  discrete_candidates {\n",
            "    possible_values {\n",
            "      categorical: \"AXIS_ALIGNED\"\n",
            "    }\n",
            "    possible_values {\n",
            "      categorical: \"SPARSE_OBLIQUE\"\n",
            "    }\n",
            "  }\n",
            "  children {\n",
            "    name: \"sparse_oblique_projection_density_factor\"\n",
            "    discrete_candidates {\n",
            "      possible_values {\n",
            "        real: 1\n",
            "      }\n",
            "      possible_values {\n",
            "        real: 2\n",
            "      }\n",
            "      possible_values {\n",
            "        real: 3\n",
            "      }\n",
            "      possible_values {\n",
            "        real: 4\n",
            "      }\n",
            "      possible_values {\n",
            "        real: 5\n",
            "      }\n",
            "    }\n",
            "    parent_discrete_values {\n",
            "      possible_values {\n",
            "        categorical: \"SPARSE_OBLIQUE\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  children {\n",
            "    name: \"sparse_oblique_normalization\"\n",
            "    discrete_candidates {\n",
            "      possible_values {\n",
            "        categorical: \"NONE\"\n",
            "      }\n",
            "      possible_values {\n",
            "        categorical: \"STANDARD_DEVIATION\"\n",
            "      }\n",
            "      possible_values {\n",
            "        categorical: \"MIN_MAX\"\n",
            "      }\n",
            "    }\n",
            "    parent_discrete_values {\n",
            "      possible_values {\n",
            "        categorical: \"SPARSE_OBLIQUE\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  children {\n",
            "    name: \"sparse_oblique_weights\"\n",
            "    discrete_candidates {\n",
            "      possible_values {\n",
            "        categorical: \"BINARY\"\n",
            "      }\n",
            "      possible_values {\n",
            "        categorical: \"CONTINUOUS\"\n",
            "      }\n",
            "    }\n",
            "    parent_discrete_values {\n",
            "      possible_values {\n",
            "        categorical: \"SPARSE_OBLIQUE\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"categorical_algorithm\"\n",
            "  discrete_candidates {\n",
            "    possible_values {\n",
            "      categorical: \"CART\"\n",
            "    }\n",
            "    possible_values {\n",
            "      categorical: \"RANDOM\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"growing_strategy\"\n",
            "  discrete_candidates {\n",
            "    possible_values {\n",
            "      categorical: \"LOCAL\"\n",
            "    }\n",
            "    possible_values {\n",
            "      categorical: \"BEST_FIRST_GLOBAL\"\n",
            "    }\n",
            "  }\n",
            "  children {\n",
            "    name: \"max_num_nodes\"\n",
            "    discrete_candidates {\n",
            "      possible_values {\n",
            "        integer: 16\n",
            "      }\n",
            "      possible_values {\n",
            "        integer: 32\n",
            "      }\n",
            "      possible_values {\n",
            "        integer: 64\n",
            "      }\n",
            "      possible_values {\n",
            "        integer: 128\n",
            "      }\n",
            "      possible_values {\n",
            "        integer: 256\n",
            "      }\n",
            "      possible_values {\n",
            "        integer: 512\n",
            "      }\n",
            "    }\n",
            "    parent_discrete_values {\n",
            "      possible_values {\n",
            "        categorical: \"BEST_FIRST_GLOBAL\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  children {\n",
            "    name: \"max_depth\"\n",
            "    discrete_candidates {\n",
            "      possible_values {\n",
            "        integer: 3\n",
            "      }\n",
            "      possible_values {\n",
            "        integer: 4\n",
            "      }\n",
            "      possible_values {\n",
            "        integer: 6\n",
            "      }\n",
            "      possible_values {\n",
            "        integer: 8\n",
            "      }\n",
            "    }\n",
            "    parent_discrete_values {\n",
            "      possible_values {\n",
            "        categorical: \"LOCAL\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"sampling_method\"\n",
            "  discrete_candidates {\n",
            "    possible_values {\n",
            "      categorical: \"RANDOM\"\n",
            "    }\n",
            "  }\n",
            "  children {\n",
            "    name: \"subsample\"\n",
            "    discrete_candidates {\n",
            "      possible_values {\n",
            "        real: 0.6\n",
            "      }\n",
            "      possible_values {\n",
            "        real: 0.8\n",
            "      }\n",
            "      possible_values {\n",
            "        real: 0.9\n",
            "      }\n",
            "      possible_values {\n",
            "        real: 1\n",
            "      }\n",
            "    }\n",
            "    parent_discrete_values {\n",
            "      possible_values {\n",
            "        categorical: \"RANDOM\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"shrinkage\"\n",
            "  discrete_candidates {\n",
            "    possible_values {\n",
            "      real: 0.02\n",
            "    }\n",
            "    possible_values {\n",
            "      real: 0.05\n",
            "    }\n",
            "    possible_values {\n",
            "      real: 0.1\n",
            "    }\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"min_examples\"\n",
            "  discrete_candidates {\n",
            "    possible_values {\n",
            "      integer: 5\n",
            "    }\n",
            "    possible_values {\n",
            "      integer: 7\n",
            "    }\n",
            "    possible_values {\n",
            "      integer: 10\n",
            "    }\n",
            "    possible_values {\n",
            "      integer: 20\n",
            "    }\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"num_candidate_attributes_ratio\"\n",
            "  discrete_candidates {\n",
            "    possible_values {\n",
            "      real: 0.2\n",
            "    }\n",
            "    possible_values {\n",
            "      real: 0.5\n",
            "    }\n",
            "    possible_values {\n",
            "      real: 0.9\n",
            "    }\n",
            "    possible_values {\n",
            "      real: 1\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "[INFO 23-11-04 17:31:58.5421 UTC hyperparameters_optimizer.cc:500] Start local tuner with 12 thread(s)\n",
            "[INFO 23-11-04 17:31:58.5436 UTC gradient_boosted_trees.cc:[INFO 23-11-04 17:31:58.5437 UTC gradient_boosted_trees.cc470] Default loss set to SQUARED_ERROR\n",
            ":470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 17:31:58.5437 UTC gradient_boosted_trees.cc[INFO:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            " 23-11-04 17:31:58.5437 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 17:31:58.5442 UTC gradient_boosted_trees.cc[INFO:470] Default loss set to  23-11-04 17:31:58.5442 UTC [INFOgradient_boosted_trees.cc:470] Default loss set to  23-11-04 17:31:58.5443 UTC SQUARED_ERRORSQUARED_ERROR\n",
            "\n",
            "[gradient_boosted_trees.cc:470] Default loss set to INFOSQUARED_ERROR\n",
            " 23-11-04 17:31:58.5443 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000[[INFOINFO 23-11-04 17:31:58.5443 UTC gradient_boosted_trees.cc: example(s) and [49 feature(s).\n",
            "INFO1097 23-11-04 17:31:58.5443 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            " 23-11-04 17:31:58.5443 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 17:31:58.5443 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 17:31:58.5443 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 17:31:58.5443 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 17:31:58.5445 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 17:31:58.5445 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 17:31:58.5449 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 17:31:58.5449 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 17:31:58.5456 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 17:31:58.5456 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 17:31:58.5459 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 17:31:58.5459 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 17:31:58.5470 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 17:31:58.5470 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[[INFOINFO 23-11-04 17:31:58.6240 UTC gradient_boosted_trees.cc:1140]  23-11-04 17:31:58.6240 UTC gradient_boosted_trees.cc:360301140]  examples used for training and 3970 examples used for validation\n",
            "36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:31:58.6242 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:31:58.6249 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:31:58.6256 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:31:58.6257 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:31:58.6260 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:31:58.6260 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:31:58.6261 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:31:58.6272 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:31:58.6277 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:31:58.6288 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 17:32:06.8587 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.638248 train-rmse:8.638248 valid-loss:8.556311 valid-rmse:8.556311\n",
            "[INFO 23-11-04 17:32:12.4338 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.615995 train-rmse:8.615995 valid-loss:8.539273 valid-rmse:8.539273\n",
            "[INFO 23-11-04 17:32:13.2454 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.588593 train-rmse:8.588593 valid-loss:8.525038 valid-rmse:8.525038\n",
            "[INFO 23-11-04 17:32:13.9510 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.616847 train-rmse:8.616847 valid-loss:8.538195 valid-rmse:8.538195\n",
            "[INFO 23-11-04 17:32:15.0663 UTC gradient_boosted_trees.cc:1556] \tnum-trees:2 train-loss:8.622404 train-rmse:8.622404 valid-loss:8.552759 valid-rmse:8.552759\n",
            "[INFO 23-11-04 17:32:16.3049 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.594133 train-rmse:8.594133 valid-loss:8.527593 valid-rmse:8.527593\n",
            "[INFO 23-11-04 17:32:19.0306 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.620039 train-rmse:8.620039 valid-loss:8.545753 valid-rmse:8.545753\n",
            "[INFO 23-11-04 17:32:24.1031 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.611106 train-rmse:8.611106 valid-loss:8.546303 valid-rmse:8.546303\n",
            "[INFO 23-11-04 17:32:25.7434 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.619170 train-rmse:8.619170 valid-loss:8.538978 valid-rmse:8.538978\n",
            "[INFO 23-11-04 17:32:25.9271 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.616047 train-rmse:8.616047 valid-loss:8.540732 valid-rmse:8.540732\n",
            "[INFO 23-11-04 17:32:26.2806 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.587803 train-rmse:8.587803 valid-loss:8.506313 valid-rmse:8.506313\n",
            "[INFO 23-11-04 17:32:29.8731 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.584060 train-rmse:8.584060 valid-loss:8.504259 valid-rmse:8.504259\n",
            "[INFO 23-11-04 17:32:34.0280 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.636563 train-rmse:8.636563 valid-loss:8.551507 valid-rmse:8.551507\n",
            "[INFO 23-11-04 17:32:45.7503 UTC gradient_boosted_trees.cc:1556] \tnum-trees:3 train-loss:8.562968 train-rmse:8.562968 valid-loss:8.514781 valid-rmse:8.514781\n",
            "[INFO 23-11-04 17:33:17.5359 UTC gradient_boosted_trees.cc:1556] \tnum-trees:5 train-loss:8.508411 train-rmse:8.508411 valid-loss:8.485032 valid-rmse:8.485032\n",
            "[INFO 23-11-04 17:33:48.3269 UTC gradient_boosted_trees.cc:1556] \tnum-trees:4 train-loss:8.517160 train-rmse:8.517160 valid-loss:8.487201 valid-rmse:8.487201\n",
            "[INFO 23-11-04 17:34:18.4340 UTC gradient_boosted_trees.cc:1556] \tnum-trees:5 train-loss:8.383707 train-rmse:8.383707 valid-loss:8.445627 valid-rmse:8.445627\n",
            "[INFO 23-11-04 17:34:52.4409 UTC gradient_boosted_trees.cc:1556] \tnum-trees:11 train-loss:8.364084 train-rmse:8.364084 valid-loss:8.420601 valid-rmse:8.420601\n",
            "[INFO 23-11-04 17:35:23.7925 UTC gradient_boosted_trees.cc:1556] \tnum-trees:13 train-loss:8.320973 train-rmse:8.320973 valid-loss:8.396062 valid-rmse:8.396062\n",
            "[INFO 23-11-04 17:35:54.5202 UTC gradient_boosted_trees.cc:1556] \tnum-trees:16 train-loss:8.123292 train-rmse:8.123292 valid-loss:8.347697 valid-rmse:8.347697\n",
            "[INFO 23-11-04 17:36:26.4486 UTC gradient_boosted_trees.cc:1556] \tnum-trees:17 train-loss:8.247189 train-rmse:8.247189 valid-loss:8.371318 valid-rmse:8.371318\n",
            "[INFO 23-11-04 17:36:56.6571 UTC gradient_boosted_trees.cc:1556] \tnum-trees:11 train-loss:8.344237 train-rmse:8.344237 valid-loss:8.431623 valid-rmse:8.431623\n",
            "[INFO 23-11-04 17:37:28.9257 UTC gradient_boosted_trees.cc:1556] \tnum-trees:40 train-loss:8.331039 train-rmse:8.331039 valid-loss:8.451235 valid-rmse:8.451235\n",
            "[INFO 23-11-04 17:38:00.4650 UTC gradient_boosted_trees.cc:1556] \tnum-trees:20 train-loss:8.107463 train-rmse:8.107463 valid-loss:8.351893 valid-rmse:8.351893\n",
            "[INFO 23-11-04 17:38:32.5614 UTC gradient_boosted_trees.cc:1556] \tnum-trees:14 train-loss:8.113905 train-rmse:8.113905 valid-loss:8.356574 valid-rmse:8.356574\n",
            "[INFO 23-11-04 17:39:04.2893 UTC gradient_boosted_trees.cc:1556] \tnum-trees:27 train-loss:8.112280 train-rmse:8.112280 valid-loss:8.328620 valid-rmse:8.328620\n",
            "[INFO 23-11-04 17:39:35.6731 UTC gradient_boosted_trees.cc:1556] \tnum-trees:18 train-loss:8.158340 train-rmse:8.158340 valid-loss:8.360351 valid-rmse:8.360351\n",
            "[INFO 23-11-04 17:40:06.3560 UTC gradient_boosted_trees.cc:1556] \tnum-trees:14 train-loss:8.444339 train-rmse:8.444339 valid-loss:8.460644 valid-rmse:8.460644\n",
            "[INFO 23-11-04 17:40:40.7056 UTC gradient_boosted_trees.cc:1556] \tnum-trees:33 train-loss:8.057167 train-rmse:8.057167 valid-loss:8.310140 valid-rmse:8.310140\n",
            "[INFO 23-11-04 17:41:13.0096 UTC gradient_boosted_trees.cc:1556] \tnum-trees:35 train-loss:8.035466 train-rmse:8.035466 valid-loss:8.296981 valid-rmse:8.296981\n",
            "[INFO 23-11-04 17:41:44.0553 UTC gradient_boosted_trees.cc:1556] \tnum-trees:21 train-loss:8.188186 train-rmse:8.188186 valid-loss:8.344859 valid-rmse:8.344859\n",
            "[INFO 23-11-04 17:42:14.1278 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:8.230419 train-rmse:8.230419 valid-loss:8.382823 valid-rmse:8.382823\n",
            "[INFO 23-11-04 17:42:44.6491 UTC gradient_boosted_trees.cc:1556] \tnum-trees:45 train-loss:7.991722 train-rmse:7.991722 valid-loss:8.288523 valid-rmse:8.288523\n",
            "[INFO 23-11-04 17:43:14.8491 UTC gradient_boosted_trees.cc:1556] \tnum-trees:25 train-loss:8.121395 train-rmse:8.121395 valid-loss:8.354197 valid-rmse:8.354197\n",
            "[INFO 23-11-04 17:43:45.0569 UTC gradient_boosted_trees.cc:1556] \tnum-trees:25 train-loss:7.953839 train-rmse:7.953839 valid-loss:8.322123 valid-rmse:8.322123\n",
            "[INFO 23-11-04 17:44:16.1155 UTC gradient_boosted_trees.cc:1556] \tnum-trees:50 train-loss:7.688675 train-rmse:7.688675 valid-loss:8.235433 valid-rmse:8.235433\n",
            "[INFO 23-11-04 17:44:47.0748 UTC gradient_boosted_trees.cc:1556] \tnum-trees:36 train-loss:8.086420 train-rmse:8.086420 valid-loss:8.313293 valid-rmse:8.313293\n",
            "[INFO 23-11-04 17:45:19.4080 UTC gradient_boosted_trees.cc:1556] \tnum-trees:23 train-loss:8.343883 train-rmse:8.343883 valid-loss:8.416344 valid-rmse:8.416344\n",
            "[INFO 23-11-04 17:45:52.5557 UTC gradient_boosted_trees.cc:1556] \tnum-trees:58 train-loss:7.890029 train-rmse:7.890029 valid-loss:8.241487 valid-rmse:8.241487\n",
            "[INFO 23-11-04 17:46:23.8905 UTC gradient_boosted_trees.cc:1556] \tnum-trees:47 train-loss:7.869946 train-rmse:7.869946 valid-loss:8.310179 valid-rmse:8.310179\n",
            "[INFO 23-11-04 17:46:56.1700 UTC gradient_boosted_trees.cc:1556] \tnum-trees:32 train-loss:8.069304 train-rmse:8.069304 valid-loss:8.306046 valid-rmse:8.306046\n",
            "[INFO 23-11-04 17:47:27.4016 UTC gradient_boosted_trees.cc:1556] \tnum-trees:63 train-loss:7.556797 train-rmse:7.556797 valid-loss:8.215213 valid-rmse:8.215213\n",
            "[INFO 23-11-04 17:47:57.6056 UTC gradient_boosted_trees.cc:1556] \tnum-trees:117 train-loss:8.138626 train-rmse:8.138626 valid-loss:8.333867 valid-rmse:8.333867\n",
            "[INFO 23-11-04 17:48:28.1654 UTC gradient_boosted_trees.cc:1556] \tnum-trees:62 train-loss:7.835907 train-rmse:7.835907 valid-loss:8.249990 valid-rmse:8.249990\n",
            "[INFO 23-11-04 17:48:59.0916 UTC gradient_boosted_trees.cc:1556] \tnum-trees:32 train-loss:7.724291 train-rmse:7.724291 valid-loss:8.238240 valid-rmse:8.238240\n",
            "[INFO 23-11-04 17:49:29.9825 UTC gradient_boosted_trees.cc:1556] \tnum-trees:73 train-loss:7.780625 train-rmse:7.780625 valid-loss:8.204402 valid-rmse:8.204402\n",
            "[INFO 23-11-04 17:50:00.2923 UTC gradient_boosted_trees.cc:1556] \tnum-trees:38 train-loss:7.788770 train-rmse:7.788770 valid-loss:8.283295 valid-rmse:8.283295\n",
            "[INFO 23-11-04 17:50:31.5905 UTC gradient_boosted_trees.cc:1556] \tnum-trees:41 train-loss:7.963360 train-rmse:7.963360 valid-loss:8.309945 valid-rmse:8.309945\n",
            "[INFO 23-11-04 17:51:04.3119 UTC gradient_boosted_trees.cc:1556] \tnum-trees:62 train-loss:7.746540 train-rmse:7.746540 valid-loss:8.281479 valid-rmse:8.281479\n",
            "[INFO 23-11-04 17:51:37.2994 UTC gradient_boosted_trees.cc:1556] \tnum-trees:80 train-loss:7.391380 train-rmse:7.391380 valid-loss:8.175473 valid-rmse:8.175473\n",
            "[INFO 23-11-04 17:52:08.5038 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:7.709889 train-rmse:7.709889 valid-loss:8.191254 valid-rmse:8.191254\n",
            "[INFO 23-11-04 17:52:38.8319 UTC gradient_boosted_trees.cc:1556] \tnum-trees:58 train-loss:7.930508 train-rmse:7.930508 valid-loss:8.268314 valid-rmse:8.268314\n",
            "[INFO 23-11-04 17:53:09.3823 UTC gradient_boosted_trees.cc:1556] \tnum-trees:45 train-loss:7.967201 train-rmse:7.967201 valid-loss:8.254163 valid-rmse:8.254163\n",
            "[INFO 23-11-04 17:53:43.1735 UTC gradient_boosted_trees.cc:1556] \tnum-trees:61 train-loss:7.911262 train-rmse:7.911262 valid-loss:8.261592 valid-rmse:8.261592\n",
            "[INFO 23-11-04 17:54:17.9669 UTC gradient_boosted_trees.cc:1556] \tnum-trees:163 train-loss:8.067040 train-rmse:8.067040 valid-loss:8.311289 valid-rmse:8.311289\n",
            "[INFO 23-11-04 17:54:48.7886 UTC gradient_boosted_trees.cc:1556] \tnum-trees:93 train-loss:7.257835 train-rmse:7.257835 valid-loss:8.141228 valid-rmse:8.141228\n",
            "[INFO 23-11-04 17:55:19.4098 UTC gradient_boosted_trees.cc:1556] \tnum-trees:49 train-loss:7.652422 train-rmse:7.652422 valid-loss:8.224399 valid-rmse:8.224399\n",
            "[INFO 23-11-04 17:55:49.4345 UTC gradient_boosted_trees.cc:1556] \tnum-trees:174 train-loss:8.052426 train-rmse:8.052426 valid-loss:8.300452 valid-rmse:8.300452\n",
            "[INFO 23-11-04 17:56:22.7319 UTC gradient_boosted_trees.cc:1556] \tnum-trees:178 train-loss:8.047037 train-rmse:8.047037 valid-loss:8.298456 valid-rmse:8.298456\n",
            "[INFO 23-11-04 17:56:54.2108 UTC gradient_boosted_trees.cc:1556] \tnum-trees:59 train-loss:7.660069 train-rmse:7.660069 valid-loss:8.192480 valid-rmse:8.192480\n",
            "[INFO 23-11-04 17:57:25.7078 UTC gradient_boosted_trees.cc:1556] \tnum-trees:106 train-loss:7.583476 train-rmse:7.583476 valid-loss:8.140545 valid-rmse:8.140545\n",
            "[INFO 23-11-04 17:57:56.0248 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:7.574786 train-rmse:7.574786 valid-loss:8.210042 valid-rmse:8.210042\n",
            "[INFO 23-11-04 17:58:26.1750 UTC gradient_boosted_trees.cc:1556] \tnum-trees:193 train-loss:8.027350 train-rmse:8.027350 valid-loss:8.289392 valid-rmse:8.289392\n",
            "[INFO 23-11-04 17:58:59.2049 UTC gradient_boosted_trees.cc:1556] \tnum-trees:197 train-loss:8.022568 train-rmse:8.022568 valid-loss:8.285763 valid-rmse:8.285763\n",
            "[INFO 23-11-04 17:59:29.5604 UTC gradient_boosted_trees.cc:1556] \tnum-trees:89 train-loss:7.542449 train-rmse:7.542449 valid-loss:8.205120 valid-rmse:8.205120\n",
            "[INFO 23-11-04 18:00:02.1841 UTC gradient_boosted_trees.cc:1556] \tnum-trees:114 train-loss:7.112978 train-rmse:7.112978 valid-loss:8.120892 valid-rmse:8.120892\n",
            "[INFO 23-11-04 18:00:33.5931 UTC gradient_boosted_trees.cc:1556] \tnum-trees:81 train-loss:7.792523 train-rmse:7.792523 valid-loss:8.215636 valid-rmse:8.215636\n",
            "[INFO 23-11-04 18:01:03.6547 UTC gradient_boosted_trees.cc:1556] \tnum-trees:121 train-loss:7.502710 train-rmse:7.502710 valid-loss:8.116151 valid-rmse:8.116151\n",
            "[INFO 23-11-04 18:01:33.6876 UTC gradient_boosted_trees.cc:1556] \tnum-trees:62 train-loss:7.501983 train-rmse:7.501983 valid-loss:8.181867 valid-rmse:8.181867\n",
            "[INFO 23-11-04 18:02:05.6840 UTC gradient_boosted_trees.cc:1556] \tnum-trees:113 train-loss:7.518864 train-rmse:7.518864 valid-loss:8.175566 valid-rmse:8.175566\n",
            "[INFO 23-11-04 18:02:36.2350 UTC gradient_boosted_trees.cc:1556] \tnum-trees:99 train-loss:7.482814 train-rmse:7.482814 valid-loss:8.194220 valid-rmse:8.194220\n",
            "[INFO 23-11-04 18:03:10.7557 UTC gradient_boosted_trees.cc:1556] \tnum-trees:117 train-loss:7.494944 train-rmse:7.494944 valid-loss:8.165740 valid-rmse:8.165740\n",
            "[INFO 23-11-04 18:03:41.0402 UTC gradient_boosted_trees.cc:1556] \tnum-trees:67 train-loss:7.826887 train-rmse:7.826887 valid-loss:8.211494 valid-rmse:8.211494\n",
            "[INFO 23-11-04 18:04:11.6503 UTC gradient_boosted_trees.cc:1556] \tnum-trees:235 train-loss:7.971599 train-rmse:7.971599 valid-loss:8.254185 valid-rmse:8.254185\n",
            "[INFO 23-11-04 18:04:41.8431 UTC gradient_boosted_trees.cc:1556] \tnum-trees:136 train-loss:7.414863 train-rmse:7.414863 valid-loss:8.099148 valid-rmse:8.099148\n",
            "[INFO 23-11-04 18:05:13.4850 UTC gradient_boosted_trees.cc:1556] \tnum-trees:62 train-loss:7.364594 train-rmse:7.364594 valid-loss:8.103435 valid-rmse:8.103435\n",
            "[INFO 23-11-04 18:05:44.8097 UTC gradient_boosted_trees.cc:1556] \tnum-trees:63 train-loss:7.349908 train-rmse:7.349908 valid-loss:8.107100 valid-rmse:8.107100\n",
            "[INFO 23-11-04 18:06:15.1040 UTC gradient_boosted_trees.cc:1556] \tnum-trees:81 train-loss:7.481593 train-rmse:7.481593 valid-loss:8.118996 valid-rmse:8.118996\n",
            "[INFO 23-11-04 18:06:47.5933 UTC gradient_boosted_trees.cc:1556] \tnum-trees:254 train-loss:7.948676 train-rmse:7.948676 valid-loss:8.250827 valid-rmse:8.250827\n",
            "[INFO 23-11-04 18:07:20.3521 UTC gradient_boosted_trees.cc:1556] \tnum-trees:258 train-loss:7.943368 train-rmse:7.943368 valid-loss:8.245169 valid-rmse:8.245169\n",
            "[INFO 23-11-04 18:07:52.2083 UTC gradient_boosted_trees.cc:1556] \tnum-trees:146 train-loss:6.869876 train-rmse:6.869876 valid-loss:8.054891 valid-rmse:8.054891\n",
            "[INFO 23-11-04 18:08:26.0991 UTC gradient_boosted_trees.cc:1556] \tnum-trees:266 train-loss:7.930621 train-rmse:7.930621 valid-loss:8.237146 valid-rmse:8.237146\n",
            "[INFO 23-11-04 18:08:58.0030 UTC gradient_boosted_trees.cc:1556] \tnum-trees:78 train-loss:7.747209 train-rmse:7.747209 valid-loss:8.187686 valid-rmse:8.187686\n",
            "[INFO 23-11-04 18:09:29.8732 UTC gradient_boosted_trees.cc:1556] \tnum-trees:107 train-loss:7.659927 train-rmse:7.659927 valid-loss:8.195986 valid-rmse:8.195986\n",
            "[INFO 23-11-04 18:10:02.7476 UTC gradient_boosted_trees.cc:1556] \tnum-trees:123 train-loss:7.319670 train-rmse:7.319670 valid-loss:8.188296 valid-rmse:8.188296\n",
            "[INFO 23-11-04 18:10:34.2610 UTC gradient_boosted_trees.cc:1556] \tnum-trees:66 train-loss:8.007997 train-rmse:8.007997 valid-loss:8.307251 valid-rmse:8.307251\n",
            "[INFO 23-11-04 18:11:07.2963 UTC gradient_boosted_trees.cc:1556] \tnum-trees:73 train-loss:7.268456 train-rmse:7.268456 valid-loss:8.075726 valid-rmse:8.075726\n",
            "[INFO 23-11-04 18:11:39.7136 UTC gradient_boosted_trees.cc:1556] \tnum-trees:74 train-loss:7.257917 train-rmse:7.257917 valid-loss:8.072008 valid-rmse:8.072008\n",
            "[INFO 23-11-04 18:12:10.3079 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:7.317049 train-rmse:7.317049 valid-loss:8.144490 valid-rmse:8.144490\n",
            "[INFO 23-11-04 18:12:41.8898 UTC gradient_boosted_trees.cc:1556] \tnum-trees:297 train-loss:7.891524 train-rmse:7.891524 valid-loss:8.221350 valid-rmse:8.221350\n",
            "[INFO 23-11-04 18:13:06.5048 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.887765 train-rmse:7.887765 valid-loss:8.218175 valid-rmse:8.218175\n",
            "[INFO 23-11-04 18:13:06.5048 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 18:13:06.5048 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.218175 valid-rmse:8.218175\n",
            "[INFO 23-11-04 18:13:06.5077 UTC hyperparameters_optimizer.cc:582] [1/100] Score: -8.21817 / -8.21817 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 3 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-04 18:13:06.5083 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 18:13:06.5083 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 18:13:06.5231 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 18:13:12.4576 UTC gradient_boosted_trees.cc:1556] \tnum-trees:118 train-loss:7.600811 train-rmse:7.600811 valid-loss:8.172999 valid-rmse:8.172999\n",
            "[INFO 23-11-04 18:13:26.0898 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.635784 train-rmse:8.635784 valid-loss:8.550377 valid-rmse:8.550377\n",
            "[INFO 23-11-04 18:13:43.2848 UTC gradient_boosted_trees.cc:1556] \tnum-trees:88 train-loss:7.663222 train-rmse:7.663222 valid-loss:8.149622 valid-rmse:8.149622\n",
            "[INFO 23-11-04 18:14:14.8070 UTC gradient_boosted_trees.cc:1556] \tnum-trees:172 train-loss:6.676058 train-rmse:6.676058 valid-loss:8.022137 valid-rmse:8.022137\n",
            "[INFO 23-11-04 18:14:45.3792 UTC gradient_boosted_trees.cc:1556] \tnum-trees:5 train-loss:8.577215 train-rmse:8.577215 valid-loss:8.522782 valid-rmse:8.522782\n",
            "[INFO 23-11-04 18:15:15.9539 UTC gradient_boosted_trees.cc:1556] \tnum-trees:140 train-loss:7.221179 train-rmse:7.221179 valid-loss:8.151382 valid-rmse:8.151382\n",
            "[INFO 23-11-04 18:15:46.4796 UTC gradient_boosted_trees.cc:1556] \tnum-trees:164 train-loss:7.253547 train-rmse:7.253547 valid-loss:8.091112 valid-rmse:8.091112\n",
            "[INFO 23-11-04 18:16:18.0912 UTC gradient_boosted_trees.cc:1556] \tnum-trees:97 train-loss:7.556291 train-rmse:7.556291 valid-loss:8.164314 valid-rmse:8.164314\n",
            "[INFO 23-11-04 18:16:49.1756 UTC gradient_boosted_trees.cc:1556] \tnum-trees:186 train-loss:7.189587 train-rmse:7.189587 valid-loss:8.034534 valid-rmse:8.034534\n",
            "[INFO 23-11-04 18:17:22.8166 UTC gradient_boosted_trees.cc:1556] \tnum-trees:170 train-loss:7.219010 train-rmse:7.219010 valid-loss:8.081207 valid-rmse:8.081207\n",
            "[INFO 23-11-04 18:17:55.0114 UTC gradient_boosted_trees.cc:1556] \tnum-trees:172 train-loss:7.209423 train-rmse:7.209423 valid-loss:8.078952 valid-rmse:8.078952\n",
            "[INFO 23-11-04 18:18:26.2201 UTC gradient_boosted_trees.cc:1556] \tnum-trees:135 train-loss:7.527017 train-rmse:7.527017 valid-loss:8.149214 valid-rmse:8.149214\n",
            "[INFO 23-11-04 18:18:58.4011 UTC gradient_boosted_trees.cc:1556] \tnum-trees:99 train-loss:7.588923 train-rmse:7.588923 valid-loss:8.117728 valid-rmse:8.117728\n",
            "[INFO 23-11-04 18:19:28.6721 UTC gradient_boosted_trees.cc:1556] \tnum-trees:81 train-loss:7.924032 train-rmse:7.924032 valid-loss:8.281804 valid-rmse:8.281804\n",
            "[INFO 23-11-04 18:19:58.7128 UTC gradient_boosted_trees.cc:1556] \tnum-trees:199 train-loss:7.127944 train-rmse:7.127944 valid-loss:8.018218 valid-rmse:8.018218\n",
            "[INFO 23-11-04 18:20:31.4855 UTC gradient_boosted_trees.cc:1556] \tnum-trees:157 train-loss:7.152340 train-rmse:7.152340 valid-loss:8.160703 valid-rmse:8.160703\n",
            "[INFO 23-11-04 18:21:02.0348 UTC gradient_boosted_trees.cc:1556] \tnum-trees:200 train-loss:6.488562 train-rmse:6.488562 valid-loss:7.983098 valid-rmse:7.983098\n",
            "[INFO 23-11-04 18:21:39.1228 UTC gradient_boosted_trees.cc:1556] \tnum-trees:117 train-loss:7.283482 train-rmse:7.283482 valid-loss:8.081940 valid-rmse:8.081940\n",
            "[INFO 23-11-04 18:22:09.2652 UTC gradient_boosted_trees.cc:1556] \tnum-trees:208 train-loss:7.090605 train-rmse:7.090605 valid-loss:8.001805 valid-rmse:8.001805\n",
            "[INFO 23-11-04 18:22:41.4665 UTC gradient_boosted_trees.cc:1556] \tnum-trees:149 train-loss:7.467673 train-rmse:7.467673 valid-loss:8.108386 valid-rmse:8.108386\n",
            "[INFO 23-11-04 18:23:13.2164 UTC gradient_boosted_trees.cc:1556] \tnum-trees:107 train-loss:7.152264 train-rmse:7.152264 valid-loss:8.131037 valid-rmse:8.131037\n",
            "[INFO 23-11-04 18:23:47.5263 UTC gradient_boosted_trees.cc:1556] \tnum-trees:122 train-loss:7.254702 train-rmse:7.254702 valid-loss:8.074040 valid-rmse:8.074040\n",
            "[INFO 23-11-04 18:24:18.2107 UTC gradient_boosted_trees.cc:1556] \tnum-trees:110 train-loss:7.527240 train-rmse:7.527240 valid-loss:8.105639 valid-rmse:8.105639\n",
            "[INFO 23-11-04 18:24:49.8122 UTC gradient_boosted_trees.cc:1556] \tnum-trees:171 train-loss:7.090481 train-rmse:7.090481 valid-loss:8.148721 valid-rmse:8.148721\n",
            "[INFO 23-11-04 18:25:08.1326 UTC early_stopping.cc:53] Early stop of the training because the validation loss does not decrease anymore. Best valid-loss: 8.14321\n",
            "[INFO 23-11-04 18:25:08.1326 UTC gradient_boosted_trees.cc:249] Truncates the model to 142 tree(s) i.e. 142  iteration(s).\n",
            "[INFO 23-11-04 18:25:08.1329 UTC gradient_boosted_trees.cc:312] Final model num-trees:142 valid-loss:8.143211 valid-rmse:8.143211\n",
            "[INFO 23-11-04 18:25:08.1351 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 18:25:08.1351 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 18:25:08.1351 UTC hyperparameters_optimizer.cc:582] [2/100] Score: -8.14321 / -8.14321 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 16 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-04 18:25:08.1475 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 18:25:15.4486 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.630530 train-rmse:8.630530 valid-loss:8.547394 valid-rmse:8.547394\n",
            "[INFO 23-11-04 18:25:21.6881 UTC gradient_boosted_trees.cc:1556] \tnum-trees:91 train-loss:7.876587 train-rmse:7.876587 valid-loss:8.263227 valid-rmse:8.263227\n",
            "[INFO 23-11-04 18:25:52.8029 UTC gradient_boosted_trees.cc:1556] \tnum-trees:6 train-loss:8.541693 train-rmse:8.541693 valid-loss:8.511541 valid-rmse:8.511541\n",
            "[INFO 23-11-04 18:26:22.9252 UTC gradient_boosted_trees.cc:1556] \tnum-trees:222 train-loss:6.354047 train-rmse:6.354047 valid-loss:7.973989 valid-rmse:7.973989\n",
            "[INFO 23-11-04 18:26:56.4323 UTC gradient_boosted_trees.cc:1556] \tnum-trees:120 train-loss:7.413124 train-rmse:7.413124 valid-loss:8.129547 valid-rmse:8.129547\n",
            "[INFO 23-11-04 18:27:29.2182 UTC gradient_boosted_trees.cc:1556] \tnum-trees:19 train-loss:8.425616 train-rmse:8.425616 valid-loss:8.445969 valid-rmse:8.445969\n",
            "[INFO 23-11-04 18:28:00.1089 UTC gradient_boosted_trees.cc:1556] \tnum-trees:232 train-loss:6.995350 train-rmse:6.995350 valid-loss:7.978732 valid-rmse:7.978732\n",
            "[INFO 23-11-04 18:28:30.2036 UTC gradient_boosted_trees.cc:1556] \tnum-trees:133 train-loss:7.196597 train-rmse:7.196597 valid-loss:8.055931 valid-rmse:8.055931\n",
            "[INFO 23-11-04 18:29:03.3778 UTC gradient_boosted_trees.cc:1556] \tnum-trees:233 train-loss:6.298814 train-rmse:6.298814 valid-loss:7.966885 valid-rmse:7.966885\n",
            "[INFO 23-11-04 18:29:36.4501 UTC gradient_boosted_trees.cc:1556] \tnum-trees:36 train-loss:8.338563 train-rmse:8.338563 valid-loss:8.415440 valid-rmse:8.415440\n",
            "[INFO 23-11-04 18:30:06.4711 UTC gradient_boosted_trees.cc:1556] \tnum-trees:122 train-loss:7.470236 train-rmse:7.470236 valid-loss:8.089351 valid-rmse:8.089351\n",
            "[INFO 23-11-04 18:30:36.7102 UTC gradient_boosted_trees.cc:1556] \tnum-trees:44 train-loss:8.309574 train-rmse:8.309574 valid-loss:8.401020 valid-rmse:8.401020\n",
            "[INFO 23-11-04 18:31:07.5942 UTC gradient_boosted_trees.cc:1556] \tnum-trees:221 train-loss:6.989402 train-rmse:6.989402 valid-loss:8.027746 valid-rmse:8.027746\n",
            "[INFO 23-11-04 18:31:39.2105 UTC gradient_boosted_trees.cc:1556] \tnum-trees:223 train-loss:6.977147 train-rmse:6.977147 valid-loss:8.027872 valid-rmse:8.027872\n",
            "[INFO 23-11-04 18:32:11.7292 UTC gradient_boosted_trees.cc:1556] \tnum-trees:225 train-loss:6.970100 train-rmse:6.970100 valid-loss:8.025506 valid-rmse:8.025506\n",
            "[INFO 23-11-04 18:32:41.8989 UTC gradient_boosted_trees.cc:1556] \tnum-trees:248 train-loss:6.218321 train-rmse:6.218321 valid-loss:7.948479 valid-rmse:7.948479\n",
            "[INFO 23-11-04 18:33:14.0187 UTC gradient_boosted_trees.cc:1556] \tnum-trees:65 train-loss:8.250671 train-rmse:8.250671 valid-loss:8.369057 valid-rmse:8.369057\n",
            "[INFO 23-11-04 18:33:45.1602 UTC gradient_boosted_trees.cc:1556] \tnum-trees:184 train-loss:7.344197 train-rmse:7.344197 valid-loss:8.078620 valid-rmse:8.078620\n",
            "[INFO 23-11-04 18:34:17.9835 UTC gradient_boosted_trees.cc:1556] \tnum-trees:106 train-loss:7.812041 train-rmse:7.812041 valid-loss:8.245710 valid-rmse:8.245710\n",
            "[INFO 23-11-04 18:34:50.5084 UTC gradient_boosted_trees.cc:1556] \tnum-trees:137 train-loss:7.325877 train-rmse:7.325877 valid-loss:8.098417 valid-rmse:8.098417\n",
            "[INFO 23-11-04 18:35:20.9784 UTC gradient_boosted_trees.cc:1556] \tnum-trees:82 train-loss:8.211964 train-rmse:8.211964 valid-loss:8.349086 valid-rmse:8.349086\n",
            "[INFO 23-11-04 18:35:51.2698 UTC gradient_boosted_trees.cc:1556] \tnum-trees:261 train-loss:6.150592 train-rmse:6.150592 valid-loss:7.941700 valid-rmse:7.941700\n",
            "[INFO 23-11-04 18:36:21.8578 UTC gradient_boosted_trees.cc:1556] \tnum-trees:70 train-loss:8.049945 train-rmse:8.049945 valid-loss:8.328061 valid-rmse:8.328061\n",
            "[INFO 23-11-04 18:36:52.9819 UTC gradient_boosted_trees.cc:1556] \tnum-trees:194 train-loss:7.311675 train-rmse:7.311675 valid-loss:8.070040 valid-rmse:8.070040\n",
            "[INFO 23-11-04 18:37:27.1794 UTC gradient_boosted_trees.cc:1556] \tnum-trees:271 train-loss:6.835556 train-rmse:6.835556 valid-loss:7.929405 valid-rmse:7.929405\n",
            "[INFO 23-11-04 18:37:57.4858 UTC gradient_boosted_trees.cc:1556] \tnum-trees:155 train-loss:7.074338 train-rmse:7.074338 valid-loss:8.024437 valid-rmse:8.024437\n",
            "[INFO 23-11-04 18:38:28.1649 UTC gradient_boosted_trees.cc:1556] \tnum-trees:113 train-loss:7.790308 train-rmse:7.790308 valid-loss:8.242717 valid-rmse:8.242717\n",
            "[INFO 23-11-04 18:38:58.6832 UTC gradient_boosted_trees.cc:1556] \tnum-trees:111 train-loss:8.142256 train-rmse:8.142256 valid-loss:8.315613 valid-rmse:8.315613\n",
            "[INFO 23-11-04 18:39:28.8253 UTC gradient_boosted_trees.cc:1556] \tnum-trees:276 train-loss:6.061205 train-rmse:6.061205 valid-loss:7.934410 valid-rmse:7.934410\n",
            "[INFO 23-11-04 18:39:58.8637 UTC gradient_boosted_trees.cc:1556] \tnum-trees:142 train-loss:6.880103 train-rmse:6.880103 valid-loss:8.085976 valid-rmse:8.085976\n",
            "[INFO 23-11-04 18:40:32.4395 UTC gradient_boosted_trees.cc:1556] \tnum-trees:256 train-loss:6.841368 train-rmse:6.841368 valid-loss:7.989432 valid-rmse:7.989432\n",
            "[INFO 23-11-04 18:41:04.2319 UTC gradient_boosted_trees.cc:1556] \tnum-trees:258 train-loss:6.831883 train-rmse:6.831883 valid-loss:7.984009 valid-rmse:7.984009\n",
            "[INFO 23-11-04 18:41:36.0197 UTC gradient_boosted_trees.cc:1556] \tnum-trees:260 train-loss:6.819592 train-rmse:6.819592 valid-loss:7.983201 valid-rmse:7.983201\n",
            "[INFO 23-11-04 18:42:06.7703 UTC gradient_boosted_trees.cc:1556] \tnum-trees:287 train-loss:5.996017 train-rmse:5.996017 valid-loss:7.929580 valid-rmse:7.929580\n",
            "[INFO 23-11-04 18:42:37.3975 UTC gradient_boosted_trees.cc:1556] \tnum-trees:140 train-loss:8.075894 train-rmse:8.075894 valid-loss:8.305570 valid-rmse:8.305570\n",
            "[INFO 23-11-04 18:43:07.6267 UTC gradient_boosted_trees.cc:1556] \tnum-trees:144 train-loss:8.068497 train-rmse:8.068497 valid-loss:8.307652 valid-rmse:8.307652\n",
            "[INFO 23-11-04 18:43:37.9584 UTC gradient_boosted_trees.cc:1556] \tnum-trees:148 train-loss:8.062741 train-rmse:8.062741 valid-loss:8.302156 valid-rmse:8.302156\n",
            "[INFO 23-11-04 18:44:08.1642 UTC gradient_boosted_trees.cc:1556] \tnum-trees:152 train-loss:8.058099 train-rmse:8.058099 valid-loss:8.302424 valid-rmse:8.302424\n",
            "[INFO 23-11-04 18:44:25.3177 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.724351 train-rmse:6.724351 valid-loss:7.894035 valid-rmse:7.894035\n",
            "[INFO 23-11-04 18:44:25.3177 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-04 18:44:25.3177 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.892285 valid-rmse:7.892285\n",
            "[INFO 23-11-04 18:44:25.3249 UTC hyperparameters_optimizer.cc:582] [3/100] Score: -7.89228 / -7.89228 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 64 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-04 18:44:25.3256 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 18:44:25.3256 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 18:44:25.3436 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 18:44:38.5509 UTC gradient_boosted_trees.cc:1556] \tnum-trees:156 train-loss:8.054488 train-rmse:8.054488 valid-loss:8.301462 valid-rmse:8.301462\n",
            "[INFO 23-11-04 18:44:47.8284 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.635650 train-rmse:8.635650 valid-loss:8.548719 valid-rmse:8.548719\n",
            "[INFO 23-11-04 18:45:08.7614 UTC gradient_boosted_trees.cc:1556] \tnum-trees:136 train-loss:6.700255 train-rmse:6.700255 valid-loss:7.889335 valid-rmse:7.889335\n",
            "[INFO 23-11-04 18:45:15.4716 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.917533 train-rmse:5.917533 valid-loss:7.911223 valid-rmse:7.911223\n",
            "[INFO 23-11-04 18:45:15.4717 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 18:45:15.4717 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.911223 valid-rmse:7.911223\n",
            "[INFO 23-11-04 18:45:15.4798 UTC hyperparameters_optimizer.cc:582] [4/100] Score: -7.91122 / -7.89228 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 256 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 18:45:15.4889 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 18:45:15.4889 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 18:45:15.5036 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 18:45:28.0352 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.633028 train-rmse:8.633028 valid-loss:8.549603 valid-rmse:8.549603\n",
            "[INFO 23-11-04 18:45:39.1640 UTC gradient_boosted_trees.cc:1556] \tnum-trees:164 train-loss:8.039700 train-rmse:8.039700 valid-loss:8.292337 valid-rmse:8.292337\n",
            "[INFO 23-11-04 18:46:09.4757 UTC gradient_boosted_trees.cc:1556] \tnum-trees:168 train-loss:8.032212 train-rmse:8.032212 valid-loss:8.290980 valid-rmse:8.290980\n",
            "[INFO 23-11-04 18:46:39.9844 UTC gradient_boosted_trees.cc:1556] \tnum-trees:172 train-loss:8.026679 train-rmse:8.026679 valid-loss:8.286038 valid-rmse:8.286038\n",
            "[INFO 23-11-04 18:47:10.2808 UTC gradient_boosted_trees.cc:1556] \tnum-trees:176 train-loss:8.019374 train-rmse:8.019374 valid-loss:8.283415 valid-rmse:8.283415\n",
            "[INFO 23-11-04 18:47:40.3745 UTC gradient_boosted_trees.cc:1556] \tnum-trees:158 train-loss:6.785086 train-rmse:6.785086 valid-loss:8.064328 valid-rmse:8.064328\n",
            "[INFO 23-11-04 18:48:10.9260 UTC gradient_boosted_trees.cc:1556] \tnum-trees:184 train-loss:8.002270 train-rmse:8.002270 valid-loss:8.278065 valid-rmse:8.278065\n",
            "[INFO 23-11-04 18:48:41.1172 UTC gradient_boosted_trees.cc:1556] \tnum-trees:188 train-loss:7.993312 train-rmse:7.993312 valid-loss:8.273522 valid-rmse:8.273522\n",
            "[INFO 23-11-04 18:49:11.5262 UTC gradient_boosted_trees.cc:1556] \tnum-trees:192 train-loss:7.988925 train-rmse:7.988925 valid-loss:8.273116 valid-rmse:8.273116\n",
            "[INFO 23-11-04 18:49:41.8574 UTC gradient_boosted_trees.cc:1556] \tnum-trees:196 train-loss:7.982030 train-rmse:7.982030 valid-loss:8.269124 valid-rmse:8.269124\n",
            "[INFO 23-11-04 18:50:12.0395 UTC gradient_boosted_trees.cc:1556] \tnum-trees:200 train-loss:7.975436 train-rmse:7.975436 valid-loss:8.268194 valid-rmse:8.268194\n",
            "[INFO 23-11-04 18:50:42.1428 UTC gradient_boosted_trees.cc:1556] \tnum-trees:204 train-loss:7.970193 train-rmse:7.970193 valid-loss:8.263767 valid-rmse:8.263767\n",
            "[INFO 23-11-04 18:51:12.5244 UTC gradient_boosted_trees.cc:1556] \tnum-trees:208 train-loss:7.966453 train-rmse:7.966453 valid-loss:8.259738 valid-rmse:8.259738\n",
            "[INFO 23-11-04 18:51:42.9198 UTC gradient_boosted_trees.cc:1556] \tnum-trees:212 train-loss:7.960726 train-rmse:7.960726 valid-loss:8.258504 valid-rmse:8.258504\n",
            "[INFO 23-11-04 18:52:13.4075 UTC gradient_boosted_trees.cc:1556] \tnum-trees:216 train-loss:7.954618 train-rmse:7.954618 valid-loss:8.258392 valid-rmse:8.258392\n",
            "[INFO 23-11-04 18:52:15.5186 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.662222 train-rmse:6.662222 valid-loss:7.945287 valid-rmse:7.945287\n",
            "[INFO 23-11-04 18:52:15.5186 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 18:52:15.5186 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.945287 valid-rmse:7.945287\n",
            "[INFO 23-11-04 18:52:15.5240 UTC hyperparameters_optimizer.cc:582] [5/100] Score: -7.94529 / -7.89228 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 18:52:15.5296 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 18:52:15.5296 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 18:52:15.5416 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 18:52:41.0532 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.636088 train-rmse:8.636088 valid-loss:8.547543 valid-rmse:8.547543\n",
            "[INFO 23-11-04 18:52:43.5069 UTC gradient_boosted_trees.cc:1556] \tnum-trees:220 train-loss:7.948196 train-rmse:7.948196 valid-loss:8.252983 valid-rmse:8.252983\n",
            "[INFO 23-11-04 18:53:13.7360 UTC gradient_boosted_trees.cc:1556] \tnum-trees:224 train-loss:7.942760 train-rmse:7.942760 valid-loss:8.251768 valid-rmse:8.251768\n",
            "[INFO 23-11-04 18:53:43.7910 UTC gradient_boosted_trees.cc:1556] \tnum-trees:228 train-loss:7.935809 train-rmse:7.935809 valid-loss:8.246510 valid-rmse:8.246510\n",
            "[INFO 23-11-04 18:54:13.7977 UTC gradient_boosted_trees.cc:1556] \tnum-trees:232 train-loss:7.931011 train-rmse:7.931011 valid-loss:8.245703 valid-rmse:8.245703\n",
            "[INFO 23-11-04 18:54:44.3889 UTC gradient_boosted_trees.cc:1556] \tnum-trees:236 train-loss:7.923934 train-rmse:7.923934 valid-loss:8.243748 valid-rmse:8.243748\n",
            "[INFO 23-11-04 18:55:14.8247 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:7.919518 train-rmse:7.919518 valid-loss:8.239954 valid-rmse:8.239954\n",
            "[INFO 23-11-04 18:55:45.0202 UTC gradient_boosted_trees.cc:1556] \tnum-trees:128 train-loss:7.806380 train-rmse:7.806380 valid-loss:8.232456 valid-rmse:8.232456\n",
            "[INFO 23-11-04 18:56:15.4289 UTC gradient_boosted_trees.cc:1556] \tnum-trees:248 train-loss:7.908637 train-rmse:7.908637 valid-loss:8.235016 valid-rmse:8.235016\n",
            "[INFO 23-11-04 18:56:45.8265 UTC gradient_boosted_trees.cc:1556] \tnum-trees:252 train-loss:7.903271 train-rmse:7.903271 valid-loss:8.233079 valid-rmse:8.233079\n",
            "[INFO 23-11-04 18:57:16.0993 UTC gradient_boosted_trees.cc:1556] \tnum-trees:256 train-loss:7.899188 train-rmse:7.899188 valid-loss:8.234378 valid-rmse:8.234378\n",
            "[INFO 23-11-04 18:57:46.6427 UTC gradient_boosted_trees.cc:1556] \tnum-trees:260 train-loss:7.893208 train-rmse:7.893208 valid-loss:8.231318 valid-rmse:8.231318\n",
            "[INFO 23-11-04 18:58:17.0523 UTC gradient_boosted_trees.cc:1556] \tnum-trees:264 train-loss:7.888651 train-rmse:7.888651 valid-loss:8.228183 valid-rmse:8.228183\n",
            "[INFO 23-11-04 18:58:47.5989 UTC gradient_boosted_trees.cc:1556] \tnum-trees:189 train-loss:7.077298 train-rmse:7.077298 valid-loss:8.049654 valid-rmse:8.049654\n",
            "[INFO 23-11-04 18:59:21.6701 UTC gradient_boosted_trees.cc:1556] \tnum-trees:266 train-loss:7.098280 train-rmse:7.098280 valid-loss:8.009019 valid-rmse:8.009019\n",
            "[INFO 23-11-04 18:59:52.7435 UTC gradient_boosted_trees.cc:1556] \tnum-trees:41 train-loss:8.188167 train-rmse:8.188167 valid-loss:8.369795 valid-rmse:8.369795\n",
            "[INFO 23-11-04 19:00:23.7712 UTC gradient_boosted_trees.cc:1556] \tnum-trees:185 train-loss:7.195491 train-rmse:7.195491 valid-loss:8.008531 valid-rmse:8.008531\n",
            "[INFO 23-11-04 19:00:54.4471 UTC gradient_boosted_trees.cc:1556] \tnum-trees:285 train-loss:7.866506 train-rmse:7.866506 valid-loss:8.227126 valid-rmse:8.227126\n",
            "[INFO 23-11-04 19:01:24.5072 UTC gradient_boosted_trees.cc:1556] \tnum-trees:289 train-loss:7.860608 train-rmse:7.860608 valid-loss:8.221163 valid-rmse:8.221163\n",
            "[INFO 23-11-04 19:01:54.6710 UTC gradient_boosted_trees.cc:1556] \tnum-trees:293 train-loss:7.855803 train-rmse:7.855803 valid-loss:8.218671 valid-rmse:8.218671\n",
            "[INFO 23-11-04 19:02:24.7891 UTC gradient_boosted_trees.cc:1556] \tnum-trees:297 train-loss:7.850603 train-rmse:7.850603 valid-loss:8.213674 valid-rmse:8.213674\n",
            "[INFO 23-11-04 19:02:47.3311 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.846620 train-rmse:7.846620 valid-loss:8.208583 valid-rmse:8.208583\n",
            "[INFO 23-11-04 19:02:47.3311 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 19:02:47.3311 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.208583 valid-rmse:8.208583\n",
            "[INFO 23-11-04 19:02:47.3327 UTC hyperparameters_optimizer.cc:582] [6/100] Score: -8.20858 / -7.89228 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 19:02:47.3347 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 19:02:47.3347 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 19:02:47.3473 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 19:02:54.9402 UTC gradient_boosted_trees.cc:1556] \tnum-trees:49 train-loss:8.132838 train-rmse:8.132838 valid-loss:8.353322 valid-rmse:8.353322\n",
            "[INFO 23-11-04 19:03:12.2181 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.637896 train-rmse:8.637896 valid-loss:8.548444 valid-rmse:8.548444\n",
            "[INFO 23-11-04 19:03:27.8248 UTC gradient_boosted_trees.cc:1556] \tnum-trees:151 train-loss:7.722929 train-rmse:7.722929 valid-loss:8.212057 valid-rmse:8.212057\n",
            "[INFO 23-11-04 19:04:01.6898 UTC gradient_boosted_trees.cc:1556] \tnum-trees:3 train-loss:8.612004 train-rmse:8.612004 valid-loss:8.535036 valid-rmse:8.535036\n",
            "[INFO 23-11-04 19:04:34.7798 UTC gradient_boosted_trees.cc:1556] \tnum-trees:93 train-loss:8.033383 train-rmse:8.033383 valid-loss:8.288532 valid-rmse:8.288532\n",
            "[INFO 23-11-04 19:05:04.9165 UTC gradient_boosted_trees.cc:1556] \tnum-trees:286 train-loss:7.041325 train-rmse:7.041325 valid-loss:7.998927 valid-rmse:7.998927\n",
            "[INFO 23-11-04 19:05:37.3685 UTC gradient_boosted_trees.cc:1556] \tnum-trees:98 train-loss:8.016815 train-rmse:8.016815 valid-loss:8.279883 valid-rmse:8.279883\n",
            "[INFO 23-11-04 19:06:07.4964 UTC gradient_boosted_trees.cc:1556] \tnum-trees:205 train-loss:6.992349 train-rmse:6.992349 valid-loss:8.022715 valid-rmse:8.022715\n",
            "[INFO 23-11-04 19:06:37.9044 UTC gradient_boosted_trees.cc:1556] \tnum-trees:198 train-loss:7.133011 train-rmse:7.133011 valid-loss:7.991148 valid-rmse:7.991148\n",
            "[INFO 23-11-04 19:07:08.4545 UTC gradient_boosted_trees.cc:1556] \tnum-trees:162 train-loss:7.680506 train-rmse:7.680506 valid-loss:8.202345 valid-rmse:8.202345\n",
            "[INFO 23-11-04 19:07:43.6556 UTC gradient_boosted_trees.cc:1556] \tnum-trees:108 train-loss:7.988704 train-rmse:7.988704 valid-loss:8.251881 valid-rmse:8.251881\n",
            "[INFO 23-11-04 19:08:19.2428 UTC gradient_boosted_trees.cc:1556] \tnum-trees:201 train-loss:6.514979 train-rmse:6.514979 valid-loss:8.014337 valid-rmse:8.014337\n",
            "[INFO 23-11-04 19:08:49.8105 UTC gradient_boosted_trees.cc:1556] \tnum-trees:167 train-loss:7.666850 train-rmse:7.666850 valid-loss:8.198656 valid-rmse:8.198656\n",
            "[INFO 23-11-04 19:09:06.3749 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.004332 train-rmse:7.004332 valid-loss:7.989278 valid-rmse:7.989278\n",
            "[INFO 23-11-04 19:09:06.3749 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-04 19:09:06.3750 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.987954 valid-rmse:7.987954\n",
            "[INFO 23-11-04 19:09:06.3784 UTC hyperparameters_optimizer.cc:582] [7/100] Score: -7.98795 / -7.89228 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 16 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-04 19:09:06.3831 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 19:09:06.3831 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 19:09:06.3949 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 19:09:20.6841 UTC gradient_boosted_trees.cc:1556] \tnum-trees:228 train-loss:6.759083 train-rmse:6.759083 valid-loss:7.926208 valid-rmse:7.926208\n",
            "[INFO 23-11-04 19:09:30.8293 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.615593 train-rmse:8.615593 valid-loss:8.538013 valid-rmse:8.538013\n",
            "[INFO 23-11-04 19:09:55.1254 UTC gradient_boosted_trees.cc:1556] \tnum-trees:2 train-loss:8.583238 train-rmse:8.583238 valid-loss:8.522180 valid-rmse:8.522180\n",
            "[INFO 23-11-04 19:10:26.5191 UTC gradient_boosted_trees.cc:1556] \tnum-trees:121 train-loss:7.951631 train-rmse:7.951631 valid-loss:8.228824 valid-rmse:8.228824\n",
            "[INFO 23-11-04 19:10:58.8577 UTC gradient_boosted_trees.cc:1556] \tnum-trees:207 train-loss:7.101933 train-rmse:7.101933 valid-loss:7.977679 valid-rmse:7.977679\n",
            "[INFO 23-11-04 19:11:28.9681 UTC gradient_boosted_trees.cc:1556] \tnum-trees:126 train-loss:7.940910 train-rmse:7.940910 valid-loss:8.225501 valid-rmse:8.225501\n",
            "[INFO 23-11-04 19:11:59.8488 UTC gradient_boosted_trees.cc:1556] \tnum-trees:73 train-loss:7.992163 train-rmse:7.992163 valid-loss:8.307618 valid-rmse:8.307618\n",
            "[INFO 23-11-04 19:12:31.6725 UTC gradient_boosted_trees.cc:1556] \tnum-trees:131 train-loss:7.929131 train-rmse:7.929131 valid-loss:8.219696 valid-rmse:8.219696\n",
            "[INFO 23-11-04 19:13:02.8566 UTC gradient_boosted_trees.cc:1556] \tnum-trees:220 train-loss:6.931992 train-rmse:6.931992 valid-loss:7.994739 valid-rmse:7.994739\n",
            "[INFO 23-11-04 19:13:33.5288 UTC gradient_boosted_trees.cc:1556] \tnum-trees:11 train-loss:8.346753 train-rmse:8.346753 valid-loss:8.422944 valid-rmse:8.422944\n",
            "[INFO 23-11-04 19:14:04.2696 UTC gradient_boosted_trees.cc:1556] \tnum-trees:239 train-loss:6.717829 train-rmse:6.717829 valid-loss:7.913112 valid-rmse:7.913112\n",
            "[INFO 23-11-04 19:14:34.7373 UTC gradient_boosted_trees.cc:1556] \tnum-trees:214 train-loss:6.448399 train-rmse:6.448399 valid-loss:8.016348 valid-rmse:8.016348\n",
            "[INFO 23-11-04 19:15:06.3579 UTC gradient_boosted_trees.cc:1556] \tnum-trees:174 train-loss:7.605000 train-rmse:7.605000 valid-loss:8.192265 valid-rmse:8.192265\n",
            "[INFO 23-11-04 19:15:37.0272 UTC gradient_boosted_trees.cc:1556] \tnum-trees:31 train-loss:8.338153 train-rmse:8.338153 valid-loss:8.412847 valid-rmse:8.412847\n",
            "[INFO 23-11-04 19:16:09.8090 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:7.934365 train-rmse:7.934365 valid-loss:8.287273 valid-rmse:8.287273\n",
            "[INFO 23-11-04 19:16:41.5990 UTC gradient_boosted_trees.cc:1556] \tnum-trees:151 train-loss:7.877599 train-rmse:7.877599 valid-loss:8.196206 valid-rmse:8.196206\n",
            "[INFO 23-11-04 19:17:12.4150 UTC gradient_boosted_trees.cc:1556] \tnum-trees:20 train-loss:8.182142 train-rmse:8.182142 valid-loss:8.365817 valid-rmse:8.365817\n",
            "[INFO 23-11-04 19:17:42.5591 UTC gradient_boosted_trees.cc:1556] \tnum-trees:221 train-loss:7.057428 train-rmse:7.057428 valid-loss:7.967720 valid-rmse:7.967720\n",
            "[INFO 23-11-04 19:18:15.4542 UTC gradient_boosted_trees.cc:1556] \tnum-trees:195 train-loss:7.576582 train-rmse:7.576582 valid-loss:8.166557 valid-rmse:8.166557\n",
            "[INFO 23-11-04 19:18:46.3056 UTC gradient_boosted_trees.cc:1556] \tnum-trees:180 train-loss:7.592577 train-rmse:7.592577 valid-loss:8.186295 valid-rmse:8.186295\n",
            "[INFO 23-11-04 19:19:21.2420 UTC gradient_boosted_trees.cc:1556] \tnum-trees:40 train-loss:8.278449 train-rmse:8.278449 valid-loss:8.394317 valid-rmse:8.394317\n",
            "[INFO 23-11-04 19:19:51.2494 UTC gradient_boosted_trees.cc:1556] \tnum-trees:166 train-loss:7.847631 train-rmse:7.847631 valid-loss:8.175250 valid-rmse:8.175250\n",
            "[INFO 23-11-04 19:20:21.4489 UTC gradient_boosted_trees.cc:1556] \tnum-trees:95 train-loss:7.887681 train-rmse:7.887681 valid-loss:8.271592 valid-rmse:8.271592\n",
            "[INFO 23-11-04 19:20:53.3960 UTC gradient_boosted_trees.cc:1556] \tnum-trees:67 train-loss:8.032334 train-rmse:8.032334 valid-loss:8.321557 valid-rmse:8.321557\n",
            "[INFO 23-11-04 19:21:26.2113 UTC gradient_boosted_trees.cc:1556] \tnum-trees:45 train-loss:8.250881 train-rmse:8.250881 valid-loss:8.383252 valid-rmse:8.383252\n",
            "[INFO 23-11-04 19:21:57.6780 UTC gradient_boosted_trees.cc:1556] \tnum-trees:206 train-loss:7.538461 train-rmse:7.538461 valid-loss:8.151606 valid-rmse:8.151606\n",
            "[INFO 23-11-04 19:22:27.9584 UTC gradient_boosted_trees.cc:1556] \tnum-trees:33 train-loss:8.016081 train-rmse:8.016081 valid-loss:8.312171 valid-rmse:8.312171\n",
            "[INFO 23-11-04 19:22:58.6015 UTC gradient_boosted_trees.cc:1556] \tnum-trees:232 train-loss:7.015934 train-rmse:7.015934 valid-loss:7.958347 valid-rmse:7.958347\n",
            "[INFO 23-11-04 19:23:28.8441 UTC gradient_boosted_trees.cc:1556] \tnum-trees:73 train-loss:8.003095 train-rmse:8.003095 valid-loss:8.309872 valid-rmse:8.309872\n",
            "[INFO 23-11-04 19:24:00.0543 UTC gradient_boosted_trees.cc:1556] \tnum-trees:212 train-loss:7.521605 train-rmse:7.521605 valid-loss:8.146147 valid-rmse:8.146147\n",
            "[INFO 23-11-04 19:24:32.2474 UTC gradient_boosted_trees.cc:1556] \tnum-trees:106 train-loss:7.838361 train-rmse:7.838361 valid-loss:8.255637 valid-rmse:8.255637\n",
            "[INFO 23-11-04 19:25:04.5549 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:6.829934 train-rmse:6.829934 valid-loss:7.970947 valid-rmse:7.970947\n",
            "[INFO 23-11-04 19:25:35.3347 UTC gradient_boosted_trees.cc:1556] \tnum-trees:266 train-loss:6.608312 train-rmse:6.608312 valid-loss:7.885808 valid-rmse:7.885808\n",
            "[INFO 23-11-04 19:26:05.6667 UTC gradient_boosted_trees.cc:1556] \tnum-trees:238 train-loss:6.335962 train-rmse:6.335962 valid-loss:7.988601 valid-rmse:7.988601\n",
            "[INFO 23-11-04 19:26:36.9538 UTC gradient_boosted_trees.cc:1556] \tnum-trees:213 train-loss:6.227410 train-rmse:6.227410 valid-loss:7.805857 valid-rmse:7.805857\n",
            "[INFO 23-11-04 19:27:09.8494 UTC gradient_boosted_trees.cc:1556] \tnum-trees:214 train-loss:6.225192 train-rmse:6.225192 valid-loss:7.804872 valid-rmse:7.804872\n",
            "[INFO 23-11-04 19:27:40.2097 UTC gradient_boosted_trees.cc:1556] \tnum-trees:223 train-loss:7.486793 train-rmse:7.486793 valid-loss:8.126366 valid-rmse:8.126366\n",
            "[INFO 23-11-04 19:28:10.9941 UTC gradient_boosted_trees.cc:1556] \tnum-trees:47 train-loss:7.873321 train-rmse:7.873321 valid-loss:8.251422 valid-rmse:8.251422\n",
            "[INFO 23-11-04 19:28:42.0536 UTC gradient_boosted_trees.cc:1556] \tnum-trees:208 train-loss:7.760487 train-rmse:7.760487 valid-loss:8.137363 valid-rmse:8.137363\n",
            "[INFO 23-11-04 19:29:12.8433 UTC gradient_boosted_trees.cc:1556] \tnum-trees:245 train-loss:6.970286 train-rmse:6.970286 valid-loss:7.940776 valid-rmse:7.940776\n",
            "[INFO 23-11-04 19:29:45.4331 UTC gradient_boosted_trees.cc:1556] \tnum-trees:213 train-loss:7.746669 train-rmse:7.746669 valid-loss:8.129068 valid-rmse:8.129068\n",
            "[INFO 23-11-04 19:30:17.2371 UTC gradient_boosted_trees.cc:1556] \tnum-trees:199 train-loss:7.536774 train-rmse:7.536774 valid-loss:8.167993 valid-rmse:8.167993\n",
            "[INFO 23-11-04 19:30:47.4362 UTC gradient_boosted_trees.cc:1556] \tnum-trees:90 train-loss:7.913908 train-rmse:7.913908 valid-loss:8.270587 valid-rmse:8.270587\n",
            "[INFO 23-11-04 19:31:21.2860 UTC gradient_boosted_trees.cc:1556] \tnum-trees:234 train-loss:7.455519 train-rmse:7.455519 valid-loss:8.117093 valid-rmse:8.117093\n",
            "[INFO 23-11-04 19:31:51.8964 UTC gradient_boosted_trees.cc:1556] \tnum-trees:250 train-loss:6.279593 train-rmse:6.279593 valid-loss:7.980292 valid-rmse:7.980292\n",
            "[INFO 23-11-04 19:32:22.4046 UTC gradient_boosted_trees.cc:1556] \tnum-trees:237 train-loss:7.447616 train-rmse:7.447616 valid-loss:8.114087 valid-rmse:8.114087\n",
            "[INFO 23-11-04 19:32:53.0180 UTC gradient_boosted_trees.cc:1556] \tnum-trees:228 train-loss:7.711579 train-rmse:7.711579 valid-loss:8.113204 valid-rmse:8.113204\n",
            "[INFO 23-11-04 19:33:23.8606 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:7.437537 train-rmse:7.437537 valid-loss:8.109809 valid-rmse:8.109809\n",
            "[INFO 23-11-04 19:33:55.4379 UTC gradient_boosted_trees.cc:1556] \tnum-trees:233 train-loss:7.701257 train-rmse:7.701257 valid-loss:8.109430 valid-rmse:8.109430\n",
            "[INFO 23-11-04 19:34:26.0902 UTC gradient_boosted_trees.cc:1556] \tnum-trees:76 train-loss:8.114829 train-rmse:8.114829 valid-loss:8.332364 valid-rmse:8.332364\n",
            "[INFO 23-11-04 19:34:57.1660 UTC gradient_boosted_trees.cc:1556] \tnum-trees:238 train-loss:7.689145 train-rmse:7.689145 valid-loss:8.106927 valid-rmse:8.106927\n",
            "[INFO 23-11-04 19:35:28.1609 UTC gradient_boosted_trees.cc:1556] \tnum-trees:258 train-loss:6.925992 train-rmse:6.925992 valid-loss:7.929528 valid-rmse:7.929528\n",
            "[INFO 23-11-04 19:35:59.1299 UTC gradient_boosted_trees.cc:1556] \tnum-trees:136 train-loss:7.707439 train-rmse:7.707439 valid-loss:8.204071 valid-rmse:8.204071\n",
            "[INFO 23-11-04 19:36:31.9042 UTC gradient_boosted_trees.cc:1556] \tnum-trees:81 train-loss:8.098121 train-rmse:8.098121 valid-loss:8.328874 valid-rmse:8.328874\n",
            "[INFO 23-11-04 19:37:03.1129 UTC gradient_boosted_trees.cc:1556] \tnum-trees:248 train-loss:7.667673 train-rmse:7.667673 valid-loss:8.095008 valid-rmse:8.095008\n",
            "[INFO 23-11-04 19:37:35.6950 UTC gradient_boosted_trees.cc:1556] \tnum-trees:273 train-loss:6.723996 train-rmse:6.723996 valid-loss:7.934327 valid-rmse:7.934327\n",
            "[INFO 23-11-04 19:38:07.0895 UTC gradient_boosted_trees.cc:1556] \tnum-trees:253 train-loss:7.657340 train-rmse:7.657340 valid-loss:8.088205 valid-rmse:8.088205\n",
            "[INFO 23-11-04 19:38:38.5276 UTC gradient_boosted_trees.cc:1556] \tnum-trees:86 train-loss:8.080310 train-rmse:8.080310 valid-loss:8.326507 valid-rmse:8.326507\n",
            "[INFO 23-11-04 19:39:09.2939 UTC gradient_boosted_trees.cc:1556] \tnum-trees:258 train-loss:7.647142 train-rmse:7.647142 valid-loss:8.085340 valid-rmse:8.085340\n",
            "[INFO 23-11-04 19:39:44.2716 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:7.651122 train-rmse:7.651122 valid-loss:8.157934 valid-rmse:8.157934\n",
            "[INFO 23-11-04 19:40:13.8486 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.482693 train-rmse:6.482693 valid-loss:7.851069 valid-rmse:7.851069\n",
            "[INFO 23-11-04 19:40:13.8486 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 19:40:13.8486 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.851069 valid-rmse:7.851069\n",
            "[INFO 23-11-04 19:40:13.8567 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 19:40:13.8567 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 19:40:13.8605 UTC hyperparameters_optimizer.cc:582] [8/100] Score: -7.85107 / -7.85107 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 19:40:13.8695 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 19:40:16.5907 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:6.896906 train-rmse:6.896906 valid-loss:7.925319 valid-rmse:7.925319\n",
            "[INFO 23-11-04 19:40:40.0894 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.613931 train-rmse:8.613931 valid-loss:8.543895 valid-rmse:8.543895\n",
            "[INFO 23-11-04 19:40:49.1922 UTC gradient_boosted_trees.cc:1556] \tnum-trees:266 train-loss:7.634274 train-rmse:7.634274 valid-loss:8.078773 valid-rmse:8.078773\n",
            "[INFO 23-11-04 19:41:19.5547 UTC gradient_boosted_trees.cc:1556] \tnum-trees:281 train-loss:6.697918 train-rmse:6.697918 valid-loss:7.923017 valid-rmse:7.923017\n",
            "[INFO 23-11-04 19:41:49.9120 UTC gradient_boosted_trees.cc:1556] \tnum-trees:265 train-loss:7.370477 train-rmse:7.370477 valid-loss:8.083015 valid-rmse:8.083015\n",
            "[INFO 23-11-04 19:42:25.5688 UTC gradient_boosted_trees.cc:1556] \tnum-trees:5 train-loss:8.478710 train-rmse:8.478710 valid-loss:8.483535 valid-rmse:8.483535\n",
            "[INFO 23-11-04 19:42:59.8647 UTC gradient_boosted_trees.cc:1556] \tnum-trees:273 train-loss:6.164566 train-rmse:6.164566 valid-loss:7.967083 valid-rmse:7.967083\n",
            "[INFO 23-11-04 19:43:31.0577 UTC gradient_boosted_trees.cc:1556] \tnum-trees:270 train-loss:7.357835 train-rmse:7.357835 valid-loss:8.081300 valid-rmse:8.081300\n",
            "[INFO 23-11-04 19:44:03.3860 UTC gradient_boosted_trees.cc:1556] \tnum-trees:222 train-loss:7.470189 train-rmse:7.470189 valid-loss:8.145533 valid-rmse:8.145533\n",
            "[INFO 23-11-04 19:44:34.4610 UTC gradient_boosted_trees.cc:1556] \tnum-trees:277 train-loss:6.866130 train-rmse:6.866130 valid-loss:7.918931 valid-rmse:7.918931\n",
            "[INFO 23-11-04 19:45:05.1974 UTC gradient_boosted_trees.cc:1556] \tnum-trees:11 train-loss:8.322512 train-rmse:8.322512 valid-loss:8.415870 valid-rmse:8.415870\n",
            "[INFO 23-11-04 19:45:37.1903 UTC gradient_boosted_trees.cc:1556] \tnum-trees:289 train-loss:7.595890 train-rmse:7.595890 valid-loss:8.067369 valid-rmse:8.067369\n",
            "[INFO 23-11-04 19:46:14.0642 UTC gradient_boosted_trees.cc:1556] \tnum-trees:104 train-loss:8.025878 train-rmse:8.025878 valid-loss:8.307921 valid-rmse:8.307921\n",
            "[INFO 23-11-04 19:46:46.0472 UTC gradient_boosted_trees.cc:1556] \tnum-trees:92 train-loss:7.546507 train-rmse:7.546507 valid-loss:8.125485 valid-rmse:8.125485\n",
            "[INFO 23-11-04 19:47:16.2509 UTC gradient_boosted_trees.cc:1556] \tnum-trees:294 train-loss:6.646548 train-rmse:6.646548 valid-loss:7.916438 valid-rmse:7.916438\n",
            "[INFO 23-11-04 19:47:49.7057 UTC gradient_boosted_trees.cc:1556] \tnum-trees:283 train-loss:6.117473 train-rmse:6.117473 valid-loss:7.962738 valid-rmse:7.962738\n",
            "[INFO 23-11-04 19:47:54.1234 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.576226 train-rmse:7.576226 valid-loss:8.061547 valid-rmse:8.061547\n",
            "[INFO 23-11-04 19:47:54.1234 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 19:47:54.1234 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.061547 valid-rmse:8.061547\n",
            "[INFO 23-11-04 19:47:54.1247 UTC hyperparameters_optimizer.cc:582] [9/100] Score: -8.06155 / -7.85107 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-04 19:47:54.1272 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 19:47:54.1272 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 19:47:54.1390 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 19:48:07.3172 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.643236 train-rmse:8.643236 valid-loss:8.554326 valid-rmse:8.554326\n",
            "[INFO 23-11-04 19:48:20.4391 UTC gradient_boosted_trees.cc:1556] \tnum-trees:2 train-loss:8.634416 train-rmse:8.634416 valid-loss:8.549934 valid-rmse:8.549934\n",
            "[INFO 23-11-04 19:48:50.5270 UTC gradient_boosted_trees.cc:1556] \tnum-trees:97 train-loss:7.520515 train-rmse:7.520515 valid-loss:8.117842 valid-rmse:8.117842\n",
            "[INFO 23-11-04 19:49:20.5296 UTC gradient_boosted_trees.cc:1556] \tnum-trees:287 train-loss:6.829717 train-rmse:6.829717 valid-loss:7.909281 valid-rmse:7.909281\n",
            "[INFO 23-11-04 19:49:52.0084 UTC gradient_boosted_trees.cc:1556] \tnum-trees:9 train-loss:8.577468 train-rmse:8.577468 valid-loss:8.525738 valid-rmse:8.525738\n",
            "[INFO 23-11-04 19:50:00.4563 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.624436 train-rmse:6.624436 valid-loss:7.915017 valid-rmse:7.915017\n",
            "[INFO 23-11-04 19:50:00.4563 UTC gradient_boosted_trees.cc:249] Truncates the model to 297 tree(s) i.e. 297  iteration(s).\n",
            "[INFO 23-11-04 19:50:00.4564 UTC gradient_boosted_trees.cc:312] Final model num-trees:297 valid-loss:7.913866 valid-rmse:7.913866\n",
            "[INFO 23-11-04 19:50:00.4621 UTC hyperparameters_optimizer.cc:582] [10/100] Score: -7.91387 / -7.85107 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 64 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-04 19:50:00.4631 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 19:50:00.4631 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 19:50:00.4843 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 19:50:25.8802 UTC gradient_boosted_trees.cc:1556] \tnum-trees:23 train-loss:8.111461 train-rmse:8.111461 valid-loss:8.337708 valid-rmse:8.337708\n",
            "[INFO 23-11-04 19:50:27.2827 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.589849 train-rmse:8.589849 valid-loss:8.515243 valid-rmse:8.515243\n",
            "[INFO 23-11-04 19:50:55.9660 UTC gradient_boosted_trees.cc:1556] \tnum-trees:2 train-loss:8.530205 train-rmse:8.530205 valid-loss:8.495149 valid-rmse:8.495149\n",
            "[INFO 23-11-04 19:51:32.5433 UTC gradient_boosted_trees.cc:1556] \tnum-trees:138 train-loss:7.717352 train-rmse:7.717352 valid-loss:8.207325 valid-rmse:8.207325\n",
            "[INFO 23-11-04 19:52:02.9110 UTC gradient_boosted_trees.cc:1556] \tnum-trees:19 train-loss:8.511143 train-rmse:8.511143 valid-loss:8.504246 valid-rmse:8.504246\n",
            "[INFO 23-11-04 19:52:37.1792 UTC gradient_boosted_trees.cc:1556] \tnum-trees:119 train-loss:7.987705 train-rmse:7.987705 valid-loss:8.296516 valid-rmse:8.296516\n",
            "[INFO 23-11-04 19:53:08.0286 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:6.797732 train-rmse:6.797732 valid-loss:7.904619 valid-rmse:7.904619\n",
            "[INFO 23-11-04 19:53:38.4237 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:6.068280 train-rmse:6.068280 valid-loss:7.952633 valid-rmse:7.952633\n",
            "[INFO 23-11-04 19:53:38.6089 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.291032 train-rmse:7.291032 valid-loss:8.064582 valid-rmse:8.064582\n",
            "[INFO 23-11-04 19:53:38.6089 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 19:53:38.6089 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.064582 valid-rmse:8.064582\n",
            "[INFO 23-11-04 19:53:38.6140 UTC hyperparameters_optimizer.cc:582] [11/100] Score: -8.06458 / -7.85107 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 19:53:38.6205 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 19:53:38.6205 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 19:53:38.6314 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 19:53:50.6241 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.639825 train-rmse:8.639825 valid-loss:8.549803 valid-rmse:8.549803\n",
            "[INFO 23-11-04 19:54:10.7271 UTC gradient_boosted_trees.cc:1556] \tnum-trees:239 train-loss:7.413693 train-rmse:7.413693 valid-loss:8.123881 valid-rmse:8.123881\n",
            "[INFO 23-11-04 19:54:44.9435 UTC gradient_boosted_trees.cc:1556] \tnum-trees:124 train-loss:7.973344 train-rmse:7.973344 valid-loss:8.290916 valid-rmse:8.290916\n",
            "[INFO 23-11-04 19:55:15.8293 UTC gradient_boosted_trees.cc:1556] \tnum-trees:8 train-loss:8.556351 train-rmse:8.556351 valid-loss:8.515800 valid-rmse:8.515800\n",
            "[INFO 23-11-04 19:55:32.2370 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.783607 train-rmse:6.783607 valid-loss:7.903327 valid-rmse:7.903327\n",
            "[INFO 23-11-04 19:55:32.2463 UTC gradient_boosted_trees.cc:249] Truncates the model to 298 tree(s) i.e. 298  iteration(s).\n",
            "[INFO 23-11-04 19:55:32.2487 UTC gradient_boosted_trees.cc:312] Final model num-trees:298 valid-loss:7.903126 valid-rmse:7.903126\n",
            "[INFO 23-11-04 19:55:32.2527 UTC hyperparameters_optimizer.cc:582] [12/100] Score: -7.90313 / -7.85107 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 128 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 19:55:32.2533 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 19:55:32.2533 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 19:55:32.2765 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 19:55:47.9164 UTC gradient_boosted_trees.cc:1556] \tnum-trees:35 train-loss:7.955022 train-rmse:7.955022 valid-loss:8.272326 valid-rmse:8.272326\n",
            "[INFO 23-11-04 19:55:51.8065 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.582994 train-rmse:8.582994 valid-loss:8.524734 valid-rmse:8.524734\n",
            "[INFO 23-11-04 19:56:01.7638 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.043690 train-rmse:6.043690 valid-loss:7.944935 valid-rmse:7.944935\n",
            "[INFO 23-11-04 19:56:01.7638 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-04 19:56:01.7638 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.943418 valid-rmse:7.943418\n",
            "[INFO 23-11-04 19:56:01.7690 UTC hyperparameters_optimizer.cc:582] [13/100] Score: -7.94342 / -7.85107 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 64 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-04 19:56:01.7764 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 19:56:01.7764 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 19:56:01.7884 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 19:56:16.2702 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.588611 train-rmse:8.588611 valid-loss:8.521831 valid-rmse:8.521831\n",
            "[INFO 23-11-04 19:56:18.2904 UTC gradient_boosted_trees.cc:1556] \tnum-trees:189 train-loss:7.516595 train-rmse:7.516595 valid-loss:8.138882 valid-rmse:8.138882\n",
            "[INFO 23-11-04 19:56:48.4673 UTC gradient_boosted_trees.cc:1556] \tnum-trees:15 train-loss:8.081298 train-rmse:8.081298 valid-loss:8.304135 valid-rmse:8.304135\n",
            "[INFO 23-11-04 19:57:18.9424 UTC gradient_boosted_trees.cc:1556] \tnum-trees:130 train-loss:7.958207 train-rmse:7.958207 valid-loss:8.286207 valid-rmse:8.286207\n",
            "[INFO 23-11-04 19:57:49.0249 UTC gradient_boosted_trees.cc:1556] \tnum-trees:7 train-loss:8.304444 train-rmse:8.304444 valid-loss:8.399659 valid-rmse:8.399659\n",
            "[INFO 23-11-04 19:58:19.4888 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:7.390760 train-rmse:7.390760 valid-loss:8.114608 valid-rmse:8.114608\n",
            "[INFO 23-11-04 19:58:49.8315 UTC gradient_boosted_trees.cc:1556] \tnum-trees:50 train-loss:8.383296 train-rmse:8.383296 valid-loss:8.452607 valid-rmse:8.452607\n",
            "[INFO 23-11-04 19:59:24.1278 UTC gradient_boosted_trees.cc:1556] \tnum-trees:43 train-loss:7.887215 train-rmse:7.887215 valid-loss:8.261402 valid-rmse:8.261402\n",
            "[INFO 23-11-04 19:59:55.3521 UTC gradient_boosted_trees.cc:1556] \tnum-trees:55 train-loss:8.369102 train-rmse:8.369102 valid-loss:8.446505 valid-rmse:8.446505\n",
            "[INFO 23-11-04 20:00:25.9476 UTC gradient_boosted_trees.cc:1556] \tnum-trees:15 train-loss:8.074580 train-rmse:8.074580 valid-loss:8.354700 valid-rmse:8.354700\n",
            "[INFO 23-11-04 20:00:56.5368 UTC gradient_boosted_trees.cc:1556] \tnum-trees:201 train-loss:7.473023 train-rmse:7.473023 valid-loss:8.124523 valid-rmse:8.124523\n",
            "[INFO 23-11-04 20:01:27.0436 UTC gradient_boosted_trees.cc:1556] \tnum-trees:62 train-loss:8.351652 train-rmse:8.351652 valid-loss:8.437219 valid-rmse:8.437219\n",
            "[INFO 23-11-04 20:01:59.5204 UTC gradient_boosted_trees.cc:1556] \tnum-trees:41 train-loss:8.303946 train-rmse:8.303946 valid-loss:8.405395 valid-rmse:8.405395\n",
            "[INFO 23-11-04 20:02:29.5403 UTC gradient_boosted_trees.cc:1556] \tnum-trees:205 train-loss:7.462257 train-rmse:7.462257 valid-loss:8.122552 valid-rmse:8.122552\n",
            "[INFO 23-11-04 20:03:01.0720 UTC gradient_boosted_trees.cc:1556] \tnum-trees:51 train-loss:7.823601 train-rmse:7.823601 valid-loss:8.251594 valid-rmse:8.251594\n",
            "[INFO 23-11-04 20:03:37.0356 UTC gradient_boosted_trees.cc:1556] \tnum-trees:145 train-loss:7.913320 train-rmse:7.913320 valid-loss:8.264357 valid-rmse:8.264357\n",
            "[INFO 23-11-04 20:04:07.6823 UTC gradient_boosted_trees.cc:1556] \tnum-trees:31 train-loss:7.807835 train-rmse:7.807835 valid-loss:8.210426 valid-rmse:8.210426\n",
            "[INFO 23-11-04 20:04:40.0476 UTC gradient_boosted_trees.cc:1556] \tnum-trees:54 train-loss:8.238437 train-rmse:8.238437 valid-loss:8.377560 valid-rmse:8.377560\n",
            "[INFO 23-11-04 20:05:10.1625 UTC gradient_boosted_trees.cc:1556] \tnum-trees:79 train-loss:8.311751 train-rmse:8.311751 valid-loss:8.419950 valid-rmse:8.419950\n",
            "[INFO 23-11-04 20:05:41.5039 UTC gradient_boosted_trees.cc:1556] \tnum-trees:59 train-loss:8.216849 train-rmse:8.216849 valid-loss:8.371303 valid-rmse:8.371303\n",
            "[INFO 23-11-04 20:06:12.8930 UTC gradient_boosted_trees.cc:1556] \tnum-trees:139 train-loss:7.304070 train-rmse:7.304070 valid-loss:8.034245 valid-rmse:8.034245\n",
            "[INFO 23-11-04 20:06:42.9565 UTC gradient_boosted_trees.cc:1556] \tnum-trees:64 train-loss:8.196707 train-rmse:8.196707 valid-loss:8.365427 valid-rmse:8.365427\n",
            "[INFO 23-11-04 20:07:15.8696 UTC gradient_boosted_trees.cc:1556] \tnum-trees:174 train-loss:7.604267 train-rmse:7.604267 valid-loss:8.159730 valid-rmse:8.159730\n",
            "[INFO 23-11-04 20:07:46.4583 UTC gradient_boosted_trees.cc:1556] \tnum-trees:91 train-loss:8.285216 train-rmse:8.285216 valid-loss:8.408814 valid-rmse:8.408814\n",
            "[INFO 23-11-04 20:08:17.6813 UTC gradient_boosted_trees.cc:1556] \tnum-trees:220 train-loss:7.418929 train-rmse:7.418929 valid-loss:8.110315 valid-rmse:8.110315\n",
            "[INFO 23-11-04 20:08:51.8713 UTC gradient_boosted_trees.cc:1556] \tnum-trees:96 train-loss:8.275203 train-rmse:8.275203 valid-loss:8.402277 valid-rmse:8.402277\n",
            "[INFO 23-11-04 20:09:23.3585 UTC gradient_boosted_trees.cc:1556] \tnum-trees:77 train-loss:8.147243 train-rmse:8.147243 valid-loss:8.350845 valid-rmse:8.350845\n",
            "[INFO 23-11-04 20:09:53.5161 UTC gradient_boosted_trees.cc:1556] \tnum-trees:160 train-loss:7.869014 train-rmse:7.869014 valid-loss:8.248967 valid-rmse:8.248967\n",
            "[INFO 23-11-04 20:10:23.5933 UTC gradient_boosted_trees.cc:1556] \tnum-trees:103 train-loss:8.262094 train-rmse:8.262094 valid-loss:8.394173 valid-rmse:8.394173\n",
            "[INFO 23-11-04 20:10:54.9571 UTC gradient_boosted_trees.cc:1556] \tnum-trees:60 train-loss:7.538914 train-rmse:7.538914 valid-loss:8.174025 valid-rmse:8.174025\n",
            "[INFO 23-11-04 20:11:26.6087 UTC gradient_boosted_trees.cc:1556] \tnum-trees:87 train-loss:8.116636 train-rmse:8.116636 valid-loss:8.336063 valid-rmse:8.336063\n",
            "[INFO 23-11-04 20:11:57.9589 UTC gradient_boosted_trees.cc:1556] \tnum-trees:153 train-loss:7.223362 train-rmse:7.223362 valid-loss:8.005520 valid-rmse:8.005520\n",
            "[INFO 23-11-04 20:12:28.4972 UTC gradient_boosted_trees.cc:1556] \tnum-trees:186 train-loss:7.558918 train-rmse:7.558918 valid-loss:8.142854 valid-rmse:8.142854\n",
            "[INFO 23-11-04 20:12:59.0626 UTC gradient_boosted_trees.cc:1556] \tnum-trees:53 train-loss:7.460877 train-rmse:7.460877 valid-loss:8.157470 valid-rmse:8.157470\n",
            "[INFO 23-11-04 20:13:16.0559 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.780329 train-rmse:5.780329 valid-loss:7.745423 valid-rmse:7.745423\n",
            "[INFO 23-11-04 20:13:16.0560 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 20:13:16.0560 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.745423 valid-rmse:7.745423\n",
            "[INFO 23-11-04 20:13:16.0706 UTC hyperparameters_optimizer.cc:582] [14/100] Score: -7.74542 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-04 20:13:16.0712 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 20:13:16.0712 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 20:13:16.0854 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 20:13:24.9631 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.646974 train-rmse:8.646974 valid-loss:8.555338 valid-rmse:8.555338\n",
            "[INFO 23-11-04 20:13:29.5159 UTC gradient_boosted_trees.cc:1556] \tnum-trees:97 train-loss:8.085445 train-rmse:8.085445 valid-loss:8.320839 valid-rmse:8.320839\n",
            "[INFO 23-11-04 20:14:00.5914 UTC gradient_boosted_trees.cc:1556] \tnum-trees:5 train-loss:8.619740 train-rmse:8.619740 valid-loss:8.547661 valid-rmse:8.547661\n",
            "[INFO 23-11-04 20:14:30.8036 UTC gradient_boosted_trees.cc:1556] \tnum-trees:102 train-loss:8.068501 train-rmse:8.068501 valid-loss:8.318067 valid-rmse:8.318067\n",
            "[INFO 23-11-04 20:15:02.9723 UTC gradient_boosted_trees.cc:1556] \tnum-trees:12 train-loss:8.577143 train-rmse:8.577143 valid-loss:8.533130 valid-rmse:8.533130\n",
            "[INFO 23-11-04 20:15:34.3377 UTC gradient_boosted_trees.cc:1556] \tnum-trees:79 train-loss:7.310793 train-rmse:7.310793 valid-loss:8.112328 valid-rmse:8.112328\n",
            "[INFO 23-11-04 20:16:05.4920 UTC gradient_boosted_trees.cc:1556] \tnum-trees:19 train-loss:8.541280 train-rmse:8.541280 valid-loss:8.521605 valid-rmse:8.521605\n",
            "[INFO 23-11-04 20:16:37.3477 UTC gradient_boosted_trees.cc:1556] \tnum-trees:176 train-loss:7.825790 train-rmse:7.825790 valid-loss:8.232926 valid-rmse:8.232926\n",
            "[INFO 23-11-04 20:17:07.5178 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:7.347815 train-rmse:7.347815 valid-loss:8.093402 valid-rmse:8.093402\n",
            "[INFO 23-11-04 20:17:38.8932 UTC gradient_boosted_trees.cc:1556] \tnum-trees:198 train-loss:7.520445 train-rmse:7.520445 valid-loss:8.125009 valid-rmse:8.125009\n",
            "[INFO 23-11-04 20:18:08.9001 UTC gradient_boosted_trees.cc:1556] \tnum-trees:63 train-loss:7.468237 train-rmse:7.468237 valid-loss:8.068601 valid-rmse:8.068601\n",
            "[INFO 23-11-04 20:18:39.3900 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:7.335920 train-rmse:7.335920 valid-loss:8.089212 valid-rmse:8.089212\n",
            "[INFO 23-11-04 20:19:10.1162 UTC gradient_boosted_trees.cc:1556] \tnum-trees:40 train-loss:8.458060 train-rmse:8.458060 valid-loss:8.489918 valid-rmse:8.489918\n",
            "[INFO 23-11-04 20:19:42.1759 UTC gradient_boosted_trees.cc:1556] \tnum-trees:127 train-loss:7.994434 train-rmse:7.994434 valid-loss:8.289212 valid-rmse:8.289212\n",
            "[INFO 23-11-04 20:20:13.2251 UTC gradient_boosted_trees.cc:1556] \tnum-trees:89 train-loss:7.550061 train-rmse:7.550061 valid-loss:8.165439 valid-rmse:8.165439\n",
            "[INFO 23-11-04 20:20:44.2018 UTC gradient_boosted_trees.cc:1556] \tnum-trees:150 train-loss:8.188340 train-rmse:8.188340 valid-loss:8.348014 valid-rmse:8.348014\n",
            "[INFO 23-11-04 20:21:19.7062 UTC gradient_boosted_trees.cc:1556] \tnum-trees:71 train-loss:7.393185 train-rmse:7.393185 valid-loss:8.044750 valid-rmse:8.044750\n",
            "[INFO 23-11-04 20:21:50.3449 UTC gradient_boosted_trees.cc:1556] \tnum-trees:155 train-loss:8.181782 train-rmse:8.181782 valid-loss:8.343793 valid-rmse:8.343793\n",
            "[INFO 23-11-04 20:22:21.0328 UTC gradient_boosted_trees.cc:1556] \tnum-trees:286 train-loss:7.280396 train-rmse:7.280396 valid-loss:8.078667 valid-rmse:8.078667\n",
            "[INFO 23-11-04 20:22:51.2645 UTC gradient_boosted_trees.cc:1556] \tnum-trees:210 train-loss:7.480909 train-rmse:7.480909 valid-loss:8.112635 valid-rmse:8.112635\n",
            "[INFO 23-11-04 20:23:23.6067 UTC gradient_boosted_trees.cc:1556] \tnum-trees:162 train-loss:8.172669 train-rmse:8.172669 valid-loss:8.339437 valid-rmse:8.339437\n",
            "[INFO 23-11-04 20:23:53.7280 UTC gradient_boosted_trees.cc:1556] \tnum-trees:113 train-loss:7.031044 train-rmse:7.031044 valid-loss:8.003016 valid-rmse:8.003016\n",
            "[INFO 23-11-04 20:24:25.0756 UTC gradient_boosted_trees.cc:1556] \tnum-trees:76 train-loss:8.371653 train-rmse:8.371653 valid-loss:8.448995 valid-rmse:8.448995\n",
            "[INFO 23-11-04 20:24:55.3284 UTC gradient_boosted_trees.cc:1556] \tnum-trees:152 train-loss:7.928863 train-rmse:7.928863 valid-loss:8.255261 valid-rmse:8.255261\n",
            "[INFO 23-11-04 20:25:25.3732 UTC gradient_boosted_trees.cc:1556] \tnum-trees:291 train-loss:7.270507 train-rmse:7.270507 valid-loss:8.078074 valid-rmse:8.078074\n",
            "[INFO 23-11-04 20:25:57.7768 UTC gradient_boosted_trees.cc:1556] \tnum-trees:266 train-loss:7.284401 train-rmse:7.284401 valid-loss:8.073393 valid-rmse:8.073393\n",
            "[INFO 23-11-04 20:26:27.8230 UTC gradient_boosted_trees.cc:1556] \tnum-trees:90 train-loss:8.348076 train-rmse:8.348076 valid-loss:8.438379 valid-rmse:8.438379\n",
            "[INFO 23-11-04 20:27:00.0153 UTC gradient_boosted_trees.cc:1556] \tnum-trees:162 train-loss:7.904528 train-rmse:7.904528 valid-loss:8.246503 valid-rmse:8.246503\n",
            "[INFO 23-11-04 20:27:30.7646 UTC gradient_boosted_trees.cc:1556] \tnum-trees:270 train-loss:7.276442 train-rmse:7.276442 valid-loss:8.068723 valid-rmse:8.068723\n",
            "[INFO 23-11-04 20:28:01.5904 UTC gradient_boosted_trees.cc:1556] \tnum-trees:167 train-loss:7.889627 train-rmse:7.889627 valid-loss:8.237628 valid-rmse:8.237628\n",
            "[INFO 23-11-04 20:28:33.0245 UTC gradient_boosted_trees.cc:1556] \tnum-trees:204 train-loss:7.754505 train-rmse:7.754505 valid-loss:8.210018 valid-rmse:8.210018\n",
            "[INFO 23-11-04 20:29:04.2356 UTC gradient_boosted_trees.cc:1556] \tnum-trees:172 train-loss:7.878608 train-rmse:7.878608 valid-loss:8.234514 valid-rmse:8.234514\n",
            "[INFO 23-11-04 20:29:37.6431 UTC gradient_boosted_trees.cc:1556] \tnum-trees:298 train-loss:7.250349 train-rmse:7.250349 valid-loss:8.074094 valid-rmse:8.074094\n",
            "[INFO 23-11-04 20:30:08.4450 UTC gradient_boosted_trees.cc:1556] \tnum-trees:197 train-loss:7.021207 train-rmse:7.021207 valid-loss:7.949730 valid-rmse:7.949730\n",
            "[INFO 23-11-04 20:30:38.9771 UTC gradient_boosted_trees.cc:1556] \tnum-trees:209 train-loss:7.742496 train-rmse:7.742496 valid-loss:8.206087 valid-rmse:8.206087\n",
            "[INFO 23-11-04 20:30:50.3387 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.244993 train-rmse:7.244993 valid-loss:8.073519 valid-rmse:8.073519\n",
            "[INFO 23-11-04 20:30:50.3387 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 20:30:50.3387 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.073519 valid-rmse:8.073519\n",
            "[INFO 23-11-04 20:30:50.3464 UTC hyperparameters_optimizer.cc:582] [15/100] Score: -8.07352 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-04 20:30:50.3508 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 20:30:50.3514 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 20:30:50.3627 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 20:31:08.9949 UTC gradient_boosted_trees.cc:1556] \tnum-trees:182 train-loss:7.859890 train-rmse:7.859890 valid-loss:8.225376 valid-rmse:8.225376\n",
            "[INFO 23-11-04 20:31:17.4309 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.611590 train-rmse:8.611590 valid-loss:8.543801 valid-rmse:8.543801\n",
            "[INFO 23-11-04 20:31:42.7289 UTC gradient_boosted_trees.cc:1556] \tnum-trees:145 train-loss:6.808504 train-rmse:6.808504 valid-loss:7.976307 valid-rmse:7.976307\n",
            "[INFO 23-11-04 20:32:12.9302 UTC gradient_boosted_trees.cc:1556] \tnum-trees:129 train-loss:8.293134 train-rmse:8.293134 valid-loss:8.404925 valid-rmse:8.404925\n",
            "[INFO 23-11-04 20:32:44.2749 UTC gradient_boosted_trees.cc:1556] \tnum-trees:97 train-loss:7.193096 train-rmse:7.193096 valid-loss:7.951343 valid-rmse:7.951343\n",
            "[INFO 23-11-04 20:33:14.7014 UTC gradient_boosted_trees.cc:1556] \tnum-trees:136 train-loss:8.284854 train-rmse:8.284854 valid-loss:8.399357 valid-rmse:8.399357\n",
            "[INFO 23-11-04 20:33:44.9424 UTC gradient_boosted_trees.cc:1556] \tnum-trees:119 train-loss:7.368484 train-rmse:7.368484 valid-loss:8.109265 valid-rmse:8.109265\n",
            "[INFO 23-11-04 20:34:16.5460 UTC gradient_boosted_trees.cc:1556] \tnum-trees:143 train-loss:8.276928 train-rmse:8.276928 valid-loss:8.391271 valid-rmse:8.391271\n",
            "[INFO 23-11-04 20:34:50.0587 UTC gradient_boosted_trees.cc:1556] \tnum-trees:200 train-loss:7.814399 train-rmse:7.814399 valid-loss:8.198675 valid-rmse:8.198675\n",
            "[INFO 23-11-04 20:35:21.1936 UTC gradient_boosted_trees.cc:1556] \tnum-trees:10 train-loss:8.317162 train-rmse:8.317162 valid-loss:8.434680 valid-rmse:8.434680\n",
            "[INFO 23-11-04 20:35:51.3595 UTC gradient_boosted_trees.cc:1556] \tnum-trees:162 train-loss:6.672678 train-rmse:6.672678 valid-loss:7.947244 valid-rmse:7.947244\n",
            "[INFO 23-11-04 20:36:23.9316 UTC gradient_boosted_trees.cc:1556] \tnum-trees:221 train-loss:8.103220 train-rmse:8.103220 valid-loss:8.289495 valid-rmse:8.289495\n",
            "[INFO 23-11-04 20:36:54.0310 UTC gradient_boosted_trees.cc:1556] \tnum-trees:210 train-loss:7.793453 train-rmse:7.793453 valid-loss:8.189489 valid-rmse:8.189489\n",
            "[INFO 23-11-04 20:37:24.3124 UTC gradient_boosted_trees.cc:1556] \tnum-trees:225 train-loss:7.699600 train-rmse:7.699600 valid-loss:8.192660 valid-rmse:8.192660\n",
            "[INFO 23-11-04 20:37:56.3424 UTC gradient_boosted_trees.cc:1556] \tnum-trees:215 train-loss:7.780750 train-rmse:7.780750 valid-loss:8.187004 valid-rmse:8.187004\n",
            "[INFO 23-11-04 20:38:28.8049 UTC gradient_boosted_trees.cc:1556] \tnum-trees:131 train-loss:6.691147 train-rmse:6.691147 valid-loss:7.948272 valid-rmse:7.948272\n",
            "[INFO 23-11-04 20:38:59.4571 UTC gradient_boosted_trees.cc:1556] \tnum-trees:18 train-loss:8.155679 train-rmse:8.155679 valid-loss:8.393011 valid-rmse:8.393011\n",
            "[INFO 23-11-04 20:39:00.9407 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.195752 train-rmse:7.195752 valid-loss:8.048820 valid-rmse:8.048820\n",
            "[INFO 23-11-04 20:39:00.9408 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-04 20:39:00.9408 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:8.048776 valid-rmse:8.048776\n",
            "[INFO 23-11-04 20:39:00.9480 UTC hyperparameters_optimizer.cc:582] [16/100] Score: -8.04878 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-04 20:39:00.9525 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 20:39:00.9529 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 20:39:00.9657 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 20:39:31.0335 UTC gradient_boosted_trees.cc:1556] \tnum-trees:235 train-loss:8.090389 train-rmse:8.090389 valid-loss:8.284061 valid-rmse:8.284061\n",
            "[INFO 23-11-04 20:39:32.2952 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.634936 train-rmse:8.634936 valid-loss:8.546871 valid-rmse:8.546871\n",
            "[INFO 23-11-04 20:40:02.9450 UTC gradient_boosted_trees.cc:1556] \tnum-trees:133 train-loss:7.273571 train-rmse:7.273571 valid-loss:8.086605 valid-rmse:8.086605\n",
            "[INFO 23-11-04 20:40:34.6779 UTC gradient_boosted_trees.cc:1556] \tnum-trees:222 train-loss:6.917676 train-rmse:6.917676 valid-loss:7.931914 valid-rmse:7.931914\n",
            "[INFO 23-11-04 20:41:05.4457 UTC gradient_boosted_trees.cc:1556] \tnum-trees:189 train-loss:8.229956 train-rmse:8.229956 valid-loss:8.360963 valid-rmse:8.360963\n",
            "[INFO 23-11-04 20:41:37.8610 UTC gradient_boosted_trees.cc:1556] \tnum-trees:186 train-loss:6.518863 train-rmse:6.518863 valid-loss:7.942297 valid-rmse:7.942297\n",
            "[INFO 23-11-04 20:42:09.1300 UTC gradient_boosted_trees.cc:1556] \tnum-trees:196 train-loss:8.224045 train-rmse:8.224045 valid-loss:8.356354 valid-rmse:8.356354\n",
            "[INFO 23-11-04 20:42:39.2611 UTC gradient_boosted_trees.cc:1556] \tnum-trees:26 train-loss:8.024440 train-rmse:8.024440 valid-loss:8.356223 valid-rmse:8.356223\n",
            "[INFO 23-11-04 20:43:11.9246 UTC gradient_boosted_trees.cc:1556] \tnum-trees:203 train-loss:8.217690 train-rmse:8.217690 valid-loss:8.353934 valid-rmse:8.353934\n",
            "[INFO 23-11-04 20:43:43.6121 UTC gradient_boosted_trees.cc:1556] \tnum-trees:9 train-loss:8.507009 train-rmse:8.507009 valid-loss:8.504272 valid-rmse:8.504272\n",
            "[INFO 23-11-04 20:44:14.9177 UTC gradient_boosted_trees.cc:1556] \tnum-trees:10 train-loss:8.492712 train-rmse:8.492712 valid-loss:8.499074 valid-rmse:8.499074\n",
            "[INFO 23-11-04 20:44:46.1909 UTC gradient_boosted_trees.cc:1556] \tnum-trees:11 train-loss:8.479019 train-rmse:8.479019 valid-loss:8.493534 valid-rmse:8.493534\n",
            "[INFO 23-11-04 20:45:17.5279 UTC gradient_boosted_trees.cc:1556] \tnum-trees:12 train-loss:8.463456 train-rmse:8.463456 valid-loss:8.485039 valid-rmse:8.485039\n",
            "[INFO 23-11-04 20:45:48.7996 UTC gradient_boosted_trees.cc:1556] \tnum-trees:13 train-loss:8.451952 train-rmse:8.451952 valid-loss:8.477810 valid-rmse:8.477810\n",
            "[INFO 23-11-04 20:46:19.9314 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:7.649881 train-rmse:7.649881 valid-loss:8.169933 valid-rmse:8.169933\n",
            "[INFO 23-11-04 20:46:50.6070 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:8.056857 train-rmse:8.056857 valid-loss:8.268687 valid-rmse:8.268687\n",
            "[INFO 23-11-04 20:47:22.6737 UTC gradient_boosted_trees.cc:1556] \tnum-trees:16 train-loss:8.413596 train-rmse:8.413596 valid-loss:8.466468 valid-rmse:8.466468\n",
            "[INFO 23-11-04 20:47:54.1480 UTC gradient_boosted_trees.cc:1556] \tnum-trees:17 train-loss:8.401959 train-rmse:8.401959 valid-loss:8.463612 valid-rmse:8.463612\n",
            "[INFO 23-11-04 20:48:25.1398 UTC gradient_boosted_trees.cc:1556] \tnum-trees:215 train-loss:6.330672 train-rmse:6.330672 valid-loss:7.920169 valid-rmse:7.920169\n",
            "[INFO 23-11-04 20:48:57.0015 UTC gradient_boosted_trees.cc:1556] \tnum-trees:19 train-loss:8.376040 train-rmse:8.376040 valid-loss:8.452532 valid-rmse:8.452532\n",
            "[INFO 23-11-04 20:49:28.1794 UTC gradient_boosted_trees.cc:1556] \tnum-trees:41 train-loss:7.824194 train-rmse:7.824194 valid-loss:8.299019 valid-rmse:8.299019\n",
            "[INFO 23-11-04 20:49:59.6036 UTC gradient_boosted_trees.cc:1556] \tnum-trees:21 train-loss:8.353744 train-rmse:8.353744 valid-loss:8.444768 valid-rmse:8.444768\n",
            "[INFO 23-11-04 20:50:30.7804 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:6.822718 train-rmse:6.822718 valid-loss:7.912099 valid-rmse:7.912099\n",
            "[INFO 23-11-04 20:51:01.8229 UTC gradient_boosted_trees.cc:1556] \tnum-trees:256 train-loss:8.174678 train-rmse:8.174678 valid-loss:8.319143 valid-rmse:8.319143\n",
            "[INFO 23-11-04 20:51:32.6966 UTC gradient_boosted_trees.cc:1556] \tnum-trees:228 train-loss:6.250712 train-rmse:6.250712 valid-loss:7.920263 valid-rmse:7.920263\n",
            "[INFO 23-11-04 20:52:04.1390 UTC gradient_boosted_trees.cc:1556] \tnum-trees:173 train-loss:6.383584 train-rmse:6.383584 valid-loss:7.897523 valid-rmse:7.897523\n",
            "[INFO 23-11-04 20:52:35.1837 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:6.803714 train-rmse:6.803714 valid-loss:7.907924 valid-rmse:7.907924\n",
            "[INFO 23-11-04 20:53:06.5841 UTC gradient_boosted_trees.cc:1556] \tnum-trees:145 train-loss:6.888956 train-rmse:6.888956 valid-loss:7.880172 valid-rmse:7.880172\n",
            "[INFO 23-11-04 20:53:37.7112 UTC gradient_boosted_trees.cc:1556] \tnum-trees:163 train-loss:7.120286 train-rmse:7.120286 valid-loss:8.044538 valid-rmse:8.044538\n",
            "[INFO 23-11-04 20:53:42.4301 UTC early_stopping.cc:53] Early stop of the training because the validation loss does not decrease anymore. Best valid-loss: 7.90873\n",
            "[INFO 23-11-04 20:53:42.4301 UTC gradient_boosted_trees.cc:249] Truncates the model to 207 tree(s) i.e. 207  iteration(s).\n",
            "[INFO 23-11-04 20:53:42.4307 UTC gradient_boosted_trees.cc:312] Final model num-trees:207 valid-loss:7.908734 valid-rmse:7.908734\n",
            "[INFO 23-11-04 20:53:42.4357 UTC hyperparameters_optimizer.cc:582] [17/100] Score: -7.90873 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-04 20:53:42.4366 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 20:53:42.4367 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 20:53:42.4498 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 20:53:53.9696 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:8.027022 train-rmse:8.027022 valid-loss:8.252640 valid-rmse:8.252640\n",
            "[INFO 23-11-04 20:53:53.9697 UTC gradient_boosted_trees.cc:249] Truncates the model to 298 tree(s) i.e. 298  iteration(s).\n",
            "[INFO 23-11-04 20:53:53.9697 UTC gradient_boosted_trees.cc:312] Final model num-trees:298 valid-loss:8.252310 valid-rmse:8.252310\n",
            "[INFO 23-11-04 20:53:53.9726 UTC hyperparameters_optimizer.cc:582] [18/100] Score: -8.25231 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-04 20:53:53.9753 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 20:53:53.9753 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 20:53:53.9887 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 20:54:09.0324 UTC gradient_boosted_trees.cc:1556] \tnum-trees:148 train-loss:6.868504 train-rmse:6.868504 valid-loss:7.879019 valid-rmse:7.879019\n",
            "[INFO 23-11-04 20:54:10.6306 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.606154 train-rmse:8.606154 valid-loss:8.544372 valid-rmse:8.544372\n",
            "[INFO 23-11-04 20:54:16.4226 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.571601 train-rmse:8.571601 valid-loss:8.526810 valid-rmse:8.526810\n",
            "[INFO 23-11-04 20:54:39.2802 UTC gradient_boosted_trees.cc:1556] \tnum-trees:2 train-loss:8.499169 train-rmse:8.499169 valid-loss:8.493300 valid-rmse:8.493300\n",
            "[INFO 23-11-04 20:55:09.7463 UTC gradient_boosted_trees.cc:1556] \tnum-trees:284 train-loss:7.281530 train-rmse:7.281530 valid-loss:8.059787 valid-rmse:8.059787\n",
            "[INFO 23-11-04 20:55:37.6482 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.626481 train-rmse:7.626481 valid-loss:8.146975 valid-rmse:8.146975\n",
            "[INFO 23-11-04 20:55:37.6483 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 20:55:37.6483 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.146975 valid-rmse:8.146975\n",
            "[INFO 23-11-04 20:55:37.6521 UTC hyperparameters_optimizer.cc:582] [19/100] Score: -8.14697 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 6 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-04 20:55:37.6566 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 20:55:37.6566 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 20:55:37.6676 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 20:55:41.0106 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:7.604383 train-rmse:7.604383 valid-loss:8.156052 valid-rmse:8.156052\n",
            "[INFO 23-11-04 20:55:57.1590 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.637339 train-rmse:8.637339 valid-loss:8.551982 valid-rmse:8.551982\n",
            "[INFO 23-11-04 20:56:13.3259 UTC gradient_boosted_trees.cc:1556] \tnum-trees:186 train-loss:6.300302 train-rmse:6.300302 valid-loss:7.879900 valid-rmse:7.879900\n",
            "[INFO 23-11-04 20:56:46.8480 UTC gradient_boosted_trees.cc:1556] \tnum-trees:57 train-loss:7.648830 train-rmse:7.648830 valid-loss:8.228405 valid-rmse:8.228405\n",
            "[INFO 23-11-04 20:57:18.8867 UTC gradient_boosted_trees.cc:1556] \tnum-trees:35 train-loss:8.216827 train-rmse:8.216827 valid-loss:8.397527 valid-rmse:8.397527\n",
            "[INFO 23-11-04 20:57:33.3109 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:8.139977 train-rmse:8.139977 valid-loss:8.298954 valid-rmse:8.298954\n",
            "[INFO 23-11-04 20:57:33.3109 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 20:57:33.3109 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.298954 valid-rmse:8.298954\n",
            "[INFO 23-11-04 20:57:33.3118 UTC hyperparameters_optimizer.cc:582] [20/100] Score: -8.29895 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 3 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-04 20:57:33.3154 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 20:57:33.3244 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 20:57:33.3405 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 20:57:50.2746 UTC gradient_boosted_trees.cc:1556] \tnum-trees:36 train-loss:8.209301 train-rmse:8.209301 valid-loss:8.395391 valid-rmse:8.395391\n",
            "[INFO 23-11-04 20:57:59.0048 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.639105 train-rmse:8.639105 valid-loss:8.550057 valid-rmse:8.550057\n",
            "[INFO 23-11-04 20:58:22.2818 UTC gradient_boosted_trees.cc:1556] \tnum-trees:37 train-loss:8.201389 train-rmse:8.201389 valid-loss:8.394813 valid-rmse:8.394813\n",
            "[INFO 23-11-04 20:58:53.9837 UTC gradient_boosted_trees.cc:1556] \tnum-trees:38 train-loss:8.193622 train-rmse:8.193622 valid-loss:8.389507 valid-rmse:8.389507\n",
            "[INFO 23-11-04 20:59:25.5083 UTC gradient_boosted_trees.cc:1556] \tnum-trees:196 train-loss:6.226318 train-rmse:6.226318 valid-loss:7.862803 valid-rmse:7.862803\n",
            "[INFO 23-11-04 20:59:56.9081 UTC gradient_boosted_trees.cc:1556] \tnum-trees:177 train-loss:7.031403 train-rmse:7.031403 valid-loss:8.011487 valid-rmse:8.011487\n",
            "[INFO 23-11-04 21:00:28.5467 UTC gradient_boosted_trees.cc:1556] \tnum-trees:270 train-loss:6.723957 train-rmse:6.723957 valid-loss:7.884685 valid-rmse:7.884685\n",
            "[INFO 23-11-04 21:01:00.9022 UTC gradient_boosted_trees.cc:1556] \tnum-trees:42 train-loss:8.160631 train-rmse:8.160631 valid-loss:8.380503 valid-rmse:8.380503\n",
            "[INFO 23-11-04 21:01:32.5400 UTC gradient_boosted_trees.cc:1556] \tnum-trees:43 train-loss:8.152071 train-rmse:8.152071 valid-loss:8.378260 valid-rmse:8.378260\n",
            "[INFO 23-11-04 21:02:04.6906 UTC gradient_boosted_trees.cc:1556] \tnum-trees:44 train-loss:8.144980 train-rmse:8.144980 valid-loss:8.375370 valid-rmse:8.375370\n",
            "[INFO 23-11-04 21:02:10.1362 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.239336 train-rmse:7.239336 valid-loss:8.046941 valid-rmse:8.046941\n",
            "[INFO 23-11-04 21:02:10.1362 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-04 21:02:10.1362 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:8.046789 valid-rmse:8.046789\n",
            "[INFO 23-11-04 21:02:10.1412 UTC hyperparameters_optimizer.cc:582] [21/100] Score: -8.04679 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 64 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-04 21:02:10.1422 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 21:02:10.1423 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 21:02:10.1577 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 21:02:35.3651 UTC gradient_boosted_trees.cc:1556] \tnum-trees:12 train-loss:8.498161 train-rmse:8.498161 valid-loss:8.491825 valid-rmse:8.491825\n",
            "[INFO 23-11-04 21:02:38.3880 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.612195 train-rmse:8.612195 valid-loss:8.537906 valid-rmse:8.537906\n",
            "[INFO 23-11-04 21:03:06.7939 UTC gradient_boosted_trees.cc:1556] \tnum-trees:184 train-loss:6.994226 train-rmse:6.994226 valid-loss:7.996871 valid-rmse:7.996871\n",
            "[INFO 23-11-04 21:03:38.9152 UTC gradient_boosted_trees.cc:1556] \tnum-trees:72 train-loss:7.509511 train-rmse:7.509511 valid-loss:8.198330 valid-rmse:8.198330\n",
            "[INFO 23-11-04 21:04:11.0953 UTC gradient_boosted_trees.cc:1556] \tnum-trees:48 train-loss:8.116975 train-rmse:8.116975 valid-loss:8.358503 valid-rmse:8.358503\n",
            "[INFO 23-11-04 21:04:41.9883 UTC gradient_boosted_trees.cc:1556] \tnum-trees:174 train-loss:6.735434 train-rmse:6.735434 valid-loss:7.868220 valid-rmse:7.868220\n",
            "[INFO 23-11-04 21:05:14.2390 UTC gradient_boosted_trees.cc:1556] \tnum-trees:18 train-loss:8.435602 train-rmse:8.435602 valid-loss:8.459585 valid-rmse:8.459585\n",
            "[INFO 23-11-04 21:05:46.1480 UTC gradient_boosted_trees.cc:1556] \tnum-trees:51 train-loss:8.095642 train-rmse:8.095642 valid-loss:8.347707 valid-rmse:8.347707\n",
            "[INFO 23-11-04 21:06:17.2241 UTC gradient_boosted_trees.cc:1556] \tnum-trees:284 train-loss:6.667124 train-rmse:6.667124 valid-loss:7.872307 valid-rmse:7.872307\n",
            "[INFO 23-11-04 21:06:47.7679 UTC gradient_boosted_trees.cc:1556] \tnum-trees:220 train-loss:6.087909 train-rmse:6.087909 valid-loss:7.823673 valid-rmse:7.823673\n",
            "[INFO 23-11-04 21:07:18.9815 UTC gradient_boosted_trees.cc:1556] \tnum-trees:80 train-loss:7.434133 train-rmse:7.434133 valid-loss:8.171900 valid-rmse:8.171900\n",
            "[INFO 23-11-04 21:07:51.6574 UTC gradient_boosted_trees.cc:1556] \tnum-trees:36 train-loss:7.556080 train-rmse:7.556080 valid-loss:8.174981 valid-rmse:8.174981\n",
            "[INFO 23-11-04 21:08:22.9129 UTC gradient_boosted_trees.cc:1556] \tnum-trees:13 train-loss:8.241024 train-rmse:8.241024 valid-loss:8.398778 valid-rmse:8.398778\n",
            "[INFO 23-11-04 21:08:54.2803 UTC gradient_boosted_trees.cc:1556] \tnum-trees:32 train-loss:7.889091 train-rmse:7.889091 valid-loss:8.289752 valid-rmse:8.289752\n",
            "[INFO 23-11-04 21:09:21.5743 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.545444 train-rmse:7.545444 valid-loss:8.135014 valid-rmse:8.135014\n",
            "[INFO 23-11-04 21:09:21.5743 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 21:09:21.5743 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.135014 valid-rmse:8.135014\n",
            "[INFO 23-11-04 21:09:21.5807 UTC hyperparameters_optimizer.cc:582] [22/100] Score: -8.13501 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 128 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-04 21:09:21.5855 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 21:09:21.5855 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 21:09:21.6011 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 21:09:25.1648 UTC gradient_boosted_trees.cc:1556] \tnum-trees:40 train-loss:7.484202 train-rmse:7.484202 valid-loss:8.136250 valid-rmse:8.136250\n",
            "[INFO 23-11-04 21:09:47.4411 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.636838 train-rmse:8.636838 valid-loss:8.551740 valid-rmse:8.551740\n",
            "[INFO 23-11-04 21:09:55.7480 UTC gradient_boosted_trees.cc:1556] \tnum-trees:189 train-loss:6.662418 train-rmse:6.662418 valid-loss:7.851612 valid-rmse:7.851612\n",
            "[INFO 23-11-04 21:10:28.6682 UTC gradient_boosted_trees.cc:1556] \tnum-trees:294 train-loss:6.634266 train-rmse:6.634266 valid-loss:7.857981 valid-rmse:7.857981\n",
            "[INFO 23-11-04 21:10:59.7422 UTC gradient_boosted_trees.cc:1556] \tnum-trees:44 train-loss:7.420350 train-rmse:7.420350 valid-loss:8.106482 valid-rmse:8.106482\n",
            "[INFO 23-11-04 21:11:30.2798 UTC gradient_boosted_trees.cc:1556] \tnum-trees:5 train-loss:8.575067 train-rmse:8.575067 valid-loss:8.523497 valid-rmse:8.523497\n",
            "[INFO 23-11-04 21:12:03.2588 UTC gradient_boosted_trees.cc:1556] \tnum-trees:63 train-loss:8.017274 train-rmse:8.017274 valid-loss:8.312008 valid-rmse:8.312008\n",
            "[INFO 23-11-04 21:12:33.6166 UTC gradient_boosted_trees.cc:1556] \tnum-trees:205 train-loss:6.874933 train-rmse:6.874933 valid-loss:7.967309 valid-rmse:7.967309\n",
            "[INFO 23-11-04 21:12:58.6116 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.613643 train-rmse:6.613643 valid-loss:7.851799 valid-rmse:7.851799\n",
            "[INFO 23-11-04 21:12:58.6116 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 21:12:58.6116 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.851799 valid-rmse:7.851799\n",
            "[INFO 23-11-04 21:12:58.6162 UTC hyperparameters_optimizer.cc:582] [23/100] Score: -7.8518 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 128 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 21:12:58.6223 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 21:12:58.6223 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 21:12:58.6354 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 21:13:05.7353 UTC gradient_boosted_trees.cc:1556] \tnum-trees:65 train-loss:8.004320 train-rmse:8.004320 valid-loss:8.308135 valid-rmse:8.308135\n",
            "[INFO 23-11-04 21:13:23.1803 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.585273 train-rmse:8.585273 valid-loss:8.529624 valid-rmse:8.529624\n",
            "[INFO 23-11-04 21:13:36.9923 UTC gradient_boosted_trees.cc:1556] \tnum-trees:66 train-loss:7.999084 train-rmse:7.999084 valid-loss:8.305256 valid-rmse:8.305256\n",
            "[INFO 23-11-04 21:14:08.3401 UTC gradient_boosted_trees.cc:1556] \tnum-trees:67 train-loss:7.993888 train-rmse:7.993888 valid-loss:8.307865 valid-rmse:8.307865\n",
            "[INFO 23-11-04 21:14:39.8363 UTC gradient_boosted_trees.cc:1556] \tnum-trees:68 train-loss:7.987995 train-rmse:7.987995 valid-loss:8.307612 valid-rmse:8.307612\n",
            "[INFO 23-11-04 21:15:11.2818 UTC gradient_boosted_trees.cc:1556] \tnum-trees:69 train-loss:7.981947 train-rmse:7.981947 valid-loss:8.305230 valid-rmse:8.305230\n",
            "[INFO 23-11-04 21:15:41.4946 UTC gradient_boosted_trees.cc:1556] \tnum-trees:42 train-loss:8.247399 train-rmse:8.247399 valid-loss:8.389432 valid-rmse:8.389432\n",
            "[INFO 23-11-04 21:16:12.6554 UTC gradient_boosted_trees.cc:1556] \tnum-trees:16 train-loss:8.436849 train-rmse:8.436849 valid-loss:8.466851 valid-rmse:8.466851\n",
            "[INFO 23-11-04 21:16:42.6739 UTC gradient_boosted_trees.cc:1556] \tnum-trees:207 train-loss:6.575075 train-rmse:6.575075 valid-loss:7.837311 valid-rmse:7.837311\n",
            "[INFO 23-11-04 21:17:16.3637 UTC gradient_boosted_trees.cc:1556] \tnum-trees:60 train-loss:7.232483 train-rmse:7.232483 valid-loss:8.047241 valid-rmse:8.047241\n",
            "[INFO 23-11-04 21:17:46.4592 UTC gradient_boosted_trees.cc:1556] \tnum-trees:47 train-loss:8.215872 train-rmse:8.215872 valid-loss:8.377277 valid-rmse:8.377277\n",
            "[INFO 23-11-04 21:18:17.8652 UTC gradient_boosted_trees.cc:1556] \tnum-trees:13 train-loss:8.078451 train-rmse:8.078451 valid-loss:8.343405 valid-rmse:8.343405\n",
            "[INFO 23-11-04 21:18:48.4379 UTC gradient_boosted_trees.cc:1556] \tnum-trees:105 train-loss:7.248116 train-rmse:7.248116 valid-loss:8.120301 valid-rmse:8.120301\n",
            "[INFO 23-11-04 21:19:20.1491 UTC gradient_boosted_trees.cc:1556] \tnum-trees:72 train-loss:8.049864 train-rmse:8.049864 valid-loss:8.338357 valid-rmse:8.338357\n",
            "[INFO 23-11-04 21:19:54.3351 UTC gradient_boosted_trees.cc:1556] \tnum-trees:78 train-loss:7.924299 train-rmse:7.924299 valid-loss:8.281076 valid-rmse:8.281076\n",
            "[INFO 23-11-04 21:20:25.0925 UTC gradient_boosted_trees.cc:1556] \tnum-trees:53 train-loss:8.180925 train-rmse:8.180925 valid-loss:8.366862 valid-rmse:8.366862\n",
            "[INFO 23-11-04 21:20:55.4152 UTC gradient_boosted_trees.cc:1556] \tnum-trees:265 train-loss:5.856138 train-rmse:5.856138 valid-loss:7.779653 valid-rmse:7.779653\n",
            "[INFO 23-11-04 21:21:28.0239 UTC gradient_boosted_trees.cc:1556] \tnum-trees:81 train-loss:7.905492 train-rmse:7.905492 valid-loss:8.277340 valid-rmse:8.277340\n",
            "[INFO 23-11-04 21:21:59.2651 UTC gradient_boosted_trees.cc:1556] \tnum-trees:22 train-loss:7.892713 train-rmse:7.892713 valid-loss:8.264953 valid-rmse:8.264953\n",
            "[INFO 23-11-04 21:22:31.1659 UTC gradient_boosted_trees.cc:1556] \tnum-trees:83 train-loss:7.894776 train-rmse:7.894776 valid-loss:8.273762 valid-rmse:8.273762\n",
            "[INFO 23-11-04 21:23:02.5571 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:7.889545 train-rmse:7.889545 valid-loss:8.271582 valid-rmse:8.271582\n",
            "[INFO 23-11-04 21:23:33.7525 UTC gradient_boosted_trees.cc:1556] \tnum-trees:60 train-loss:8.143942 train-rmse:8.143942 valid-loss:8.351935 valid-rmse:8.351935\n",
            "[INFO 23-11-04 21:24:05.6771 UTC gradient_boosted_trees.cc:1556] \tnum-trees:45 train-loss:7.794703 train-rmse:7.794703 valid-loss:8.259550 valid-rmse:8.259550\n",
            "[INFO 23-11-04 21:24:37.5709 UTC gradient_boosted_trees.cc:1556] \tnum-trees:87 train-loss:7.874836 train-rmse:7.874836 valid-loss:8.264632 valid-rmse:8.264632\n",
            "[INFO 23-11-04 21:25:09.7173 UTC gradient_boosted_trees.cc:1556] \tnum-trees:233 train-loss:6.739017 train-rmse:6.739017 valid-loss:7.932134 valid-rmse:7.932134\n",
            "[INFO 23-11-04 21:25:41.0517 UTC gradient_boosted_trees.cc:1556] \tnum-trees:89 train-loss:7.866085 train-rmse:7.866085 valid-loss:8.262549 valid-rmse:8.262549\n",
            "[INFO 23-11-04 21:26:12.3124 UTC gradient_boosted_trees.cc:1556] \tnum-trees:90 train-loss:7.861104 train-rmse:7.861104 valid-loss:8.259167 valid-rmse:8.259167\n",
            "[INFO 23-11-04 21:26:42.3460 UTC gradient_boosted_trees.cc:1556] \tnum-trees:67 train-loss:8.110403 train-rmse:8.110403 valid-loss:8.344103 valid-rmse:8.344103\n",
            "[INFO 23-11-04 21:27:15.0110 UTC gradient_boosted_trees.cc:1556] \tnum-trees:92 train-loss:7.850921 train-rmse:7.850921 valid-loss:8.255731 valid-rmse:8.255731\n",
            "[INFO 23-11-04 21:27:45.7262 UTC gradient_boosted_trees.cc:1556] \tnum-trees:43 train-loss:8.198973 train-rmse:8.198973 valid-loss:8.377896 valid-rmse:8.377896\n",
            "[INFO 23-11-04 21:28:17.2115 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:6.695188 train-rmse:6.695188 valid-loss:7.919252 valid-rmse:7.919252\n",
            "[INFO 23-11-04 21:28:51.0683 UTC gradient_boosted_trees.cc:1556] \tnum-trees:290 train-loss:5.718561 train-rmse:5.718561 valid-loss:7.755545 valid-rmse:7.755545\n",
            "[INFO 23-11-04 21:29:22.7272 UTC gradient_boosted_trees.cc:1556] \tnum-trees:96 train-loss:7.828515 train-rmse:7.828515 valid-loss:8.249284 valid-rmse:8.249284\n",
            "[INFO 23-11-04 21:29:54.3733 UTC gradient_boosted_trees.cc:1556] \tnum-trees:97 train-loss:7.822559 train-rmse:7.822559 valid-loss:8.247313 valid-rmse:8.247313\n",
            "[INFO 23-11-04 21:30:25.1880 UTC gradient_boosted_trees.cc:1556] \tnum-trees:98 train-loss:7.813386 train-rmse:7.813386 valid-loss:8.244210 valid-rmse:8.244210\n",
            "[INFO 23-11-04 21:30:57.6459 UTC gradient_boosted_trees.cc:1556] \tnum-trees:99 train-loss:7.810224 train-rmse:7.810224 valid-loss:8.241320 valid-rmse:8.241320\n",
            "[INFO 23-11-04 21:31:28.2068 UTC gradient_boosted_trees.cc:1556] \tnum-trees:60 train-loss:7.648870 train-rmse:7.648870 valid-loss:8.207461 valid-rmse:8.207461\n",
            "[INFO 23-11-04 21:31:55.0658 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.667325 train-rmse:5.667325 valid-loss:7.749890 valid-rmse:7.749890\n",
            "[INFO 23-11-04 21:31:55.0659 UTC gradient_boosted_trees.cc:249] Truncates the model to 298 tree(s) i.e. 298  iteration(s).\n",
            "[INFO 23-11-04 21:31:55.0659 UTC gradient_boosted_trees.cc:312] Final model num-trees:298 valid-loss:7.747954 valid-rmse:7.747954\n",
            "[INFO 23-11-04 21:31:55.0724 UTC hyperparameters_optimizer.cc:582] [24/100] Score: -7.74795 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 21:31:55.0745 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 21:31:55.0757 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 21:31:55.0970 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 21:32:00.4159 UTC gradient_boosted_trees.cc:1556] \tnum-trees:110 train-loss:7.908163 train-rmse:7.908163 valid-loss:8.284950 valid-rmse:8.284950\n",
            "[INFO 23-11-04 21:32:02.4879 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.643733 train-rmse:8.643733 valid-loss:8.553048 valid-rmse:8.553048\n",
            "[INFO 23-11-04 21:32:31.0495 UTC gradient_boosted_trees.cc:1556] \tnum-trees:54 train-loss:8.120634 train-rmse:8.120634 valid-loss:8.354541 valid-rmse:8.354541\n",
            "[INFO 23-11-04 21:33:02.2963 UTC gradient_boosted_trees.cc:1556] \tnum-trees:9 train-loss:8.586951 train-rmse:8.586951 valid-loss:8.526562 valid-rmse:8.526562\n",
            "[INFO 23-11-04 21:33:34.8280 UTC gradient_boosted_trees.cc:1556] \tnum-trees:104 train-loss:7.785950 train-rmse:7.785950 valid-loss:8.228406 valid-rmse:8.228406\n",
            "[INFO 23-11-04 21:34:05.3838 UTC gradient_boosted_trees.cc:1556] \tnum-trees:103 train-loss:6.747482 train-rmse:6.747482 valid-loss:7.953543 valid-rmse:7.953543\n",
            "[INFO 23-11-04 21:34:35.7268 UTC gradient_boosted_trees.cc:1556] \tnum-trees:254 train-loss:6.627207 train-rmse:6.627207 valid-loss:7.910741 valid-rmse:7.910741\n",
            "[INFO 23-11-04 21:35:06.7450 UTC gradient_boosted_trees.cc:1556] \tnum-trees:60 train-loss:8.085512 train-rmse:8.085512 valid-loss:8.340656 valid-rmse:8.340656\n",
            "[INFO 23-11-04 21:35:38.9073 UTC gradient_boosted_trees.cc:1556] \tnum-trees:121 train-loss:7.861573 train-rmse:7.861573 valid-loss:8.265922 valid-rmse:8.265922\n",
            "[INFO 23-11-04 21:36:09.4230 UTC gradient_boosted_trees.cc:1556] \tnum-trees:34 train-loss:8.466856 train-rmse:8.466856 valid-loss:8.472716 valid-rmse:8.472716\n",
            "[INFO 23-11-04 21:36:39.9550 UTC gradient_boosted_trees.cc:1556] \tnum-trees:38 train-loss:8.453906 train-rmse:8.453906 valid-loss:8.463671 valid-rmse:8.463671\n",
            "[INFO 23-11-04 21:37:10.4776 UTC gradient_boosted_trees.cc:1556] \tnum-trees:42 train-loss:8.441312 train-rmse:8.441312 valid-loss:8.458773 valid-rmse:8.458773\n",
            "[INFO 23-11-04 21:37:41.8165 UTC gradient_boosted_trees.cc:1556] \tnum-trees:266 train-loss:6.321364 train-rmse:6.321364 valid-loss:7.792822 valid-rmse:7.792822\n",
            "[INFO 23-11-04 21:38:13.2562 UTC gradient_boosted_trees.cc:1556] \tnum-trees:147 train-loss:6.977816 train-rmse:6.977816 valid-loss:8.040940 valid-rmse:8.040940\n",
            "[INFO 23-11-04 21:38:44.2056 UTC gradient_boosted_trees.cc:1556] \tnum-trees:263 train-loss:6.599016 train-rmse:6.599016 valid-loss:7.906642 valid-rmse:7.906642\n",
            "[INFO 23-11-04 21:39:14.7316 UTC gradient_boosted_trees.cc:1556] \tnum-trees:64 train-loss:7.232439 train-rmse:7.232439 valid-loss:8.047468 valid-rmse:8.047468\n",
            "[INFO 23-11-04 21:39:46.9532 UTC gradient_boosted_trees.cc:1556] \tnum-trees:97 train-loss:7.981711 train-rmse:7.981711 valid-loss:8.293296 valid-rmse:8.293296\n",
            "[INFO 23-11-04 21:40:17.5663 UTC gradient_boosted_trees.cc:1556] \tnum-trees:135 train-loss:7.818170 train-rmse:7.818170 valid-loss:8.251712 valid-rmse:8.251712\n",
            "[INFO 23-11-04 21:40:50.3027 UTC gradient_boosted_trees.cc:1556] \tnum-trees:71 train-loss:8.377371 train-rmse:8.377371 valid-loss:8.432690 valid-rmse:8.432690\n",
            "[INFO 23-11-04 21:41:20.8483 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:8.371642 train-rmse:8.371642 valid-loss:8.431126 valid-rmse:8.431126\n",
            "[INFO 23-11-04 21:41:51.0251 UTC gradient_boosted_trees.cc:1556] \tnum-trees:79 train-loss:8.363984 train-rmse:8.363984 valid-loss:8.427410 valid-rmse:8.427410\n",
            "[INFO 23-11-04 21:42:21.3160 UTC gradient_boosted_trees.cc:1556] \tnum-trees:83 train-loss:8.357567 train-rmse:8.357567 valid-loss:8.424539 valid-rmse:8.424539\n",
            "[INFO 23-11-04 21:42:51.5167 UTC gradient_boosted_trees.cc:1556] \tnum-trees:87 train-loss:8.350442 train-rmse:8.350442 valid-loss:8.418021 valid-rmse:8.418021\n",
            "[INFO 23-11-04 21:43:21.5618 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:7.442714 train-rmse:7.442714 valid-loss:8.150195 valid-rmse:8.150195\n",
            "[INFO 23-11-04 21:43:52.4902 UTC gradient_boosted_trees.cc:1556] \tnum-trees:95 train-loss:8.339801 train-rmse:8.339801 valid-loss:8.413278 valid-rmse:8.413278\n",
            "[INFO 23-11-04 21:44:22.9953 UTC gradient_boosted_trees.cc:1556] \tnum-trees:99 train-loss:8.332829 train-rmse:8.332829 valid-loss:8.409744 valid-rmse:8.409744\n",
            "[INFO 23-11-04 21:44:53.5497 UTC gradient_boosted_trees.cc:1556] \tnum-trees:103 train-loss:8.328255 train-rmse:8.328255 valid-loss:8.403170 valid-rmse:8.403170\n",
            "[INFO 23-11-04 21:45:23.8883 UTC gradient_boosted_trees.cc:1556] \tnum-trees:110 train-loss:7.934236 train-rmse:7.934236 valid-loss:8.263820 valid-rmse:8.263820\n",
            "[INFO 23-11-04 21:45:54.2750 UTC gradient_boosted_trees.cc:1556] \tnum-trees:152 train-loss:7.749252 train-rmse:7.749252 valid-loss:8.226271 valid-rmse:8.226271\n",
            "[INFO 23-11-04 21:46:27.4397 UTC gradient_boosted_trees.cc:1556] \tnum-trees:110 train-loss:7.161325 train-rmse:7.161325 valid-loss:8.044086 valid-rmse:8.044086\n",
            "[INFO 23-11-04 21:46:58.3938 UTC gradient_boosted_trees.cc:1556] \tnum-trees:136 train-loss:6.407596 train-rmse:6.407596 valid-loss:7.871413 valid-rmse:7.871413\n",
            "[INFO 23-11-04 21:47:31.7419 UTC gradient_boosted_trees.cc:1556] \tnum-trees:124 train-loss:8.301738 train-rmse:8.301738 valid-loss:8.385525 valid-rmse:8.385525\n",
            "[INFO 23-11-04 21:48:01.7496 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:6.207976 train-rmse:6.207976 valid-loss:7.774749 valid-rmse:7.774749\n",
            "[INFO 23-11-04 21:48:31.9511 UTC gradient_boosted_trees.cc:1556] \tnum-trees:132 train-loss:7.664881 train-rmse:7.664881 valid-loss:8.169355 valid-rmse:8.169355\n",
            "[INFO 23-11-04 21:49:02.5438 UTC gradient_boosted_trees.cc:1556] \tnum-trees:298 train-loss:6.197163 train-rmse:6.197163 valid-loss:7.774885 valid-rmse:7.774885\n",
            "[INFO 23-11-04 21:49:34.5509 UTC gradient_boosted_trees.cc:1556] \tnum-trees:89 train-loss:6.988713 train-rmse:6.988713 valid-loss:7.984602 valid-rmse:7.984602\n",
            "[INFO 23-11-04 21:49:43.7725 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.190148 train-rmse:6.190148 valid-loss:7.774369 valid-rmse:7.774369\n",
            "[INFO 23-11-04 21:49:43.7817 UTC gradient_boosted_trees.cc:249] Truncates the model to 294 tree(s) i.e. 294  iteration(s).\n",
            "[INFO 23-11-04 21:49:43.7839 UTC gradient_boosted_trees.cc:312] Final model num-trees:294 valid-loss:7.773740 valid-rmse:7.773740\n",
            "[INFO 23-11-04 21:49:43.7871 UTC hyperparameters_optimizer.cc:582] [25/100] Score: -7.77374 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 16 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 21:49:43.7877 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 21:49:43.7877 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 21:49:43.8038 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 21:50:05.9785 UTC gradient_boosted_trees.cc:1556] \tnum-trees:144 train-loss:6.342787 train-rmse:6.342787 valid-loss:7.858939 valid-rmse:7.858939\n",
            "[INFO 23-11-04 21:50:14.0860 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.573120 train-rmse:8.573120 valid-loss:8.527412 valid-rmse:8.527412\n",
            "[INFO 23-11-04 21:50:37.9280 UTC gradient_boosted_trees.cc:1556] \tnum-trees:96 train-loss:7.899602 train-rmse:7.899602 valid-loss:8.273625 valid-rmse:8.273625\n",
            "[INFO 23-11-04 21:51:08.1665 UTC gradient_boosted_trees.cc:1556] \tnum-trees:175 train-loss:6.834803 train-rmse:6.834803 valid-loss:8.008938 valid-rmse:8.008938\n",
            "[INFO 23-11-04 21:51:38.5828 UTC gradient_boosted_trees.cc:1556] \tnum-trees:94 train-loss:6.944115 train-rmse:6.944115 valid-loss:7.977614 valid-rmse:7.977614\n",
            "[INFO 23-11-04 21:52:09.7959 UTC gradient_boosted_trees.cc:1556] \tnum-trees:171 train-loss:7.686050 train-rmse:7.686050 valid-loss:8.199588 valid-rmse:8.199588\n",
            "[INFO 23-11-04 21:52:41.8738 UTC gradient_boosted_trees.cc:1556] \tnum-trees:126 train-loss:7.884460 train-rmse:7.884460 valid-loss:8.236265 valid-rmse:8.236265\n",
            "[INFO 23-11-04 21:53:12.6347 UTC gradient_boosted_trees.cc:1556] \tnum-trees:104 train-loss:7.290454 train-rmse:7.290454 valid-loss:8.094807 valid-rmse:8.094807\n",
            "[INFO 23-11-04 21:53:42.8895 UTC gradient_boosted_trees.cc:1556] \tnum-trees:296 train-loss:6.449478 train-rmse:6.449478 valid-loss:7.865568 valid-rmse:7.865568\n",
            "[INFO 23-11-04 21:54:13.7252 UTC gradient_boosted_trees.cc:1556] \tnum-trees:106 train-loss:7.281485 train-rmse:7.281485 valid-loss:8.095110 valid-rmse:8.095110\n",
            "[INFO 23-11-04 21:54:44.1608 UTC gradient_boosted_trees.cc:1556] \tnum-trees:107 train-loss:7.276422 train-rmse:7.276422 valid-loss:8.093556 valid-rmse:8.093556\n",
            "[INFO 23-11-04 21:55:14.3801 UTC gradient_boosted_trees.cc:1556] \tnum-trees:108 train-loss:7.270772 train-rmse:7.270772 valid-loss:8.093911 valid-rmse:8.093911\n",
            "[INFO 23-11-04 21:55:31.7876 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.438028 train-rmse:6.438028 valid-loss:7.860815 valid-rmse:7.860815\n",
            "[INFO 23-11-04 21:55:31.7877 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-04 21:55:31.7877 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.860103 valid-rmse:7.860103\n",
            "[INFO 23-11-04 21:55:31.7937 UTC hyperparameters_optimizer.cc:582] [26/100] Score: -7.8601 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-04 21:55:31.7948 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 21:55:31.7948 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 21:55:31.8138 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 21:55:44.4841 UTC gradient_boosted_trees.cc:1556] \tnum-trees:129 train-loss:7.006083 train-rmse:7.006083 valid-loss:8.007201 valid-rmse:8.007201\n",
            "[INFO 23-11-04 21:55:54.3748 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.635650 train-rmse:8.635650 valid-loss:8.548719 valid-rmse:8.548719\n",
            "[INFO 23-11-04 21:56:14.8995 UTC gradient_boosted_trees.cc:1556] \tnum-trees:110 train-loss:7.262526 train-rmse:7.262526 valid-loss:8.092748 valid-rmse:8.092748\n",
            "[INFO 23-11-04 21:56:45.2066 UTC gradient_boosted_trees.cc:1556] \tnum-trees:111 train-loss:7.260990 train-rmse:7.260990 valid-loss:8.091822 valid-rmse:8.091822\n",
            "[INFO 23-11-04 21:57:15.5861 UTC gradient_boosted_trees.cc:1556] \tnum-trees:112 train-loss:7.257308 train-rmse:7.257308 valid-loss:8.088359 valid-rmse:8.088359\n",
            "[INFO 23-11-04 21:57:45.8397 UTC gradient_boosted_trees.cc:1556] \tnum-trees:113 train-loss:7.250817 train-rmse:7.250817 valid-loss:8.086973 valid-rmse:8.086973\n",
            "[INFO 23-11-04 21:58:15.8546 UTC gradient_boosted_trees.cc:1556] \tnum-trees:114 train-loss:7.243408 train-rmse:7.243408 valid-loss:8.086270 valid-rmse:8.086270\n",
            "[INFO 23-11-04 21:58:46.7907 UTC gradient_boosted_trees.cc:1556] \tnum-trees:214 train-loss:8.208798 train-rmse:8.208798 valid-loss:8.344059 valid-rmse:8.344059\n",
            "[INFO 23-11-04 21:59:18.8163 UTC gradient_boosted_trees.cc:1556] \tnum-trees:10 train-loss:8.501448 train-rmse:8.501448 valid-loss:8.493301 valid-rmse:8.493301\n",
            "[INFO 23-11-04 21:59:49.4977 UTC gradient_boosted_trees.cc:1556] \tnum-trees:117 train-loss:7.819379 train-rmse:7.819379 valid-loss:8.240693 valid-rmse:8.240693\n",
            "[INFO 23-11-04 22:00:21.9026 UTC gradient_boosted_trees.cc:1556] \tnum-trees:21 train-loss:7.844834 train-rmse:7.844834 valid-loss:8.304318 valid-rmse:8.304318\n",
            "[INFO 23-11-04 22:00:52.0266 UTC gradient_boosted_trees.cc:1556] \tnum-trees:22 train-loss:7.804155 train-rmse:7.804155 valid-loss:8.297646 valid-rmse:8.297646\n",
            "[INFO 23-11-04 22:01:22.8499 UTC gradient_boosted_trees.cc:1556] \tnum-trees:23 train-loss:7.781384 train-rmse:7.781384 valid-loss:8.282872 valid-rmse:8.282872\n",
            "[INFO 23-11-04 22:01:52.9215 UTC gradient_boosted_trees.cc:1556] \tnum-trees:24 train-loss:7.751816 train-rmse:7.751816 valid-loss:8.277996 valid-rmse:8.277996\n",
            "[INFO 23-11-04 22:02:23.4070 UTC gradient_boosted_trees.cc:1556] \tnum-trees:25 train-loss:7.736351 train-rmse:7.736351 valid-loss:8.281191 valid-rmse:8.281191\n",
            "[INFO 23-11-04 22:02:53.6998 UTC gradient_boosted_trees.cc:1556] \tnum-trees:124 train-loss:7.791476 train-rmse:7.791476 valid-loss:8.228890 valid-rmse:8.228890\n",
            "[INFO 23-11-04 22:03:24.5841 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:8.171133 train-rmse:8.171133 valid-loss:8.334822 valid-rmse:8.334822\n",
            "[INFO 23-11-04 22:03:54.6319 UTC gradient_boosted_trees.cc:1556] \tnum-trees:255 train-loss:8.167236 train-rmse:8.167236 valid-loss:8.333797 valid-rmse:8.333797\n",
            "[INFO 23-11-04 22:04:24.7191 UTC gradient_boosted_trees.cc:1556] \tnum-trees:259 train-loss:8.164244 train-rmse:8.164244 valid-loss:8.334476 valid-rmse:8.334476\n",
            "[INFO 23-11-04 22:04:55.0017 UTC gradient_boosted_trees.cc:1556] \tnum-trees:263 train-loss:8.159920 train-rmse:8.159920 valid-loss:8.333410 valid-rmse:8.333410\n",
            "[INFO 23-11-04 22:05:25.0182 UTC gradient_boosted_trees.cc:1556] \tnum-trees:267 train-loss:8.156341 train-rmse:8.156341 valid-loss:8.334510 valid-rmse:8.334510\n",
            "[INFO 23-11-04 22:05:55.3150 UTC gradient_boosted_trees.cc:1556] \tnum-trees:271 train-loss:8.152459 train-rmse:8.152459 valid-loss:8.332682 valid-rmse:8.332682\n",
            "[INFO 23-11-04 22:06:25.5791 UTC gradient_boosted_trees.cc:1556] \tnum-trees:214 train-loss:7.552287 train-rmse:7.552287 valid-loss:8.152814 valid-rmse:8.152814\n",
            "[INFO 23-11-04 22:06:56.1381 UTC gradient_boosted_trees.cc:1556] \tnum-trees:209 train-loss:6.658688 train-rmse:6.658688 valid-loss:7.967259 valid-rmse:7.967259\n",
            "[INFO 23-11-04 22:07:26.9183 UTC gradient_boosted_trees.cc:1556] \tnum-trees:283 train-loss:8.142941 train-rmse:8.142941 valid-loss:8.328533 valid-rmse:8.328533\n",
            "[INFO 23-11-04 22:07:57.2909 UTC gradient_boosted_trees.cc:1556] \tnum-trees:287 train-loss:8.139962 train-rmse:8.139962 valid-loss:8.326587 valid-rmse:8.326587\n",
            "[INFO 23-11-04 22:08:27.4699 UTC gradient_boosted_trees.cc:1556] \tnum-trees:291 train-loss:8.135583 train-rmse:8.135583 valid-loss:8.324193 valid-rmse:8.324193\n",
            "[INFO 23-11-04 22:08:57.4925 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:8.132150 train-rmse:8.132150 valid-loss:8.322556 valid-rmse:8.322556\n",
            "[INFO 23-11-04 22:09:27.7120 UTC gradient_boosted_trees.cc:1556] \tnum-trees:299 train-loss:8.127867 train-rmse:8.127867 valid-loss:8.319112 valid-rmse:8.319112\n",
            "[INFO 23-11-04 22:09:35.3103 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:8.126924 train-rmse:8.126924 valid-loss:8.318165 valid-rmse:8.318165\n",
            "[INFO 23-11-04 22:09:35.3192 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 22:09:35.3215 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.318165 valid-rmse:8.318165\n",
            "[INFO 23-11-04 22:09:35.3229 UTC hyperparameters_optimizer.cc:582] [27/100] Score: -8.31816 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-04 22:09:35.3244 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 22:09:35.3244 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 22:09:35.3354 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 22:09:58.2943 UTC gradient_boosted_trees.cc:1556] \tnum-trees:172 train-loss:7.518332 train-rmse:7.518332 valid-loss:8.112597 valid-rmse:8.112597\n",
            "[INFO 23-11-04 22:09:58.8456 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.636806 train-rmse:8.636806 valid-loss:8.550903 valid-rmse:8.550903\n",
            "[INFO 23-11-04 22:10:28.8947 UTC gradient_boosted_trees.cc:1556] \tnum-trees:226 train-loss:7.522920 train-rmse:7.522920 valid-loss:8.145428 valid-rmse:8.145428\n",
            "[INFO 23-11-04 22:11:02.8546 UTC gradient_boosted_trees.cc:1556] \tnum-trees:167 train-loss:7.765652 train-rmse:7.765652 valid-loss:8.178095 valid-rmse:8.178095\n",
            "[INFO 23-11-04 22:11:32.8617 UTC gradient_boosted_trees.cc:1556] \tnum-trees:143 train-loss:6.503360 train-rmse:6.503360 valid-loss:7.867943 valid-rmse:7.867943\n",
            "[INFO 23-11-04 22:12:07.1709 UTC gradient_boosted_trees.cc:1556] \tnum-trees:142 train-loss:7.055297 train-rmse:7.055297 valid-loss:8.019702 valid-rmse:8.019702\n",
            "[INFO 23-11-04 22:12:37.2131 UTC gradient_boosted_trees.cc:1556] \tnum-trees:143 train-loss:7.051829 train-rmse:7.051829 valid-loss:8.020965 valid-rmse:8.020965\n",
            "[INFO 23-11-04 22:13:09.3475 UTC gradient_boosted_trees.cc:1556] \tnum-trees:234 train-loss:7.502358 train-rmse:7.502358 valid-loss:8.136352 valid-rmse:8.136352\n",
            "[INFO 23-11-04 22:13:42.5109 UTC gradient_boosted_trees.cc:1556] \tnum-trees:11 train-loss:8.508088 train-rmse:8.508088 valid-loss:8.499875 valid-rmse:8.499875\n",
            "[INFO 23-11-04 22:14:15.5208 UTC gradient_boosted_trees.cc:1556] \tnum-trees:174 train-loss:7.749338 train-rmse:7.749338 valid-loss:8.171356 valid-rmse:8.171356\n",
            "[INFO 23-11-04 22:14:48.1711 UTC gradient_boosted_trees.cc:1556] \tnum-trees:151 train-loss:6.444279 train-rmse:6.444279 valid-loss:7.860091 valid-rmse:7.860091\n",
            "[INFO 23-11-04 22:15:19.9917 UTC gradient_boosted_trees.cc:1556] \tnum-trees:50 train-loss:7.334492 train-rmse:7.334492 valid-loss:8.139419 valid-rmse:8.139419\n",
            "[INFO 23-11-04 22:15:51.5405 UTC gradient_boosted_trees.cc:1556] \tnum-trees:51 train-loss:7.326973 train-rmse:7.326973 valid-loss:8.136536 valid-rmse:8.136536\n",
            "[INFO 23-11-04 22:16:21.7970 UTC gradient_boosted_trees.cc:1556] \tnum-trees:55 train-loss:8.092494 train-rmse:8.092494 valid-loss:8.338447 valid-rmse:8.338447\n",
            "[INFO 23-11-04 22:16:53.9330 UTC gradient_boosted_trees.cc:1556] \tnum-trees:213 train-loss:5.706679 train-rmse:5.706679 valid-loss:7.800077 valid-rmse:7.800077\n",
            "[INFO 23-11-04 22:17:25.3037 UTC gradient_boosted_trees.cc:1556] \tnum-trees:54 train-loss:7.283025 train-rmse:7.283025 valid-loss:8.127572 valid-rmse:8.127572\n",
            "[INFO 23-11-04 22:17:55.4922 UTC gradient_boosted_trees.cc:1556] \tnum-trees:248 train-loss:7.471517 train-rmse:7.471517 valid-loss:8.129615 valid-rmse:8.129615\n",
            "[INFO 23-11-04 22:18:26.2895 UTC gradient_boosted_trees.cc:1556] \tnum-trees:217 train-loss:5.671403 train-rmse:5.671403 valid-loss:7.802874 valid-rmse:7.802874\n",
            "[INFO 23-11-04 22:18:56.2950 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:7.466500 train-rmse:7.466500 valid-loss:8.129115 valid-rmse:8.129115\n",
            "[INFO 23-11-04 22:19:28.9782 UTC gradient_boosted_trees.cc:1556] \tnum-trees:157 train-loss:6.966727 train-rmse:6.966727 valid-loss:7.996975 valid-rmse:7.996975\n",
            "[INFO 23-11-04 22:20:00.6651 UTC gradient_boosted_trees.cc:1556] \tnum-trees:221 train-loss:5.654547 train-rmse:5.654547 valid-loss:7.795025 valid-rmse:7.795025\n",
            "[INFO 23-11-04 22:20:30.7960 UTC gradient_boosted_trees.cc:1556] \tnum-trees:165 train-loss:6.332603 train-rmse:6.332603 valid-loss:7.834303 valid-rmse:7.834303\n",
            "[INFO 23-11-04 22:21:03.2854 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:6.477381 train-rmse:6.477381 valid-loss:7.924428 valid-rmse:7.924428\n",
            "[INFO 23-11-04 22:21:34.1466 UTC gradient_boosted_trees.cc:1556] \tnum-trees:32 train-loss:8.308437 train-rmse:8.308437 valid-loss:8.417174 valid-rmse:8.417174\n",
            "[INFO 23-11-04 22:22:04.7366 UTC gradient_boosted_trees.cc:1556] \tnum-trees:70 train-loss:8.010209 train-rmse:8.010209 valid-loss:8.310807 valid-rmse:8.310807\n",
            "[INFO 23-11-04 22:22:35.4196 UTC gradient_boosted_trees.cc:1556] \tnum-trees:170 train-loss:6.313520 train-rmse:6.313520 valid-loss:7.830755 valid-rmse:7.830755\n",
            "[INFO 23-11-04 22:23:06.5650 UTC gradient_boosted_trees.cc:1556] \tnum-trees:65 train-loss:7.159523 train-rmse:7.159523 valid-loss:8.088933 valid-rmse:8.088933\n",
            "[INFO 23-11-04 22:23:36.9861 UTC gradient_boosted_trees.cc:1556] \tnum-trees:74 train-loss:7.986499 train-rmse:7.986499 valid-loss:8.303062 valid-rmse:8.303062\n",
            "[INFO 23-11-04 22:24:08.7703 UTC gradient_boosted_trees.cc:1556] \tnum-trees:67 train-loss:7.144841 train-rmse:7.144841 valid-loss:8.080564 valid-rmse:8.080564\n",
            "[INFO 23-11-04 22:24:39.0651 UTC gradient_boosted_trees.cc:1556] \tnum-trees:175 train-loss:6.275618 train-rmse:6.275618 valid-loss:7.827563 valid-rmse:7.827563\n",
            "[INFO 23-11-04 22:25:09.8063 UTC gradient_boosted_trees.cc:1556] \tnum-trees:78 train-loss:7.965797 train-rmse:7.965797 valid-loss:8.297325 valid-rmse:8.297325\n",
            "[INFO 23-11-04 22:25:41.9267 UTC gradient_boosted_trees.cc:1556] \tnum-trees:43 train-loss:8.227403 train-rmse:8.227403 valid-loss:8.392029 valid-rmse:8.392029\n",
            "[INFO 23-11-04 22:26:14.5151 UTC gradient_boosted_trees.cc:1556] \tnum-trees:179 train-loss:6.248284 train-rmse:6.248284 valid-loss:7.824728 valid-rmse:7.824728\n",
            "[INFO 23-11-04 22:26:46.1043 UTC gradient_boosted_trees.cc:1556] \tnum-trees:72 train-loss:7.083573 train-rmse:7.083573 valid-loss:8.074439 valid-rmse:8.074439\n",
            "[INFO 23-11-04 22:27:16.3772 UTC gradient_boosted_trees.cc:1556] \tnum-trees:73 train-loss:7.069255 train-rmse:7.069255 valid-loss:8.070126 valid-rmse:8.070126\n",
            "[INFO 23-11-04 22:27:46.7421 UTC gradient_boosted_trees.cc:1556] \tnum-trees:205 train-loss:7.417528 train-rmse:7.417528 valid-loss:8.088502 valid-rmse:8.088502\n",
            "[INFO 23-11-04 22:28:18.8812 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:7.048034 train-rmse:7.048034 valid-loss:8.066286 valid-rmse:8.066286\n",
            "[INFO 23-11-04 22:28:49.7291 UTC gradient_boosted_trees.cc:1556] \tnum-trees:76 train-loss:7.036090 train-rmse:7.036090 valid-loss:8.066722 valid-rmse:8.066722\n",
            "[INFO 23-11-04 22:29:20.3511 UTC gradient_boosted_trees.cc:1556] \tnum-trees:77 train-loss:7.026253 train-rmse:7.026253 valid-loss:8.064461 valid-rmse:8.064461\n",
            "[INFO 23-11-04 22:29:51.7710 UTC gradient_boosted_trees.cc:1556] \tnum-trees:78 train-loss:7.018634 train-rmse:7.018634 valid-loss:8.062550 valid-rmse:8.062550\n",
            "[INFO 23-11-04 22:30:22.2252 UTC gradient_boosted_trees.cc:1556] \tnum-trees:79 train-loss:7.006911 train-rmse:7.006911 valid-loss:8.062049 valid-rmse:8.062049\n",
            "[INFO 23-11-04 22:30:52.2525 UTC gradient_boosted_trees.cc:1556] \tnum-trees:261 train-loss:6.391463 train-rmse:6.391463 valid-loss:7.901293 valid-rmse:7.901293\n",
            "[INFO 23-11-04 22:31:23.7188 UTC gradient_boosted_trees.cc:1556] \tnum-trees:81 train-loss:6.981094 train-rmse:6.981094 valid-loss:8.052401 valid-rmse:8.052401\n",
            "[INFO 23-11-04 22:31:54.7364 UTC gradient_boosted_trees.cc:1556] \tnum-trees:193 train-loss:6.143332 train-rmse:6.143332 valid-loss:7.803346 valid-rmse:7.803346\n",
            "[INFO 23-11-04 22:32:26.0549 UTC gradient_boosted_trees.cc:1556] \tnum-trees:83 train-loss:6.961008 train-rmse:6.961008 valid-loss:8.049194 valid-rmse:8.049194\n",
            "[INFO 23-11-04 22:32:56.5799 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:6.951672 train-rmse:6.951672 valid-loss:8.048596 valid-rmse:8.048596\n",
            "[INFO 23-11-04 22:33:27.3905 UTC gradient_boosted_trees.cc:1556] \tnum-trees:85 train-loss:6.938442 train-rmse:6.938442 valid-loss:8.048168 valid-rmse:8.048168\n",
            "[INFO 23-11-04 22:33:57.8378 UTC gradient_boosted_trees.cc:1556] \tnum-trees:198 train-loss:6.112902 train-rmse:6.112902 valid-loss:7.798293 valid-rmse:7.798293\n",
            "[INFO 23-11-04 22:34:29.5233 UTC gradient_boosted_trees.cc:1556] \tnum-trees:87 train-loss:6.920413 train-rmse:6.920413 valid-loss:8.044587 valid-rmse:8.044587\n",
            "[INFO 23-11-04 22:35:00.2986 UTC gradient_boosted_trees.cc:1556] \tnum-trees:299 train-loss:7.350894 train-rmse:7.350894 valid-loss:8.088401 valid-rmse:8.088401\n",
            "[INFO 23-11-04 22:35:20.6338 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.348936 train-rmse:7.348936 valid-loss:8.087590 valid-rmse:8.087590\n",
            "[INFO 23-11-04 22:35:20.6430 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 22:35:20.6684 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.087590 valid-rmse:8.087590\n",
            "[INFO 23-11-04 22:35:20.7664 UTC hyperparameters_optimizer.cc:582] [28/100] Score: -8.08759 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 128 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-04 22:35:20.8189 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 22:35:20.8311 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 22:35:20.8557 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 22:35:31.9649 UTC gradient_boosted_trees.cc:1556] \tnum-trees:261 train-loss:5.360049 train-rmse:5.360049 valid-loss:7.753907 valid-rmse:7.753907\n",
            "[INFO 23-11-04 22:35:52.4770 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.589087 train-rmse:8.589087 valid-loss:8.532928 valid-rmse:8.532928\n",
            "[INFO 23-11-04 22:36:02.2854 UTC gradient_boosted_trees.cc:1556] \tnum-trees:90 train-loss:6.891153 train-rmse:6.891153 valid-loss:8.032367 valid-rmse:8.032367\n",
            "[INFO 23-11-04 22:36:33.3157 UTC gradient_boosted_trees.cc:1556] \tnum-trees:201 train-loss:7.515423 train-rmse:7.515423 valid-loss:8.132665 valid-rmse:8.132665\n",
            "[INFO 23-11-04 22:37:04.7849 UTC gradient_boosted_trees.cc:1556] \tnum-trees:92 train-loss:6.875853 train-rmse:6.875853 valid-loss:8.030902 valid-rmse:8.030902\n",
            "[INFO 23-11-04 22:37:34.8290 UTC gradient_boosted_trees.cc:1556] \tnum-trees:215 train-loss:6.537663 train-rmse:6.537663 valid-loss:7.877453 valid-rmse:7.877453\n",
            "[INFO 23-11-04 22:38:07.5285 UTC gradient_boosted_trees.cc:1556] \tnum-trees:94 train-loss:6.858187 train-rmse:6.858187 valid-loss:8.022060 valid-rmse:8.022060\n",
            "[INFO 23-11-04 22:38:38.9025 UTC gradient_boosted_trees.cc:1556] \tnum-trees:95 train-loss:6.853324 train-rmse:6.853324 valid-loss:8.020535 valid-rmse:8.020535\n",
            "[INFO 23-11-04 22:39:09.8859 UTC gradient_boosted_trees.cc:1556] \tnum-trees:226 train-loss:7.358122 train-rmse:7.358122 valid-loss:8.069400 valid-rmse:8.069400\n",
            "[INFO 23-11-04 22:39:41.3037 UTC gradient_boosted_trees.cc:1556] \tnum-trees:97 train-loss:6.832553 train-rmse:6.832553 valid-loss:8.014005 valid-rmse:8.014005\n",
            "[INFO 23-11-04 22:40:12.3642 UTC gradient_boosted_trees.cc:1556] \tnum-trees:98 train-loss:6.821849 train-rmse:6.821849 valid-loss:8.010150 valid-rmse:8.010150\n",
            "[INFO 23-11-04 22:40:43.2702 UTC gradient_boosted_trees.cc:1556] \tnum-trees:99 train-loss:6.809157 train-rmse:6.809157 valid-loss:8.009045 valid-rmse:8.009045\n",
            "[INFO 23-11-04 22:41:13.5478 UTC gradient_boosted_trees.cc:1556] \tnum-trees:120 train-loss:7.776779 train-rmse:7.776779 valid-loss:8.233284 valid-rmse:8.233284\n",
            "[INFO 23-11-04 22:41:45.2222 UTC gradient_boosted_trees.cc:1556] \tnum-trees:213 train-loss:7.473162 train-rmse:7.473162 valid-loss:8.119796 valid-rmse:8.119796\n",
            "[INFO 23-11-04 22:42:16.9594 UTC gradient_boosted_trees.cc:1556] \tnum-trees:102 train-loss:6.779468 train-rmse:6.779468 valid-loss:8.004013 valid-rmse:8.004013\n",
            "[INFO 23-11-04 22:42:48.7965 UTC gradient_boosted_trees.cc:1556] \tnum-trees:204 train-loss:6.717136 train-rmse:6.717136 valid-loss:7.942043 valid-rmse:7.942043\n",
            "[INFO 23-11-04 22:43:19.2594 UTC gradient_boosted_trees.cc:1556] \tnum-trees:238 train-loss:7.594111 train-rmse:7.594111 valid-loss:8.110437 valid-rmse:8.110437\n",
            "[INFO 23-11-04 22:43:50.5518 UTC gradient_boosted_trees.cc:1556] \tnum-trees:105 train-loss:6.755846 train-rmse:6.755846 valid-loss:8.000218 valid-rmse:8.000218\n",
            "[INFO 23-11-04 22:44:20.6851 UTC gradient_boosted_trees.cc:1556] \tnum-trees:106 train-loss:6.739093 train-rmse:6.739093 valid-loss:7.994069 valid-rmse:7.994069\n",
            "[INFO 23-11-04 22:44:50.7656 UTC gradient_boosted_trees.cc:1556] \tnum-trees:230 train-loss:6.465722 train-rmse:6.465722 valid-loss:7.854627 valid-rmse:7.854627\n",
            "[INFO 23-11-04 22:45:22.7253 UTC gradient_boosted_trees.cc:1556] \tnum-trees:108 train-loss:6.714427 train-rmse:6.714427 valid-loss:7.997765 valid-rmse:7.997765\n",
            "[INFO 23-11-04 22:45:54.0205 UTC gradient_boosted_trees.cc:1556] \tnum-trees:109 train-loss:6.706636 train-rmse:6.706636 valid-loss:7.993702 valid-rmse:7.993702\n",
            "[INFO 23-11-04 22:46:24.1209 UTC gradient_boosted_trees.cc:1556] \tnum-trees:110 train-loss:6.692788 train-rmse:6.692788 valid-loss:7.992527 valid-rmse:7.992527\n",
            "[INFO 23-11-04 22:46:56.2816 UTC gradient_boosted_trees.cc:1556] \tnum-trees:111 train-loss:6.689534 train-rmse:6.689534 valid-loss:7.989893 valid-rmse:7.989893\n",
            "[INFO 23-11-04 22:47:26.5111 UTC gradient_boosted_trees.cc:1556] \tnum-trees:226 train-loss:7.431091 train-rmse:7.431091 valid-loss:8.102752 valid-rmse:8.102752\n",
            "[INFO 23-11-04 22:47:57.8320 UTC gradient_boosted_trees.cc:1556] \tnum-trees:113 train-loss:6.663747 train-rmse:6.663747 valid-loss:7.985585 valid-rmse:7.985585\n",
            "[INFO 23-11-04 22:48:28.1085 UTC gradient_boosted_trees.cc:1556] \tnum-trees:25 train-loss:7.968928 train-rmse:7.968928 valid-loss:8.280058 valid-rmse:8.280058\n",
            "[INFO 23-11-04 22:48:54.4984 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.210975 train-rmse:6.210975 valid-loss:7.859674 valid-rmse:7.859674\n",
            "[INFO 23-11-04 22:48:54.4985 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 22:48:54.4985 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.859674 valid-rmse:7.859674\n",
            "[INFO 23-11-04 22:48:54.5077 UTC hyperparameters_optimizer.cc:582] [29/100] Score: -7.85967 / -7.74542 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-04 22:48:54.5114 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 22:48:54.5120 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 22:48:54.5266 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 22:49:00.3434 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:6.650532 train-rmse:6.650532 valid-loss:7.981231 valid-rmse:7.981231\n",
            "[INFO 23-11-04 22:49:09.1360 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.588515 train-rmse:8.588515 valid-loss:8.525073 valid-rmse:8.525073\n",
            "[INFO 23-11-04 22:49:30.6758 UTC gradient_boosted_trees.cc:1556] \tnum-trees:252 train-loss:7.564952 train-rmse:7.564952 valid-loss:8.099349 valid-rmse:8.099349\n",
            "[INFO 23-11-04 22:50:01.6879 UTC gradient_boosted_trees.cc:1556] \tnum-trees:143 train-loss:7.679105 train-rmse:7.679105 valid-loss:8.192714 valid-rmse:8.192714\n",
            "[INFO 23-11-04 22:50:29.4293 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.077259 train-rmse:5.077259 valid-loss:7.729182 valid-rmse:7.729182\n",
            "[INFO 23-11-04 22:50:29.4293 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 22:50:29.4293 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.729182 valid-rmse:7.729182\n",
            "[INFO 23-11-04 22:50:29.4419 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 22:50:29.4420 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 22:50:29.4465 UTC hyperparameters_optimizer.cc:582] [30/100] Score: -7.72918 / -7.72918 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 22:50:29.4551 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 22:50:33.4050 UTC gradient_boosted_trees.cc:1556] \tnum-trees:118 train-loss:6.625574 train-rmse:6.625574 valid-loss:7.967411 valid-rmse:7.967411\n",
            "[INFO 23-11-04 22:50:42.3269 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.643369 train-rmse:8.643369 valid-loss:8.554175 valid-rmse:8.554175\n",
            "[INFO 23-11-04 22:51:07.1263 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:6.401397 train-rmse:6.401397 valid-loss:7.834711 valid-rmse:7.834711\n",
            "[INFO 23-11-04 22:51:37.8270 UTC gradient_boosted_trees.cc:1556] \tnum-trees:11 train-loss:8.202656 train-rmse:8.202656 valid-loss:8.347721 valid-rmse:8.347721\n",
            "[INFO 23-11-04 22:52:08.7733 UTC gradient_boosted_trees.cc:1556] \tnum-trees:223 train-loss:6.600067 train-rmse:6.600067 valid-loss:7.923565 valid-rmse:7.923565\n",
            "[INFO 23-11-04 22:52:39.7309 UTC gradient_boosted_trees.cc:1556] \tnum-trees:259 train-loss:7.550708 train-rmse:7.550708 valid-loss:8.093009 valid-rmse:8.093009\n",
            "[INFO 23-11-04 22:53:15.7474 UTC gradient_boosted_trees.cc:1556] \tnum-trees:34 train-loss:7.855881 train-rmse:7.855881 valid-loss:8.250469 valid-rmse:8.250469\n",
            "[INFO 23-11-04 22:53:47.9876 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:5.822885 train-rmse:5.822885 valid-loss:7.734786 valid-rmse:7.734786\n",
            "[INFO 23-11-04 22:54:21.3298 UTC gradient_boosted_trees.cc:1556] \tnum-trees:18 train-loss:8.522976 train-rmse:8.522976 valid-loss:8.500109 valid-rmse:8.500109\n",
            "[INFO 23-11-04 22:54:54.2992 UTC gradient_boosted_trees.cc:1556] \tnum-trees:24 train-loss:8.011723 train-rmse:8.011723 valid-loss:8.282971 valid-rmse:8.282971\n",
            "[INFO 23-11-04 22:55:24.4317 UTC gradient_boosted_trees.cc:1556] \tnum-trees:265 train-loss:7.538394 train-rmse:7.538394 valid-loss:8.090806 valid-rmse:8.090806\n",
            "[INFO 23-11-04 22:55:54.4903 UTC gradient_boosted_trees.cc:1556] \tnum-trees:39 train-loss:7.808694 train-rmse:7.808694 valid-loss:8.234015 valid-rmse:8.234015\n",
            "[INFO 23-11-04 22:56:24.9363 UTC gradient_boosted_trees.cc:1556] \tnum-trees:254 train-loss:6.341332 train-rmse:6.341332 valid-loss:7.820684 valid-rmse:7.820684\n",
            "[INFO 23-11-04 22:56:55.1118 UTC gradient_boosted_trees.cc:1556] \tnum-trees:259 train-loss:7.274105 train-rmse:7.274105 valid-loss:8.041380 valid-rmse:8.041380\n",
            "[INFO 23-11-04 22:57:26.8570 UTC gradient_boosted_trees.cc:1556] \tnum-trees:34 train-loss:7.886547 train-rmse:7.886547 valid-loss:8.260016 valid-rmse:8.260016\n",
            "[INFO 23-11-04 22:57:57.7586 UTC gradient_boosted_trees.cc:1556] \tnum-trees:36 train-loss:7.869268 train-rmse:7.869268 valid-loss:8.270729 valid-rmse:8.270729\n",
            "[INFO 23-11-04 22:58:28.3887 UTC gradient_boosted_trees.cc:1556] \tnum-trees:38 train-loss:7.851595 train-rmse:7.851595 valid-loss:8.269297 valid-rmse:8.269297\n",
            "[INFO 23-11-04 22:58:59.0547 UTC gradient_boosted_trees.cc:1556] \tnum-trees:40 train-loss:7.819823 train-rmse:7.819823 valid-loss:8.254162 valid-rmse:8.254162\n",
            "[INFO 23-11-04 22:59:29.7966 UTC gradient_boosted_trees.cc:1556] \tnum-trees:42 train-loss:7.796034 train-rmse:7.796034 valid-loss:8.249496 valid-rmse:8.249496\n",
            "[INFO 23-11-04 23:00:00.6598 UTC gradient_boosted_trees.cc:1556] \tnum-trees:44 train-loss:7.775277 train-rmse:7.775277 valid-loss:8.250475 valid-rmse:8.250475\n",
            "[INFO 23-11-04 23:00:31.0689 UTC gradient_boosted_trees.cc:1556] \tnum-trees:46 train-loss:7.744911 train-rmse:7.744911 valid-loss:8.247167 valid-rmse:8.247167\n",
            "[INFO 23-11-04 23:01:01.4925 UTC gradient_boosted_trees.cc:1556] \tnum-trees:49 train-loss:8.402195 train-rmse:8.402195 valid-loss:8.444781 valid-rmse:8.444781\n",
            "[INFO 23-11-04 23:01:32.5220 UTC gradient_boosted_trees.cc:1556] \tnum-trees:50 train-loss:7.714337 train-rmse:7.714337 valid-loss:8.234106 valid-rmse:8.234106\n",
            "[INFO 23-11-04 23:02:03.6987 UTC gradient_boosted_trees.cc:1556] \tnum-trees:52 train-loss:7.694942 train-rmse:7.694942 valid-loss:8.230694 valid-rmse:8.230694\n",
            "[INFO 23-11-04 23:02:34.7331 UTC gradient_boosted_trees.cc:1556] \tnum-trees:54 train-loss:7.679659 train-rmse:7.679659 valid-loss:8.216411 valid-rmse:8.216411\n",
            "[INFO 23-11-04 23:03:05.5520 UTC gradient_boosted_trees.cc:1556] \tnum-trees:56 train-loss:7.670261 train-rmse:7.670261 valid-loss:8.214481 valid-rmse:8.214481\n",
            "[INFO 23-11-04 23:03:35.7925 UTC gradient_boosted_trees.cc:1556] \tnum-trees:144 train-loss:7.797097 train-rmse:7.797097 valid-loss:8.266138 valid-rmse:8.266138\n",
            "[INFO 23-11-04 23:04:06.5343 UTC gradient_boosted_trees.cc:1556] \tnum-trees:60 train-loss:7.621384 train-rmse:7.621384 valid-loss:8.214643 valid-rmse:8.214643\n",
            "[INFO 23-11-04 23:04:37.2084 UTC gradient_boosted_trees.cc:1556] \tnum-trees:62 train-loss:7.604839 train-rmse:7.604839 valid-loss:8.204081 valid-rmse:8.204081\n",
            "[INFO 23-11-04 23:05:07.6039 UTC gradient_boosted_trees.cc:1556] \tnum-trees:148 train-loss:7.784283 train-rmse:7.784283 valid-loss:8.259418 valid-rmse:8.259418\n",
            "[INFO 23-11-04 23:05:38.6208 UTC gradient_boosted_trees.cc:1556] \tnum-trees:66 train-loss:7.566856 train-rmse:7.566856 valid-loss:8.194194 valid-rmse:8.194194\n",
            "[INFO 23-11-04 23:06:09.4918 UTC gradient_boosted_trees.cc:1556] \tnum-trees:68 train-loss:7.555339 train-rmse:7.555339 valid-loss:8.192090 valid-rmse:8.192090\n",
            "[INFO 23-11-04 23:06:39.7071 UTC gradient_boosted_trees.cc:1556] \tnum-trees:152 train-loss:7.771960 train-rmse:7.771960 valid-loss:8.251042 valid-rmse:8.251042\n",
            "[INFO 23-11-04 23:07:10.8663 UTC gradient_boosted_trees.cc:1556] \tnum-trees:72 train-loss:7.509361 train-rmse:7.509361 valid-loss:8.170771 valid-rmse:8.170771\n",
            "[INFO 23-11-04 23:07:41.1704 UTC gradient_boosted_trees.cc:1556] \tnum-trees:293 train-loss:7.482705 train-rmse:7.482705 valid-loss:8.069395 valid-rmse:8.069395\n",
            "[INFO 23-11-04 23:08:11.9011 UTC gradient_boosted_trees.cc:1556] \tnum-trees:76 train-loss:7.483064 train-rmse:7.483064 valid-loss:8.162494 valid-rmse:8.162494\n",
            "[INFO 23-11-04 23:08:42.8017 UTC gradient_boosted_trees.cc:1556] \tnum-trees:78 train-loss:7.472804 train-rmse:7.472804 valid-loss:8.162293 valid-rmse:8.162293\n",
            "[INFO 23-11-04 23:09:13.3104 UTC gradient_boosted_trees.cc:1556] \tnum-trees:80 train-loss:7.455553 train-rmse:7.455553 valid-loss:8.147305 valid-rmse:8.147305\n",
            "[INFO 23-11-04 23:09:44.2568 UTC gradient_boosted_trees.cc:1556] \tnum-trees:82 train-loss:7.446144 train-rmse:7.446144 valid-loss:8.135731 valid-rmse:8.135731\n",
            "[INFO 23-11-04 23:10:14.5174 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:7.422849 train-rmse:7.422849 valid-loss:8.132931 valid-rmse:8.132931\n",
            "[INFO 23-11-04 23:10:36.3159 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.467690 train-rmse:7.467690 valid-loss:8.064923 valid-rmse:8.064923\n",
            "[INFO 23-11-04 23:10:36.3159 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 23:10:36.3159 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.064923 valid-rmse:8.064923\n",
            "[INFO 23-11-04 23:10:36.3198 UTC hyperparameters_optimizer.cc:582] [31/100] Score: -8.06492 / -7.72918 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 16 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-04 23:10:36.3211 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 23:10:36.3211 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 23:10:36.3339 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 23:10:45.0193 UTC gradient_boosted_trees.cc:1556] \tnum-trees:86 train-loss:7.407155 train-rmse:7.407155 valid-loss:8.125699 valid-rmse:8.125699\n",
            "[INFO 23-11-04 23:10:55.5294 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.639991 train-rmse:8.639991 valid-loss:8.550288 valid-rmse:8.550288\n",
            "[INFO 23-11-04 23:11:15.3198 UTC gradient_boosted_trees.cc:1556] \tnum-trees:88 train-loss:7.387565 train-rmse:7.387565 valid-loss:8.116327 valid-rmse:8.116327\n",
            "[INFO 23-11-04 23:11:46.8201 UTC gradient_boosted_trees.cc:1556] \tnum-trees:99 train-loss:8.311144 train-rmse:8.311144 valid-loss:8.404433 valid-rmse:8.404433\n",
            "[INFO 23-11-04 23:12:18.7513 UTC gradient_boosted_trees.cc:1556] \tnum-trees:201 train-loss:7.473023 train-rmse:7.473023 valid-loss:8.124523 valid-rmse:8.124523\n",
            "[INFO 23-11-04 23:12:51.2344 UTC gradient_boosted_trees.cc:1556] \tnum-trees:104 train-loss:8.303613 train-rmse:8.303613 valid-loss:8.403001 valid-rmse:8.403001\n",
            "[INFO 23-11-04 23:13:23.9844 UTC gradient_boosted_trees.cc:1556] \tnum-trees:289 train-loss:6.197513 train-rmse:6.197513 valid-loss:7.782769 valid-rmse:7.782769\n",
            "[INFO 23-11-04 23:13:55.0005 UTC gradient_boosted_trees.cc:1556] \tnum-trees:171 train-loss:7.715349 train-rmse:7.715349 valid-loss:8.223142 valid-rmse:8.223142\n",
            "[INFO 23-11-04 23:14:25.8110 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.530499 train-rmse:5.530499 valid-loss:7.710152 valid-rmse:7.710152\n",
            "[INFO 23-11-04 23:14:25.8110 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 23:14:25.8110 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.710152 valid-rmse:7.710152\n",
            "[INFO 23-11-04 23:14:25.8199 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 23:14:25.8199 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 23:14:25.8275 UTC hyperparameters_optimizer.cc:582] [32/100] Score: -7.71015 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-04 23:14:25.8354 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 23:14:26.0981 UTC gradient_boosted_trees.cc:1556] \tnum-trees:288 train-loss:7.259279 train-rmse:7.259279 valid-loss:8.050941 valid-rmse:8.050941\n",
            "[INFO 23-11-04 23:14:47.7614 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.622901 train-rmse:8.622901 valid-loss:8.550106 valid-rmse:8.550106\n",
            "[INFO 23-11-04 23:14:59.0150 UTC gradient_boosted_trees.cc:1556] \tnum-trees:114 train-loss:8.288424 train-rmse:8.288424 valid-loss:8.397222 valid-rmse:8.397222\n",
            "[INFO 23-11-04 23:15:31.7587 UTC gradient_boosted_trees.cc:1556] \tnum-trees:105 train-loss:7.249580 train-rmse:7.249580 valid-loss:8.101745 valid-rmse:8.101745\n",
            "[INFO 23-11-04 23:16:02.2757 UTC gradient_boosted_trees.cc:1556] \tnum-trees:107 train-loss:7.238539 train-rmse:7.238539 valid-loss:8.094348 valid-rmse:8.094348\n",
            "[INFO 23-11-04 23:16:32.8839 UTC gradient_boosted_trees.cc:1556] \tnum-trees:212 train-loss:7.442946 train-rmse:7.442946 valid-loss:8.115357 valid-rmse:8.115357\n",
            "[INFO 23-11-04 23:17:03.1680 UTC gradient_boosted_trees.cc:1556] \tnum-trees:111 train-loss:7.211815 train-rmse:7.211815 valid-loss:8.084431 valid-rmse:8.084431\n",
            "[INFO 23-11-04 23:17:33.5977 UTC gradient_boosted_trees.cc:1556] \tnum-trees:113 train-loss:7.201228 train-rmse:7.201228 valid-loss:8.081433 valid-rmse:8.081433\n",
            "[INFO 23-11-04 23:18:03.7451 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:7.187181 train-rmse:7.187181 valid-loss:8.080224 valid-rmse:8.080224\n",
            "[INFO 23-11-04 23:18:33.9520 UTC gradient_boosted_trees.cc:1556] \tnum-trees:117 train-loss:7.171220 train-rmse:7.171220 valid-loss:8.077979 valid-rmse:8.077979\n",
            "[INFO 23-11-04 23:18:46.7850 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.148138 train-rmse:6.148138 valid-loss:7.771649 valid-rmse:7.771649\n",
            "[INFO 23-11-04 23:18:46.7851 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-04 23:18:46.7851 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.771312 valid-rmse:7.771312\n",
            "[INFO 23-11-04 23:18:46.7970 UTC hyperparameters_optimizer.cc:582] [33/100] Score: -7.77131 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-04 23:18:46.8018 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 23:18:46.8023 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 23:18:46.8148 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 23:19:03.8236 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.173727 train-rmse:7.173727 valid-loss:8.010628 valid-rmse:8.010628\n",
            "[INFO 23-11-04 23:19:03.8237 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 23:19:03.8237 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.010628 valid-rmse:8.010628\n",
            "[INFO 23-11-04 23:19:03.8333 UTC hyperparameters_optimizer.cc:582] [34/100] Score: -8.01063 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 256 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-04 23:19:03.8337 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 23:19:03.8337 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 23:19:03.8634 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 23:19:04.8529 UTC gradient_boosted_trees.cc:1556] \tnum-trees:119 train-loss:7.161779 train-rmse:7.161779 valid-loss:8.076644 valid-rmse:8.076644\n",
            "[INFO 23-11-04 23:19:23.2313 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.632155 train-rmse:8.632155 valid-loss:8.546729 valid-rmse:8.546729\n",
            "[INFO 23-11-04 23:19:28.2173 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.639122 train-rmse:8.639122 valid-loss:8.550623 valid-rmse:8.550623\n",
            "[INFO 23-11-04 23:19:35.5837 UTC gradient_boosted_trees.cc:1556] \tnum-trees:121 train-loss:7.153395 train-rmse:7.153395 valid-loss:8.074262 valid-rmse:8.074262\n",
            "[INFO 23-11-04 23:19:43.3345 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.234424 train-rmse:7.234424 valid-loss:8.042912 valid-rmse:8.042912\n",
            "[INFO 23-11-04 23:19:43.3345 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 23:19:43.3345 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.042912 valid-rmse:8.042912\n",
            "[INFO 23-11-04 23:19:43.3403 UTC hyperparameters_optimizer.cc:582] [35/100] Score: -8.04291 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-04 23:19:43.3494 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 23:19:43.3494 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 23:19:43.3605 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 23:20:05.7272 UTC gradient_boosted_trees.cc:1556] \tnum-trees:280 train-loss:6.323759 train-rmse:6.323759 valid-loss:7.866389 valid-rmse:7.866389\n",
            "[INFO 23-11-04 23:20:06.1907 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.577724 train-rmse:8.577724 valid-loss:8.517019 valid-rmse:8.517019\n",
            "[INFO 23-11-04 23:20:36.3218 UTC gradient_boosted_trees.cc:1556] \tnum-trees:3 train-loss:8.594774 train-rmse:8.594774 valid-loss:8.534365 valid-rmse:8.534365\n",
            "[INFO 23-11-04 23:21:07.3882 UTC gradient_boosted_trees.cc:1556] \tnum-trees:127 train-loss:7.100193 train-rmse:7.100193 valid-loss:8.061661 valid-rmse:8.061661\n",
            "[INFO 23-11-04 23:21:37.8801 UTC gradient_boosted_trees.cc:1556] \tnum-trees:129 train-loss:7.082682 train-rmse:7.082682 valid-loss:8.054878 valid-rmse:8.054878\n",
            "[INFO 23-11-04 23:22:12.4832 UTC gradient_boosted_trees.cc:1556] \tnum-trees:148 train-loss:8.244739 train-rmse:8.244739 valid-loss:8.371520 valid-rmse:8.371520\n",
            "[INFO 23-11-04 23:22:44.4704 UTC gradient_boosted_trees.cc:1556] \tnum-trees:8 train-loss:8.551143 train-rmse:8.551143 valid-loss:8.511659 valid-rmse:8.511659\n",
            "[INFO 23-11-04 23:23:16.8705 UTC gradient_boosted_trees.cc:1556] \tnum-trees:153 train-loss:8.238664 train-rmse:8.238664 valid-loss:8.368834 valid-rmse:8.368834\n",
            "[INFO 23-11-04 23:23:48.0356 UTC gradient_boosted_trees.cc:1556] \tnum-trees:197 train-loss:7.640211 train-rmse:7.640211 valid-loss:8.181946 valid-rmse:8.181946\n",
            "[INFO 23-11-04 23:24:18.6046 UTC gradient_boosted_trees.cc:1556] \tnum-trees:232 train-loss:7.383595 train-rmse:7.383595 valid-loss:8.098619 valid-rmse:8.098619\n",
            "[INFO 23-11-04 23:24:48.6302 UTC gradient_boosted_trees.cc:1556] \tnum-trees:44 train-loss:8.267155 train-rmse:8.267155 valid-loss:8.392306 valid-rmse:8.392306\n",
            "[INFO 23-11-04 23:25:19.6703 UTC gradient_boosted_trees.cc:1556] \tnum-trees:201 train-loss:7.629742 train-rmse:7.629742 valid-loss:8.178353 valid-rmse:8.178353\n",
            "[INFO 23-11-04 23:25:50.7453 UTC gradient_boosted_trees.cc:1556] \tnum-trees:165 train-loss:8.225757 train-rmse:8.225757 valid-loss:8.362597 valid-rmse:8.362597\n",
            "[INFO 23-11-04 23:26:24.8778 UTC gradient_boosted_trees.cc:1556] \tnum-trees:293 train-loss:6.257810 train-rmse:6.257810 valid-loss:7.852008 valid-rmse:7.852008\n",
            "[INFO 23-11-04 23:26:55.2521 UTC gradient_boosted_trees.cc:1556] \tnum-trees:170 train-loss:8.219871 train-rmse:8.219871 valid-loss:8.361636 valid-rmse:8.361636\n",
            "[INFO 23-11-04 23:27:25.7861 UTC gradient_boosted_trees.cc:1556] \tnum-trees:99 train-loss:7.324900 train-rmse:7.324900 valid-loss:8.103231 valid-rmse:8.103231\n",
            "[INFO 23-11-04 23:27:56.0955 UTC gradient_boosted_trees.cc:1556] \tnum-trees:33 train-loss:8.108878 train-rmse:8.108878 valid-loss:8.322453 valid-rmse:8.322453\n",
            "[INFO 23-11-04 23:28:26.1806 UTC gradient_boosted_trees.cc:1556] \tnum-trees:177 train-loss:8.213136 train-rmse:8.213136 valid-loss:8.357643 valid-rmse:8.357643\n",
            "[INFO 23-11-04 23:28:57.4333 UTC gradient_boosted_trees.cc:1556] \tnum-trees:24 train-loss:7.812755 train-rmse:7.812755 valid-loss:8.243229 valid-rmse:8.243229\n",
            "[INFO 23-11-04 23:29:30.4398 UTC gradient_boosted_trees.cc:1556] \tnum-trees:182 train-loss:8.207006 train-rmse:8.207006 valid-loss:8.352235 valid-rmse:8.352235\n",
            "[INFO 23-11-04 23:29:52.2977 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.228121 train-rmse:6.228121 valid-loss:7.845776 valid-rmse:7.845776\n",
            "[INFO 23-11-04 23:29:52.2977 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 23:29:52.2977 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.845776 valid-rmse:7.845776\n",
            "[INFO 23-11-04 23:29:52.3080 UTC hyperparameters_optimizer.cc:582] [36/100] Score: -7.84578 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-04 23:29:52.3126 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 23:29:52.3138 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 23:29:52.3267 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 23:30:00.8247 UTC gradient_boosted_trees.cc:1556] \tnum-trees:60 train-loss:8.191492 train-rmse:8.191492 valid-loss:8.363813 valid-rmse:8.363813\n",
            "[INFO 23-11-04 23:30:14.5337 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.583258 train-rmse:8.583258 valid-loss:8.532489 valid-rmse:8.532489\n",
            "[INFO 23-11-04 23:30:33.8352 UTC gradient_boosted_trees.cc:1556] \tnum-trees:24 train-loss:8.396132 train-rmse:8.396132 valid-loss:8.445013 valid-rmse:8.445013\n",
            "[INFO 23-11-04 23:31:04.7114 UTC gradient_boosted_trees.cc:1556] \tnum-trees:25 train-loss:8.387476 train-rmse:8.387476 valid-loss:8.440106 valid-rmse:8.440106\n",
            "[INFO 23-11-04 23:31:34.8492 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:7.327349 train-rmse:7.327349 valid-loss:8.088835 valid-rmse:8.088835\n",
            "[INFO 23-11-04 23:32:05.0879 UTC gradient_boosted_trees.cc:1556] \tnum-trees:27 train-loss:8.371260 train-rmse:8.371260 valid-loss:8.434522 valid-rmse:8.434522\n",
            "[INFO 23-11-04 23:32:35.9296 UTC gradient_boosted_trees.cc:1556] \tnum-trees:28 train-loss:8.364105 train-rmse:8.364105 valid-loss:8.430952 valid-rmse:8.430952\n",
            "[INFO 23-11-04 23:33:06.3384 UTC gradient_boosted_trees.cc:1556] \tnum-trees:29 train-loss:8.355676 train-rmse:8.355676 valid-loss:8.428798 valid-rmse:8.428798\n",
            "[INFO 23-11-04 23:33:36.3384 UTC gradient_boosted_trees.cc:1556] \tnum-trees:30 train-loss:8.347438 train-rmse:8.347438 valid-loss:8.427000 valid-rmse:8.427000\n",
            "[INFO 23-11-04 23:34:06.8490 UTC gradient_boosted_trees.cc:1556] \tnum-trees:31 train-loss:8.340585 train-rmse:8.340585 valid-loss:8.424096 valid-rmse:8.424096\n",
            "[INFO 23-11-04 23:34:37.1228 UTC gradient_boosted_trees.cc:1556] \tnum-trees:32 train-loss:8.333419 train-rmse:8.333419 valid-loss:8.418064 valid-rmse:8.418064\n",
            "[INFO 23-11-04 23:35:07.5823 UTC gradient_boosted_trees.cc:1556] \tnum-trees:27 train-loss:8.228410 train-rmse:8.228410 valid-loss:8.411516 valid-rmse:8.411516\n",
            "[INFO 23-11-04 23:35:37.7309 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:7.221794 train-rmse:7.221794 valid-loss:8.076702 valid-rmse:8.076702\n",
            "[INFO 23-11-04 23:36:08.9392 UTC gradient_boosted_trees.cc:1556] \tnum-trees:35 train-loss:8.313918 train-rmse:8.313918 valid-loss:8.413651 valid-rmse:8.413651\n",
            "[INFO 23-11-04 23:36:39.4876 UTC gradient_boosted_trees.cc:1556] \tnum-trees:36 train-loss:8.307418 train-rmse:8.307418 valid-loss:8.409335 valid-rmse:8.409335\n",
            "[INFO 23-11-04 23:37:09.9891 UTC gradient_boosted_trees.cc:1556] \tnum-trees:37 train-loss:8.300986 train-rmse:8.300986 valid-loss:8.406079 valid-rmse:8.406079\n",
            "[INFO 23-11-04 23:37:40.1208 UTC gradient_boosted_trees.cc:1556] \tnum-trees:38 train-loss:8.294873 train-rmse:8.294873 valid-loss:8.402313 valid-rmse:8.402313\n",
            "[INFO 23-11-04 23:38:11.8016 UTC gradient_boosted_trees.cc:1556] \tnum-trees:195 train-loss:6.611565 train-rmse:6.611565 valid-loss:7.965898 valid-rmse:7.965898\n",
            "[INFO 23-11-04 23:38:41.8711 UTC gradient_boosted_trees.cc:1556] \tnum-trees:49 train-loss:7.442150 train-rmse:7.442150 valid-loss:8.132681 valid-rmse:8.132681\n",
            "[INFO 23-11-04 23:39:12.5899 UTC gradient_boosted_trees.cc:1556] \tnum-trees:199 train-loss:6.587042 train-rmse:6.587042 valid-loss:7.956665 valid-rmse:7.956665\n",
            "[INFO 23-11-04 23:39:42.8924 UTC gradient_boosted_trees.cc:1556] \tnum-trees:272 train-loss:7.270541 train-rmse:7.270541 valid-loss:8.069095 valid-rmse:8.069095\n",
            "[INFO 23-11-04 23:40:14.4092 UTC gradient_boosted_trees.cc:1556] \tnum-trees:203 train-loss:6.571866 train-rmse:6.571866 valid-loss:7.949296 valid-rmse:7.949296\n",
            "[INFO 23-11-04 23:40:45.2025 UTC gradient_boosted_trees.cc:1556] \tnum-trees:93 train-loss:8.078402 train-rmse:8.078402 valid-loss:8.322284 valid-rmse:8.322284\n",
            "[INFO 23-11-04 23:41:15.7897 UTC gradient_boosted_trees.cc:1556] \tnum-trees:207 train-loss:6.546745 train-rmse:6.546745 valid-loss:7.950831 valid-rmse:7.950831\n",
            "[INFO 23-11-04 23:41:46.2266 UTC gradient_boosted_trees.cc:1556] \tnum-trees:209 train-loss:6.533589 train-rmse:6.533589 valid-loss:7.947750 valid-rmse:7.947750\n",
            "[INFO 23-11-04 23:42:16.9149 UTC gradient_boosted_trees.cc:1556] \tnum-trees:211 train-loss:6.521172 train-rmse:6.521172 valid-loss:7.947567 valid-rmse:7.947567\n",
            "[INFO 23-11-04 23:42:47.5605 UTC gradient_boosted_trees.cc:1556] \tnum-trees:213 train-loss:6.506159 train-rmse:6.506159 valid-loss:7.948692 valid-rmse:7.948692\n",
            "[INFO 23-11-04 23:43:17.6235 UTC gradient_boosted_trees.cc:1556] \tnum-trees:215 train-loss:6.491058 train-rmse:6.491058 valid-loss:7.935246 valid-rmse:7.935246\n",
            "[INFO 23-11-04 23:43:47.8930 UTC gradient_boosted_trees.cc:1556] \tnum-trees:217 train-loss:6.477417 train-rmse:6.477417 valid-loss:7.933608 valid-rmse:7.933608\n",
            "[INFO 23-11-04 23:44:18.3120 UTC gradient_boosted_trees.cc:1556] \tnum-trees:219 train-loss:6.462627 train-rmse:6.462627 valid-loss:7.928420 valid-rmse:7.928420\n",
            "[INFO 23-11-04 23:44:49.7269 UTC gradient_boosted_trees.cc:1556] \tnum-trees:43 train-loss:8.057851 train-rmse:8.057851 valid-loss:8.347989 valid-rmse:8.347989\n",
            "[INFO 23-11-04 23:45:20.9458 UTC gradient_boosted_trees.cc:1556] \tnum-trees:254 train-loss:7.486900 train-rmse:7.486900 valid-loss:8.130813 valid-rmse:8.130813\n",
            "[INFO 23-11-04 23:45:55.5612 UTC gradient_boosted_trees.cc:1556] \tnum-trees:42 train-loss:7.615535 train-rmse:7.615535 valid-loss:8.220411 valid-rmse:8.220411\n",
            "[INFO 23-11-04 23:46:26.1141 UTC gradient_boosted_trees.cc:1556] \tnum-trees:69 train-loss:7.211819 train-rmse:7.211819 valid-loss:8.075973 valid-rmse:8.075973\n",
            "[INFO 23-11-04 23:46:59.8861 UTC gradient_boosted_trees.cc:1556] \tnum-trees:264 train-loss:8.131333 train-rmse:8.131333 valid-loss:8.318706 valid-rmse:8.318706\n",
            "[INFO 23-11-04 23:47:30.7938 UTC gradient_boosted_trees.cc:1556] \tnum-trees:58 train-loss:8.185190 train-rmse:8.185190 valid-loss:8.367432 valid-rmse:8.367432\n",
            "[INFO 23-11-04 23:48:01.5329 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:8.013137 train-rmse:8.013137 valid-loss:8.297877 valid-rmse:8.297877\n",
            "[INFO 23-11-04 23:48:33.4818 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:7.204729 train-rmse:7.204729 valid-loss:8.050935 valid-rmse:8.050935\n",
            "[INFO 23-11-04 23:49:04.7023 UTC gradient_boosted_trees.cc:1556] \tnum-trees:238 train-loss:6.361337 train-rmse:6.361337 valid-loss:7.928514 valid-rmse:7.928514\n",
            "[INFO 23-11-04 23:49:35.7223 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:6.353333 train-rmse:6.353333 valid-loss:7.923757 valid-rmse:7.923757\n",
            "[INFO 23-11-04 23:50:07.4964 UTC gradient_boosted_trees.cc:1556] \tnum-trees:53 train-loss:7.464428 train-rmse:7.464428 valid-loss:8.151330 valid-rmse:8.151330\n",
            "[INFO 23-11-04 23:50:30.1460 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.195752 train-rmse:7.195752 valid-loss:8.048820 valid-rmse:8.048820\n",
            "[INFO 23-11-04 23:50:30.1461 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-04 23:50:30.1461 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:8.048776 valid-rmse:8.048776\n",
            "[INFO 23-11-04 23:50:30.1551 UTC hyperparameters_optimizer.cc:582] [37/100] Score: -8.04878 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 128 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-04 23:50:30.1560 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 23:50:30.1560 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 23:50:30.1793 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 23:50:37.8370 UTC gradient_boosted_trees.cc:1556] \tnum-trees:281 train-loss:8.115458 train-rmse:8.115458 valid-loss:8.310750 valid-rmse:8.310750\n",
            "[INFO 23-11-04 23:50:57.5578 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.615951 train-rmse:8.615951 valid-loss:8.538241 valid-rmse:8.538241\n",
            "[INFO 23-11-04 23:51:16.3665 UTC gradient_boosted_trees.cc:1556] \tnum-trees:284 train-loss:8.113028 train-rmse:8.113028 valid-loss:8.309549 valid-rmse:8.309549\n",
            "[INFO 23-11-04 23:51:47.7518 UTC gradient_boosted_trees.cc:1556] \tnum-trees:249 train-loss:6.299161 train-rmse:6.299161 valid-loss:7.921725 valid-rmse:7.921725\n",
            "[INFO 23-11-04 23:52:17.9251 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:6.291978 train-rmse:6.291978 valid-loss:7.922378 valid-rmse:7.922378\n",
            "[INFO 23-11-04 23:52:48.1973 UTC gradient_boosted_trees.cc:1556] \tnum-trees:253 train-loss:6.280127 train-rmse:6.280127 valid-loss:7.918909 valid-rmse:7.918909\n",
            "[INFO 23-11-04 23:53:18.2779 UTC gradient_boosted_trees.cc:1556] \tnum-trees:275 train-loss:7.436806 train-rmse:7.436806 valid-loss:8.110888 valid-rmse:8.110888\n",
            "[INFO 23-11-04 23:53:48.3516 UTC gradient_boosted_trees.cc:1556] \tnum-trees:257 train-loss:6.256182 train-rmse:6.256182 valid-loss:7.916256 valid-rmse:7.916256\n",
            "[INFO 23-11-04 23:54:18.4071 UTC gradient_boosted_trees.cc:1556] \tnum-trees:259 train-loss:6.244987 train-rmse:6.244987 valid-loss:7.917880 valid-rmse:7.917880\n",
            "[INFO 23-11-04 23:54:41.7271 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:8.099680 train-rmse:8.099680 valid-loss:8.302067 valid-rmse:8.302067\n",
            "[INFO 23-11-04 23:54:41.7271 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-04 23:54:41.7271 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.302067 valid-rmse:8.302067\n",
            "[INFO 23-11-04 23:54:41.7284 UTC hyperparameters_optimizer.cc:582] [38/100] Score: -8.30207 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-04 23:54:41.7305 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-04 23:54:41.7306 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-04 23:54:41.7424 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-04 23:54:50.4734 UTC gradient_boosted_trees.cc:1556] \tnum-trees:279 train-loss:7.428120 train-rmse:7.428120 valid-loss:8.108053 valid-rmse:8.108053\n",
            "[INFO 23-11-04 23:54:54.8321 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.643722 train-rmse:8.643722 valid-loss:8.554715 valid-rmse:8.554715\n",
            "[INFO 23-11-04 23:55:21.0597 UTC gradient_boosted_trees.cc:1556] \tnum-trees:3 train-loss:8.624692 train-rmse:8.624692 valid-loss:8.547092 valid-rmse:8.547092\n",
            "[INFO 23-11-04 23:55:52.6116 UTC gradient_boosted_trees.cc:1556] \tnum-trees:68 train-loss:7.269577 train-rmse:7.269577 valid-loss:8.059665 valid-rmse:8.059665\n",
            "[INFO 23-11-04 23:56:24.0862 UTC gradient_boosted_trees.cc:1556] \tnum-trees:62 train-loss:7.899416 train-rmse:7.899416 valid-loss:8.301482 valid-rmse:8.301482\n",
            "[INFO 23-11-04 23:56:54.1387 UTC gradient_boosted_trees.cc:1556] \tnum-trees:142 train-loss:7.934760 train-rmse:7.934760 valid-loss:8.276758 valid-rmse:8.276758\n",
            "[INFO 23-11-04 23:57:24.9306 UTC gradient_boosted_trees.cc:1556] \tnum-trees:72 train-loss:7.239609 train-rmse:7.239609 valid-loss:8.049976 valid-rmse:8.049976\n",
            "[INFO 23-11-04 23:57:57.7214 UTC gradient_boosted_trees.cc:1556] \tnum-trees:249 train-loss:5.630933 train-rmse:5.630933 valid-loss:7.780616 valid-rmse:7.780616\n",
            "[INFO 23-11-04 23:58:28.7917 UTC gradient_boosted_trees.cc:1556] \tnum-trees:250 train-loss:5.624905 train-rmse:5.624905 valid-loss:7.778180 valid-rmse:7.778180\n",
            "[INFO 23-11-04 23:58:59.2876 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:5.611395 train-rmse:5.611395 valid-loss:7.773727 valid-rmse:7.773727\n",
            "[INFO 23-11-04 23:59:31.7961 UTC gradient_boosted_trees.cc:1556] \tnum-trees:280 train-loss:6.130377 train-rmse:6.130377 valid-loss:7.890649 valid-rmse:7.890649\n",
            "[INFO 23-11-05 00:00:02.0201 UTC gradient_boosted_trees.cc:1556] \tnum-trees:282 train-loss:6.119893 train-rmse:6.119893 valid-loss:7.890612 valid-rmse:7.890612\n",
            "[INFO 23-11-05 00:00:32.3822 UTC gradient_boosted_trees.cc:1556] \tnum-trees:284 train-loss:6.113423 train-rmse:6.113423 valid-loss:7.891541 valid-rmse:7.891541\n",
            "[INFO 23-11-05 00:01:02.4384 UTC gradient_boosted_trees.cc:1556] \tnum-trees:286 train-loss:6.103168 train-rmse:6.103168 valid-loss:7.892937 valid-rmse:7.892937\n",
            "[INFO 23-11-05 00:01:32.8076 UTC gradient_boosted_trees.cc:1556] \tnum-trees:31 train-loss:8.432829 train-rmse:8.432829 valid-loss:8.485032 valid-rmse:8.485032\n",
            "[INFO 23-11-05 00:02:05.3422 UTC gradient_boosted_trees.cc:1556] \tnum-trees:257 train-loss:5.583615 train-rmse:5.583615 valid-loss:7.769039 valid-rmse:7.769039\n",
            "[INFO 23-11-05 00:02:36.6981 UTC gradient_boosted_trees.cc:1556] \tnum-trees:258 train-loss:5.581933 train-rmse:5.581933 valid-loss:7.768822 valid-rmse:7.768822\n",
            "[INFO 23-11-05 00:02:48.6645 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.381924 train-rmse:7.381924 valid-loss:8.091823 valid-rmse:8.091823\n",
            "[INFO 23-11-05 00:02:48.6645 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 00:02:48.6645 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.091823 valid-rmse:8.091823\n",
            "[INFO 23-11-05 00:02:48.6704 UTC hyperparameters_optimizer.cc:582] [39/100] Score: -8.09182 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 00:02:48.6741 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 00:02:48.6746 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 00:02:48.6885 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 00:02:53.6457 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.637665 train-rmse:8.637665 valid-loss:8.551425 valid-rmse:8.551425\n",
            "[INFO 23-11-05 00:03:07.1029 UTC gradient_boosted_trees.cc:1556] \tnum-trees:118 train-loss:7.620459 train-rmse:7.620459 valid-loss:8.211879 valid-rmse:8.211879\n",
            "[INFO 23-11-05 00:03:38.1111 UTC gradient_boosted_trees.cc:1556] \tnum-trees:260 train-loss:5.563464 train-rmse:5.563464 valid-loss:7.763803 valid-rmse:7.763803\n",
            "[INFO 23-11-05 00:04:08.6007 UTC gradient_boosted_trees.cc:1556] \tnum-trees:16 train-loss:8.487783 train-rmse:8.487783 valid-loss:8.477194 valid-rmse:8.477194\n",
            "[INFO 23-11-05 00:04:31.1602 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.028944 train-rmse:6.028944 valid-loss:7.888998 valid-rmse:7.888998\n",
            "[INFO 23-11-05 00:04:31.1602 UTC gradient_boosted_trees.cc:249] Truncates the model to 292 tree(s) i.e. 292  iteration(s).\n",
            "[INFO 23-11-05 00:04:31.1604 UTC gradient_boosted_trees.cc:312] Final model num-trees:292 valid-loss:7.887854 valid-rmse:7.887854\n",
            "[INFO 23-11-05 00:04:31.1670 UTC hyperparameters_optimizer.cc:582] [40/100] Score: -7.88785 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 00:04:31.1730 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 00:04:31.1730 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 00:04:31.1842 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 00:04:31.7205 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.639995 train-rmse:8.639995 valid-loss:8.550391 valid-rmse:8.550391\n",
            "[INFO 23-11-05 00:04:38.6065 UTC gradient_boosted_trees.cc:1556] \tnum-trees:22 train-loss:8.454500 train-rmse:8.454500 valid-loss:8.462613 valid-rmse:8.462613\n",
            "[INFO 23-11-05 00:05:08.7834 UTC gradient_boosted_trees.cc:1556] \tnum-trees:123 train-loss:7.598409 train-rmse:7.598409 valid-loss:8.206228 valid-rmse:8.206228\n",
            "[INFO 23-11-05 00:05:39.0847 UTC gradient_boosted_trees.cc:1556] \tnum-trees:171 train-loss:7.977998 train-rmse:7.977998 valid-loss:8.293251 valid-rmse:8.293251\n",
            "[INFO 23-11-05 00:06:09.2438 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:7.826478 train-rmse:7.826478 valid-loss:8.220855 valid-rmse:8.220855\n",
            "[INFO 23-11-05 00:06:30.4753 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.736103 train-rmse:7.736103 valid-loss:8.189413 valid-rmse:8.189413\n",
            "[INFO 23-11-05 00:06:30.4753 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 00:06:30.4754 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:8.189064 valid-rmse:8.189064\n",
            "[INFO 23-11-05 00:06:30.4815 UTC hyperparameters_optimizer.cc:582] [41/100] Score: -8.18906 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"AXIS_ALIGNED\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 00:06:30.4861 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 00:06:30.4867 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 00:06:30.4996 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 00:06:43.0704 UTC gradient_boosted_trees.cc:1556] \tnum-trees:79 train-loss:7.773607 train-rmse:7.773607 valid-loss:8.265372 valid-rmse:8.265372\n",
            "[INFO 23-11-05 00:06:59.5088 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.618098 train-rmse:8.618098 valid-loss:8.535163 valid-rmse:8.535163\n",
            "[INFO 23-11-05 00:07:13.2229 UTC gradient_boosted_trees.cc:1556] \tnum-trees:53 train-loss:8.354010 train-rmse:8.354010 valid-loss:8.426361 valid-rmse:8.426361\n",
            "[INFO 23-11-05 00:07:44.4137 UTC gradient_boosted_trees.cc:1556] \tnum-trees:175 train-loss:7.852931 train-rmse:7.852931 valid-loss:8.244979 valid-rmse:8.244979\n",
            "[INFO 23-11-05 00:08:17.9055 UTC gradient_boosted_trees.cc:1556] \tnum-trees:66 train-loss:8.330086 train-rmse:8.330086 valid-loss:8.402965 valid-rmse:8.402965\n",
            "[INFO 23-11-05 00:08:47.9906 UTC gradient_boosted_trees.cc:1556] \tnum-trees:5 train-loss:8.493361 train-rmse:8.493361 valid-loss:8.477730 valid-rmse:8.477730\n",
            "[INFO 23-11-05 00:09:18.5501 UTC gradient_boosted_trees.cc:1556] \tnum-trees:128 train-loss:6.658737 train-rmse:6.658737 valid-loss:7.938785 valid-rmse:7.938785\n",
            "[INFO 23-11-05 00:09:50.1535 UTC gradient_boosted_trees.cc:1556] \tnum-trees:102 train-loss:8.011519 train-rmse:8.011519 valid-loss:8.305507 valid-rmse:8.305507\n",
            "[INFO 23-11-05 00:10:20.9032 UTC gradient_boosted_trees.cc:1556] \tnum-trees:85 train-loss:7.729859 train-rmse:7.729859 valid-loss:8.248891 valid-rmse:8.248891\n",
            "[INFO 23-11-05 00:10:51.4902 UTC gradient_boosted_trees.cc:1556] \tnum-trees:132 train-loss:6.628081 train-rmse:6.628081 valid-loss:7.935378 valid-rmse:7.935378\n",
            "[INFO 23-11-05 00:11:21.5174 UTC gradient_boosted_trees.cc:1556] \tnum-trees:186 train-loss:7.827162 train-rmse:7.827162 valid-loss:8.234211 valid-rmse:8.234211\n",
            "[INFO 23-11-05 00:11:51.5230 UTC gradient_boosted_trees.cc:1556] \tnum-trees:106 train-loss:7.998563 train-rmse:7.998563 valid-loss:8.298605 valid-rmse:8.298605\n",
            "[INFO 23-11-05 00:12:22.3033 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:8.260064 train-rmse:8.260064 valid-loss:8.376553 valid-rmse:8.376553\n",
            "[INFO 23-11-05 00:12:52.5431 UTC gradient_boosted_trees.cc:1556] \tnum-trees:188 train-loss:6.829912 train-rmse:6.829912 valid-loss:7.986973 valid-rmse:7.986973\n",
            "[INFO 23-11-05 00:13:24.3797 UTC gradient_boosted_trees.cc:1556] \tnum-trees:109 train-loss:7.987006 train-rmse:7.987006 valid-loss:8.293635 valid-rmse:8.293635\n",
            "[INFO 23-11-05 00:13:56.0128 UTC gradient_boosted_trees.cc:1556] \tnum-trees:110 train-loss:7.984674 train-rmse:7.984674 valid-loss:8.293711 valid-rmse:8.293711\n",
            "[INFO 23-11-05 00:14:26.9007 UTC gradient_boosted_trees.cc:1556] \tnum-trees:90 train-loss:8.248447 train-rmse:8.248447 valid-loss:8.387291 valid-rmse:8.387291\n",
            "[INFO 23-11-05 00:14:57.1434 UTC gradient_boosted_trees.cc:1556] \tnum-trees:146 train-loss:8.221187 train-rmse:8.221187 valid-loss:8.369810 valid-rmse:8.369810\n",
            "[INFO 23-11-05 00:15:30.2151 UTC gradient_boosted_trees.cc:1556] \tnum-trees:113 train-loss:7.975446 train-rmse:7.975446 valid-loss:8.289753 valid-rmse:8.289753\n",
            "[INFO 23-11-05 00:16:01.1537 UTC gradient_boosted_trees.cc:1556] \tnum-trees:114 train-loss:7.971979 train-rmse:7.971979 valid-loss:8.288557 valid-rmse:8.288557\n",
            "[INFO 23-11-05 00:16:31.5469 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:7.968398 train-rmse:7.968398 valid-loss:8.287838 valid-rmse:8.287838\n",
            "[INFO 23-11-05 00:17:01.8933 UTC gradient_boosted_trees.cc:1556] \tnum-trees:171 train-loss:8.196620 train-rmse:8.196620 valid-loss:8.366249 valid-rmse:8.366249\n",
            "[INFO 23-11-05 00:17:34.3560 UTC gradient_boosted_trees.cc:1556] \tnum-trees:117 train-loss:7.962958 train-rmse:7.962958 valid-loss:8.286173 valid-rmse:8.286173\n",
            "[INFO 23-11-05 00:18:04.9363 UTC gradient_boosted_trees.cc:1556] \tnum-trees:126 train-loss:6.786602 train-rmse:6.786602 valid-loss:7.940495 valid-rmse:7.940495\n",
            "[INFO 23-11-05 00:18:35.5433 UTC gradient_boosted_trees.cc:1556] \tnum-trees:208 train-loss:7.780184 train-rmse:7.780184 valid-loss:8.222145 valid-rmse:8.222145\n",
            "[INFO 23-11-05 00:19:06.2535 UTC gradient_boosted_trees.cc:1556] \tnum-trees:196 train-loss:8.170053 train-rmse:8.170053 valid-loss:8.355860 valid-rmse:8.355860\n",
            "[INFO 23-11-05 00:19:36.3520 UTC gradient_boosted_trees.cc:1556] \tnum-trees:130 train-loss:6.756784 train-rmse:6.756784 valid-loss:7.932427 valid-rmse:7.932427\n",
            "[INFO 23-11-05 00:20:07.5560 UTC gradient_boosted_trees.cc:1556] \tnum-trees:292 train-loss:5.368207 train-rmse:5.368207 valid-loss:7.742185 valid-rmse:7.742185\n",
            "[INFO 23-11-05 00:20:38.1611 UTC gradient_boosted_trees.cc:1556] \tnum-trees:163 train-loss:7.430212 train-rmse:7.430212 valid-loss:8.151581 valid-rmse:8.151581\n",
            "[INFO 23-11-05 00:21:08.8365 UTC gradient_boosted_trees.cc:1556] \tnum-trees:294 train-loss:5.354266 train-rmse:5.354266 valid-loss:7.740019 valid-rmse:7.740019\n",
            "[INFO 23-11-05 00:21:40.2324 UTC gradient_boosted_trees.cc:1556] \tnum-trees:32 train-loss:8.064252 train-rmse:8.064252 valid-loss:8.310700 valid-rmse:8.310700\n",
            "[INFO 23-11-05 00:22:11.0060 UTC gradient_boosted_trees.cc:1556] \tnum-trees:233 train-loss:8.136744 train-rmse:8.136744 valid-loss:8.342859 valid-rmse:8.342859\n",
            "[INFO 23-11-05 00:22:45.8307 UTC gradient_boosted_trees.cc:1556] \tnum-trees:168 train-loss:7.410523 train-rmse:7.410523 valid-loss:8.145254 valid-rmse:8.145254\n",
            "[INFO 23-11-05 00:23:15.9021 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:8.125801 train-rmse:8.125801 valid-loss:8.337140 valid-rmse:8.337140\n",
            "[INFO 23-11-05 00:23:49.8985 UTC gradient_boosted_trees.cc:1556] \tnum-trees:129 train-loss:7.928859 train-rmse:7.928859 valid-loss:8.270767 valid-rmse:8.270767\n",
            "[INFO 23-11-05 00:24:13.7682 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.321344 train-rmse:5.321344 valid-loss:7.737123 valid-rmse:7.737123\n",
            "[INFO 23-11-05 00:24:13.7683 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 00:24:13.7683 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.737123 valid-rmse:7.737123\n",
            "[INFO 23-11-05 00:24:13.7770 UTC hyperparameters_optimizer.cc:582] [42/100] Score: -7.73712 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 64 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 00:24:13.7780 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 00:24:13.7780 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 00:24:13.7919 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 00:24:20.8217 UTC gradient_boosted_trees.cc:1556] \tnum-trees:259 train-loss:8.114909 train-rmse:8.114909 valid-loss:8.331346 valid-rmse:8.331346\n",
            "[INFO 23-11-05 00:24:34.9460 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.621452 train-rmse:8.621452 valid-loss:8.542269 valid-rmse:8.542269\n",
            "[INFO 23-11-05 00:24:50.9963 UTC gradient_boosted_trees.cc:1556] \tnum-trees:265 train-loss:8.109962 train-rmse:8.109962 valid-loss:8.334039 valid-rmse:8.334039\n",
            "[INFO 23-11-05 00:25:25.9043 UTC gradient_boosted_trees.cc:1556] \tnum-trees:272 train-loss:8.105783 train-rmse:8.105783 valid-loss:8.332030 valid-rmse:8.332030\n",
            "[INFO 23-11-05 00:25:55.9612 UTC gradient_boosted_trees.cc:1556] \tnum-trees:278 train-loss:8.100201 train-rmse:8.100201 valid-loss:8.332087 valid-rmse:8.332087\n",
            "[INFO 23-11-05 00:26:26.0077 UTC gradient_boosted_trees.cc:1556] \tnum-trees:284 train-loss:8.094460 train-rmse:8.094460 valid-loss:8.331110 valid-rmse:8.331110\n",
            "[INFO 23-11-05 00:26:56.2812 UTC gradient_boosted_trees.cc:1556] \tnum-trees:290 train-loss:8.088376 train-rmse:8.088376 valid-loss:8.325371 valid-rmse:8.325371\n",
            "[INFO 23-11-05 00:27:27.5274 UTC gradient_boosted_trees.cc:1556] \tnum-trees:235 train-loss:7.716288 train-rmse:7.716288 valid-loss:8.203013 valid-rmse:8.203013\n",
            "[INFO 23-11-05 00:27:46.2314 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:8.079627 train-rmse:8.079627 valid-loss:8.323797 valid-rmse:8.323797\n",
            "[INFO 23-11-05 00:27:46.2314 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 00:27:46.2314 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.323797 valid-rmse:8.323797\n",
            "[INFO 23-11-05 00:27:46.2322 UTC hyperparameters_optimizer.cc:582] [43/100] Score: -8.3238 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 3 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 00:27:46.2340 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 00:27:46.2341 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 00:27:46.2455 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 00:27:58.4937 UTC gradient_boosted_trees.cc:1556] \tnum-trees:74 train-loss:7.708011 train-rmse:7.708011 valid-loss:8.189920 valid-rmse:8.189920\n",
            "[INFO 23-11-05 00:28:03.3123 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.580970 train-rmse:8.580970 valid-loss:8.519301 valid-rmse:8.519301\n",
            "[INFO 23-11-05 00:28:30.5158 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:7.701039 train-rmse:7.701039 valid-loss:8.190990 valid-rmse:8.190990\n",
            "[INFO 23-11-05 00:29:01.1670 UTC gradient_boosted_trees.cc:1556] \tnum-trees:183 train-loss:7.352161 train-rmse:7.352161 valid-loss:8.124599 valid-rmse:8.124599\n",
            "[INFO 23-11-05 00:29:32.1768 UTC gradient_boosted_trees.cc:1556] \tnum-trees:156 train-loss:6.541955 train-rmse:6.541955 valid-loss:7.894094 valid-rmse:7.894094\n",
            "[INFO 23-11-05 00:30:04.1903 UTC gradient_boosted_trees.cc:1556] \tnum-trees:182 train-loss:6.211071 train-rmse:6.211071 valid-loss:7.863467 valid-rmse:7.863467\n",
            "[INFO 23-11-05 00:30:35.0817 UTC gradient_boosted_trees.cc:1556] \tnum-trees:163 train-loss:8.116002 train-rmse:8.116002 valid-loss:8.297056 valid-rmse:8.297056\n",
            "[INFO 23-11-05 00:31:05.2483 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:7.692168 train-rmse:7.692168 valid-loss:8.192074 valid-rmse:8.192074\n",
            "[INFO 23-11-05 00:31:35.4623 UTC gradient_boosted_trees.cc:1556] \tnum-trees:13 train-loss:8.126485 train-rmse:8.126485 valid-loss:8.333177 valid-rmse:8.333177\n",
            "[INFO 23-11-05 00:32:07.8453 UTC gradient_boosted_trees.cc:1556] \tnum-trees:82 train-loss:7.660263 train-rmse:7.660263 valid-loss:8.182918 valid-rmse:8.182918\n",
            "[INFO 23-11-05 00:32:39.9084 UTC gradient_boosted_trees.cc:1556] \tnum-trees:24 train-loss:8.191483 train-rmse:8.191483 valid-loss:8.380341 valid-rmse:8.380341\n",
            "[INFO 23-11-05 00:33:13.0037 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:7.648110 train-rmse:7.648110 valid-loss:8.182462 valid-rmse:8.182462\n",
            "[INFO 23-11-05 00:33:43.2679 UTC gradient_boosted_trees.cc:1556] \tnum-trees:27 train-loss:8.159266 train-rmse:8.159266 valid-loss:8.369154 valid-rmse:8.369154\n",
            "[INFO 23-11-05 00:34:14.8797 UTC gradient_boosted_trees.cc:1556] \tnum-trees:193 train-loss:6.125074 train-rmse:6.125074 valid-loss:7.832493 valid-rmse:7.832493\n",
            "[INFO 23-11-05 00:34:46.3429 UTC gradient_boosted_trees.cc:1556] \tnum-trees:30 train-loss:8.122076 train-rmse:8.122076 valid-loss:8.350270 valid-rmse:8.350270\n",
            "[INFO 23-11-05 00:35:16.7225 UTC gradient_boosted_trees.cc:1556] \tnum-trees:184 train-loss:8.085870 train-rmse:8.085870 valid-loss:8.282286 valid-rmse:8.282286\n",
            "[INFO 23-11-05 00:35:50.0885 UTC gradient_boosted_trees.cc:1556] \tnum-trees:33 train-loss:8.095725 train-rmse:8.095725 valid-loss:8.341583 valid-rmse:8.341583\n",
            "[INFO 23-11-05 00:36:20.8336 UTC gradient_boosted_trees.cc:1556] \tnum-trees:174 train-loss:6.385501 train-rmse:6.385501 valid-loss:7.865317 valid-rmse:7.865317\n",
            "[INFO 23-11-05 00:36:51.3107 UTC gradient_boosted_trees.cc:1556] \tnum-trees:203 train-loss:7.280632 train-rmse:7.280632 valid-loss:8.104610 valid-rmse:8.104610\n",
            "[INFO 23-11-05 00:37:22.7978 UTC gradient_boosted_trees.cc:1556] \tnum-trees:265 train-loss:7.653516 train-rmse:7.653516 valid-loss:8.175700 valid-rmse:8.175700\n",
            "[INFO 23-11-05 00:37:53.2808 UTC gradient_boosted_trees.cc:1556] \tnum-trees:34 train-loss:7.774184 train-rmse:7.774184 valid-loss:8.220476 valid-rmse:8.220476\n",
            "[INFO 23-11-05 00:38:23.5265 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:7.649297 train-rmse:7.649297 valid-loss:8.173022 valid-rmse:8.173022\n",
            "[INFO 23-11-05 00:38:54.3032 UTC gradient_boosted_trees.cc:1556] \tnum-trees:159 train-loss:7.838327 train-rmse:7.838327 valid-loss:8.238099 valid-rmse:8.238099\n",
            "[INFO 23-11-05 00:39:24.5965 UTC gradient_boosted_trees.cc:1556] \tnum-trees:182 train-loss:6.325522 train-rmse:6.325522 valid-loss:7.857962 valid-rmse:7.857962\n",
            "[INFO 23-11-05 00:39:59.1067 UTC gradient_boosted_trees.cc:1556] \tnum-trees:41 train-loss:7.695292 train-rmse:7.695292 valid-loss:8.194550 valid-rmse:8.194550\n",
            "[INFO 23-11-05 00:40:31.9733 UTC gradient_boosted_trees.cc:1556] \tnum-trees:185 train-loss:6.287513 train-rmse:6.287513 valid-loss:7.851167 valid-rmse:7.851167\n",
            "[INFO 23-11-05 00:41:06.7439 UTC gradient_boosted_trees.cc:1556] \tnum-trees:48 train-loss:7.960050 train-rmse:7.960050 valid-loss:8.292985 valid-rmse:8.292985\n",
            "[INFO 23-11-05 00:41:39.1630 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:6.580366 train-rmse:6.580366 valid-loss:7.932102 valid-rmse:7.932102\n",
            "[INFO 23-11-05 00:42:09.4555 UTC gradient_boosted_trees.cc:1556] \tnum-trees:51 train-loss:7.938203 train-rmse:7.938203 valid-loss:8.285913 valid-rmse:8.285913\n",
            "[INFO 23-11-05 00:42:39.9223 UTC gradient_boosted_trees.cc:1556] \tnum-trees:50 train-loss:7.575265 train-rmse:7.575265 valid-loss:8.158692 valid-rmse:8.158692\n",
            "[INFO 23-11-05 00:43:12.2944 UTC gradient_boosted_trees.cc:1556] \tnum-trees:192 train-loss:6.240694 train-rmse:6.240694 valid-loss:7.836109 valid-rmse:7.836109\n",
            "[INFO 23-11-05 00:43:44.9498 UTC gradient_boosted_trees.cc:1556] \tnum-trees:221 train-loss:7.215540 train-rmse:7.215540 valid-loss:8.080352 valid-rmse:8.080352\n",
            "[INFO 23-11-05 00:44:16.8289 UTC gradient_boosted_trees.cc:1556] \tnum-trees:57 train-loss:7.897000 train-rmse:7.897000 valid-loss:8.267462 valid-rmse:8.267462\n",
            "[INFO 23-11-05 00:44:49.3933 UTC gradient_boosted_trees.cc:1556] \tnum-trees:171 train-loss:7.805214 train-rmse:7.805214 valid-loss:8.227343 valid-rmse:8.227343\n",
            "[INFO 23-11-05 00:45:20.1700 UTC gradient_boosted_trees.cc:1556] \tnum-trees:59 train-loss:7.469275 train-rmse:7.469275 valid-loss:8.145521 valid-rmse:8.145521\n",
            "[INFO 23-11-05 00:45:51.3337 UTC gradient_boosted_trees.cc:1556] \tnum-trees:232 train-loss:8.024978 train-rmse:8.024978 valid-loss:8.248011 valid-rmse:8.248011\n",
            "[INFO 23-11-05 00:46:23.8186 UTC gradient_boosted_trees.cc:1556] \tnum-trees:63 train-loss:7.859365 train-rmse:7.859365 valid-loss:8.250751 valid-rmse:8.250751\n",
            "[INFO 23-11-05 00:46:56.4897 UTC gradient_boosted_trees.cc:1556] \tnum-trees:294 train-loss:7.594262 train-rmse:7.594262 valid-loss:8.152972 valid-rmse:8.152972\n",
            "[INFO 23-11-05 00:47:27.7974 UTC gradient_boosted_trees.cc:1556] \tnum-trees:66 train-loss:7.839871 train-rmse:7.839871 valid-loss:8.250473 valid-rmse:8.250473\n",
            "[INFO 23-11-05 00:48:00.4522 UTC gradient_boosted_trees.cc:1556] \tnum-trees:68 train-loss:7.377841 train-rmse:7.377841 valid-loss:8.112108 valid-rmse:8.112108\n",
            "[INFO 23-11-05 00:48:30.9637 UTC gradient_boosted_trees.cc:1556] \tnum-trees:244 train-loss:8.011539 train-rmse:8.011539 valid-loss:8.241672 valid-rmse:8.241672\n",
            "[INFO 23-11-05 00:48:55.3077 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.581692 train-rmse:7.581692 valid-loss:8.148060 valid-rmse:8.148060\n",
            "[INFO 23-11-05 00:48:55.3077 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 00:48:55.3077 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.148060 valid-rmse:8.148060\n",
            "[INFO 23-11-05 00:48:55.3122 UTC hyperparameters_optimizer.cc:582] [44/100] Score: -8.14806 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 256 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 00:48:55.3195 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 00:48:55.3196 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 00:48:55.3401 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 00:49:01.1582 UTC gradient_boosted_trees.cc:1556] \tnum-trees:116 train-loss:7.480754 train-rmse:7.480754 valid-loss:8.126218 valid-rmse:8.126218\n",
            "[INFO 23-11-05 00:49:26.7298 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.609970 train-rmse:8.609970 valid-loss:8.539419 valid-rmse:8.539419\n",
            "[INFO 23-11-05 00:49:33.9420 UTC gradient_boosted_trees.cc:1556] \tnum-trees:117 train-loss:7.476400 train-rmse:7.476400 valid-loss:8.125762 valid-rmse:8.125762\n",
            "[INFO 23-11-05 00:50:04.4958 UTC gradient_boosted_trees.cc:1556] \tnum-trees:210 train-loss:6.110104 train-rmse:6.110104 valid-loss:7.801609 valid-rmse:7.801609\n",
            "[INFO 23-11-05 00:50:39.3088 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:7.789502 train-rmse:7.789502 valid-loss:8.229259 valid-rmse:8.229259\n",
            "[INFO 23-11-05 00:51:10.9129 UTC gradient_boosted_trees.cc:1556] \tnum-trees:94 train-loss:7.616367 train-rmse:7.616367 valid-loss:8.106304 valid-rmse:8.106304\n",
            "[INFO 23-11-05 00:51:42.2971 UTC gradient_boosted_trees.cc:1556] \tnum-trees:241 train-loss:7.148727 train-rmse:7.148727 valid-loss:8.072415 valid-rmse:8.072415\n",
            "[INFO 23-11-05 00:52:12.4040 UTC gradient_boosted_trees.cc:1556] \tnum-trees:123 train-loss:7.445893 train-rmse:7.445893 valid-loss:8.117505 valid-rmse:8.117505\n",
            "[INFO 23-11-05 00:52:43.3282 UTC gradient_boosted_trees.cc:1556] \tnum-trees:217 train-loss:6.059191 train-rmse:6.059191 valid-loss:7.787023 valid-rmse:7.787023\n",
            "[INFO 23-11-05 00:53:13.4157 UTC gradient_boosted_trees.cc:1556] \tnum-trees:155 train-loss:7.330629 train-rmse:7.330629 valid-loss:8.108180 valid-rmse:8.108180\n",
            "[INFO 23-11-05 00:53:47.2024 UTC gradient_boosted_trees.cc:1556] \tnum-trees:127 train-loss:7.428515 train-rmse:7.428515 valid-loss:8.111711 valid-rmse:8.111711\n",
            "[INFO 23-11-05 00:54:18.4796 UTC gradient_boosted_trees.cc:1556] \tnum-trees:89 train-loss:7.178286 train-rmse:7.178286 valid-loss:8.061314 valid-rmse:8.061314\n",
            "[INFO 23-11-05 00:54:53.8888 UTC gradient_boosted_trees.cc:1556] \tnum-trees:87 train-loss:7.697327 train-rmse:7.697327 valid-loss:8.193764 valid-rmse:8.193764\n",
            "[INFO 23-11-05 00:55:25.2184 UTC gradient_boosted_trees.cc:1556] \tnum-trees:103 train-loss:7.570487 train-rmse:7.570487 valid-loss:8.093070 valid-rmse:8.093070\n",
            "[INFO 23-11-05 00:55:56.1932 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:5.741937 train-rmse:5.741937 valid-loss:7.784318 valid-rmse:7.784318\n",
            "[INFO 23-11-05 00:56:26.1945 UTC gradient_boosted_trees.cc:1556] \tnum-trees:133 train-loss:7.399844 train-rmse:7.399844 valid-loss:8.106723 valid-rmse:8.106723\n",
            "[INFO 23-11-05 00:56:56.8119 UTC gradient_boosted_trees.cc:1556] \tnum-trees:161 train-loss:7.308702 train-rmse:7.308702 valid-loss:8.100586 valid-rmse:8.100586\n",
            "[INFO 23-11-05 00:57:27.3035 UTC gradient_boosted_trees.cc:1556] \tnum-trees:257 train-loss:7.096011 train-rmse:7.096011 valid-loss:8.056154 valid-rmse:8.056154\n",
            "[INFO 23-11-05 00:58:00.3080 UTC gradient_boosted_trees.cc:1556] \tnum-trees:231 train-loss:5.943205 train-rmse:5.943205 valid-loss:7.771810 valid-rmse:7.771810\n",
            "[INFO 23-11-05 00:58:35.8115 UTC gradient_boosted_trees.cc:1556] \tnum-trees:258 train-loss:5.704563 train-rmse:5.704563 valid-loss:7.779876 valid-rmse:7.779876\n",
            "[INFO 23-11-05 00:59:07.5953 UTC gradient_boosted_trees.cc:1556] \tnum-trees:292 train-loss:7.952167 train-rmse:7.952167 valid-loss:8.211139 valid-rmse:8.211139\n",
            "[INFO 23-11-05 00:59:37.6457 UTC gradient_boosted_trees.cc:1556] \tnum-trees:141 train-loss:7.363072 train-rmse:7.363072 valid-loss:8.104030 valid-rmse:8.104030\n",
            "[INFO 23-11-05 01:00:10.5864 UTC gradient_boosted_trees.cc:1556] \tnum-trees:142 train-loss:7.359293 train-rmse:7.359293 valid-loss:8.099602 valid-rmse:8.099602\n",
            "[INFO 23-11-05 01:00:40.7190 UTC gradient_boosted_trees.cc:1556] \tnum-trees:204 train-loss:7.719202 train-rmse:7.719202 valid-loss:8.197857 valid-rmse:8.197857\n",
            "[INFO 23-11-05 01:00:53.7028 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.944685 train-rmse:7.944685 valid-loss:8.207796 valid-rmse:8.207796\n",
            "[INFO 23-11-05 01:00:53.7028 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 01:00:53.7028 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.207796 valid-rmse:8.207796\n",
            "[INFO 23-11-05 01:00:53.7058 UTC hyperparameters_optimizer.cc:582] [45/100] Score: -8.2078 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 01:00:53.7164 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 01:00:53.7354 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 01:00:53.7523 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 01:01:02.9881 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.644923 train-rmse:8.644923 valid-loss:8.551617 valid-rmse:8.551617\n",
            "[INFO 23-11-05 01:01:12.2535 UTC gradient_boosted_trees.cc:1556] \tnum-trees:2 train-loss:8.635780 train-rmse:8.635780 valid-loss:8.548929 valid-rmse:8.548929\n",
            "[INFO 23-11-05 01:01:45.1747 UTC gradient_boosted_trees.cc:1556] \tnum-trees:114 train-loss:6.942920 train-rmse:6.942920 valid-loss:7.990068 valid-rmse:7.990068\n",
            "[INFO 23-11-05 01:02:15.4263 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:5.643881 train-rmse:5.643881 valid-loss:7.775877 valid-rmse:7.775877\n",
            "[INFO 23-11-05 01:02:46.0179 UTC gradient_boosted_trees.cc:1556] \tnum-trees:12 train-loss:8.557239 train-rmse:8.557239 valid-loss:8.516600 valid-rmse:8.516600\n",
            "[INFO 23-11-05 01:03:16.1635 UTC gradient_boosted_trees.cc:1556] \tnum-trees:271 train-loss:5.623997 train-rmse:5.623997 valid-loss:7.768757 valid-rmse:7.768757\n",
            "[INFO 23-11-05 01:03:46.3032 UTC gradient_boosted_trees.cc:1556] \tnum-trees:172 train-loss:7.266151 train-rmse:7.266151 valid-loss:8.087418 valid-rmse:8.087418\n",
            "[INFO 23-11-05 01:04:17.3076 UTC gradient_boosted_trees.cc:1556] \tnum-trees:276 train-loss:7.036476 train-rmse:7.036476 valid-loss:8.039914 valid-rmse:8.039914\n",
            "[INFO 23-11-05 01:04:47.4972 UTC gradient_boosted_trees.cc:1556] \tnum-trees:25 train-loss:8.481062 train-rmse:8.481062 valid-loss:8.489776 valid-rmse:8.489776\n",
            "[INFO 23-11-05 01:05:20.1302 UTC gradient_boosted_trees.cc:1556] \tnum-trees:126 train-loss:6.837963 train-rmse:6.837963 valid-loss:7.995668 valid-rmse:7.995668\n",
            "[INFO 23-11-05 01:05:51.7507 UTC gradient_boosted_trees.cc:1556] \tnum-trees:118 train-loss:7.555725 train-rmse:7.555725 valid-loss:8.148273 valid-rmse:8.148273\n",
            "[INFO 23-11-05 01:06:27.7914 UTC gradient_boosted_trees.cc:1556] \tnum-trees:216 train-loss:7.692813 train-rmse:7.692813 valid-loss:8.183223 valid-rmse:8.183223\n",
            "[INFO 23-11-05 01:06:58.5138 UTC gradient_boosted_trees.cc:1556] \tnum-trees:39 train-loss:8.417689 train-rmse:8.417689 valid-loss:8.465809 valid-rmse:8.465809\n",
            "[INFO 23-11-05 01:07:29.9470 UTC gradient_boosted_trees.cc:1556] \tnum-trees:218 train-loss:7.689068 train-rmse:7.689068 valid-loss:8.182425 valid-rmse:8.182425\n",
            "[INFO 23-11-05 01:07:39.2256 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.386911 train-rmse:6.386911 valid-loss:7.905772 valid-rmse:7.905772\n",
            "[INFO 23-11-05 01:07:39.2256 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 01:07:39.2256 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.905772 valid-rmse:7.905772\n",
            "[INFO 23-11-05 01:07:39.2290 UTC hyperparameters_optimizer.cc:582] [46/100] Score: -7.90577 / -7.71015 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 16 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 01:07:39.2333 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 01:07:39.2333 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 01:07:39.2444 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 01:07:39.8820 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.578246 train-rmse:8.578246 valid-loss:8.531684 valid-rmse:8.531684\n",
            "[INFO 23-11-05 01:08:00.3639 UTC gradient_boosted_trees.cc:1556] \tnum-trees:43 train-loss:7.540583 train-rmse:7.540583 valid-loss:8.188385 valid-rmse:8.188385\n",
            "[INFO 23-11-05 01:08:30.6136 UTC gradient_boosted_trees.cc:1556] \tnum-trees:104 train-loss:6.917706 train-rmse:6.917706 valid-loss:7.978392 valid-rmse:7.978392\n",
            "[INFO 23-11-05 01:09:00.6877 UTC gradient_boosted_trees.cc:1556] \tnum-trees:52 train-loss:8.371124 train-rmse:8.371124 valid-loss:8.437286 valid-rmse:8.437286\n",
            "[INFO 23-11-05 01:09:30.9816 UTC gradient_boosted_trees.cc:1556] \tnum-trees:226 train-loss:5.998416 train-rmse:5.998416 valid-loss:7.770867 valid-rmse:7.770867\n",
            "[INFO 23-11-05 01:10:01.4778 UTC gradient_boosted_trees.cc:1556] \tnum-trees:288 train-loss:5.629900 train-rmse:5.629900 valid-loss:7.715495 valid-rmse:7.715495\n",
            "[INFO 23-11-05 01:10:07.5051 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.575892 train-rmse:5.575892 valid-loss:7.700172 valid-rmse:7.700172\n",
            "[INFO 23-11-05 01:10:07.5156 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 01:10:07.5162 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.700172 valid-rmse:7.700172\n",
            "[INFO 23-11-05 01:10:07.5254 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 01:10:07.5254 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 01:10:07.5296 UTC hyperparameters_optimizer.cc:582] [47/100] Score: -7.70017 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"AXIS_ALIGNED\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 01:10:07.5406 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 01:10:24.2408 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.635663 train-rmse:8.635663 valid-loss:8.549307 valid-rmse:8.549307\n",
            "[INFO 23-11-05 01:10:32.4163 UTC gradient_boosted_trees.cc:1556] \tnum-trees:131 train-loss:7.497027 train-rmse:7.497027 valid-loss:8.131940 valid-rmse:8.131940\n",
            "[INFO 23-11-05 01:11:02.7501 UTC gradient_boosted_trees.cc:1556] \tnum-trees:65 train-loss:8.334686 train-rmse:8.334686 valid-loss:8.418324 valid-rmse:8.418324\n",
            "[INFO 23-11-05 01:11:34.5228 UTC gradient_boosted_trees.cc:1556] \tnum-trees:147 train-loss:6.659724 train-rmse:6.659724 valid-loss:7.955310 valid-rmse:7.955310\n",
            "[INFO 23-11-05 01:12:06.6541 UTC gradient_boosted_trees.cc:1556] \tnum-trees:44 train-loss:7.665664 train-rmse:7.665664 valid-loss:8.203504 valid-rmse:8.203504\n",
            "[INFO 23-11-05 01:12:37.0783 UTC gradient_boosted_trees.cc:1556] \tnum-trees:9 train-loss:8.515858 train-rmse:8.515858 valid-loss:8.493813 valid-rmse:8.493813\n",
            "[INFO 23-11-05 01:13:07.7815 UTC gradient_boosted_trees.cc:1556] \tnum-trees:171 train-loss:7.245381 train-rmse:7.245381 valid-loss:8.069636 valid-rmse:8.069636\n",
            "[INFO 23-11-05 01:13:28.5218 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.967456 train-rmse:6.967456 valid-loss:8.020923 valid-rmse:8.020923\n",
            "[INFO 23-11-05 01:13:28.5218 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 01:13:28.5218 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.020923 valid-rmse:8.020923\n",
            "[INFO 23-11-05 01:13:28.5255 UTC hyperparameters_optimizer.cc:582] [48/100] Score: -8.02092 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 16 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 01:13:28.5265 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 01:13:28.5265 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 01:13:28.5391 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 01:13:37.0420 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.639297 train-rmse:8.639297 valid-loss:8.551619 valid-rmse:8.551619\n",
            "[INFO 23-11-05 01:13:38.1669 UTC gradient_boosted_trees.cc:1556] \tnum-trees:231 train-loss:7.660620 train-rmse:7.660620 valid-loss:8.170361 valid-rmse:8.170361\n",
            "[INFO 23-11-05 01:14:01.6944 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.466480 train-rmse:5.466480 valid-loss:7.746842 valid-rmse:7.746842\n",
            "[INFO 23-11-05 01:14:01.6944 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 01:14:01.6944 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.746842 valid-rmse:7.746842\n",
            "[INFO 23-11-05 01:14:01.7008 UTC hyperparameters_optimizer.cc:582] [49/100] Score: -7.74684 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 01:14:01.7144 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 01:14:01.7145 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 01:14:01.7294 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 01:14:09.5015 UTC gradient_boosted_trees.cc:1556] \tnum-trees:232 train-loss:7.658823 train-rmse:7.658823 valid-loss:8.168903 valid-rmse:8.168903\n",
            "[INFO 23-11-05 01:14:26.1399 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.612895 train-rmse:8.612895 valid-loss:8.532352 valid-rmse:8.532352\n",
            "[INFO 23-11-05 01:14:39.5700 UTC gradient_boosted_trees.cc:1556] \tnum-trees:146 train-loss:7.374878 train-rmse:7.374878 valid-loss:8.014691 valid-rmse:8.014691\n",
            "[INFO 23-11-05 01:15:10.0712 UTC gradient_boosted_trees.cc:1556] \tnum-trees:12 train-loss:8.509414 train-rmse:8.509414 valid-loss:8.496504 valid-rmse:8.496504\n",
            "[INFO 23-11-05 01:15:42.0720 UTC gradient_boosted_trees.cc:1556] \tnum-trees:20 train-loss:8.374857 train-rmse:8.374857 valid-loss:8.445712 valid-rmse:8.445712\n",
            "[INFO 23-11-05 01:16:14.6794 UTC gradient_boosted_trees.cc:1556] \tnum-trees:98 train-loss:8.257784 train-rmse:8.257784 valid-loss:8.381761 valid-rmse:8.381761\n",
            "[INFO 23-11-05 01:16:49.1995 UTC gradient_boosted_trees.cc:1556] \tnum-trees:24 train-loss:8.330154 train-rmse:8.330154 valid-loss:8.435086 valid-rmse:8.435086\n",
            "[INFO 23-11-05 01:17:19.8974 UTC gradient_boosted_trees.cc:1556] \tnum-trees:150 train-loss:7.417804 train-rmse:7.417804 valid-loss:8.098587 valid-rmse:8.098587\n",
            "[INFO 23-11-05 01:17:54.2307 UTC gradient_boosted_trees.cc:1556] \tnum-trees:181 train-loss:7.208603 train-rmse:7.208603 valid-loss:8.059642 valid-rmse:8.059642\n",
            "[INFO 23-11-05 01:18:24.5147 UTC gradient_boosted_trees.cc:1556] \tnum-trees:170 train-loss:6.494044 train-rmse:6.494044 valid-loss:7.932140 valid-rmse:7.932140\n",
            "[INFO 23-11-05 01:18:54.6617 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:8.229589 train-rmse:8.229589 valid-loss:8.371491 valid-rmse:8.371491\n",
            "[INFO 23-11-05 01:19:25.7526 UTC gradient_boosted_trees.cc:1556] \tnum-trees:156 train-loss:7.390079 train-rmse:7.390079 valid-loss:8.087865 valid-rmse:8.087865\n",
            "[INFO 23-11-05 01:19:59.0576 UTC gradient_boosted_trees.cc:1556] \tnum-trees:185 train-loss:7.193511 train-rmse:7.193511 valid-loss:8.057317 valid-rmse:8.057317\n",
            "[INFO 23-11-05 01:20:29.4222 UTC gradient_boosted_trees.cc:1556] \tnum-trees:159 train-loss:7.381257 train-rmse:7.381257 valid-loss:8.083619 valid-rmse:8.083619\n",
            "[INFO 23-11-05 01:20:59.6567 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:7.627214 train-rmse:7.627214 valid-loss:8.155391 valid-rmse:8.155391\n",
            "[INFO 23-11-05 01:21:30.7338 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:7.625006 train-rmse:7.625006 valid-loss:8.154314 valid-rmse:8.154314\n",
            "[INFO 23-11-05 01:22:02.0605 UTC gradient_boosted_trees.cc:1556] \tnum-trees:248 train-loss:7.622780 train-rmse:7.622780 valid-loss:8.153260 valid-rmse:8.153260\n",
            "[INFO 23-11-05 01:22:32.2883 UTC gradient_boosted_trees.cc:1556] \tnum-trees:65 train-loss:8.284910 train-rmse:8.284910 valid-loss:8.395551 valid-rmse:8.395551\n",
            "[INFO 23-11-05 01:23:02.8129 UTC gradient_boosted_trees.cc:1556] \tnum-trees:250 train-loss:7.618992 train-rmse:7.618992 valid-loss:8.151019 valid-rmse:8.151019\n",
            "[INFO 23-11-05 01:23:33.7924 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:7.617844 train-rmse:7.617844 valid-loss:8.150532 valid-rmse:8.150532\n",
            "[INFO 23-11-05 01:24:04.1392 UTC gradient_boosted_trees.cc:1556] \tnum-trees:76 train-loss:8.259861 train-rmse:8.259861 valid-loss:8.376279 valid-rmse:8.376279\n",
            "[INFO 23-11-05 01:24:04.5145 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.470682 train-rmse:5.470682 valid-loss:7.716882 valid-rmse:7.716882\n",
            "[INFO 23-11-05 01:24:04.5145 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 01:24:04.5145 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.716882 valid-rmse:7.716882\n",
            "[INFO 23-11-05 01:24:04.5229 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 01:24:04.5229 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 01:24:04.5232 UTC hyperparameters_optimizer.cc:582] [50/100] Score: -7.71688 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 01:24:04.5377 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 01:24:13.2747 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.645914 train-rmse:8.645914 valid-loss:8.555444 valid-rmse:8.555444\n",
            "[INFO 23-11-05 01:24:35.0387 UTC gradient_boosted_trees.cc:1556] \tnum-trees:253 train-loss:7.613479 train-rmse:7.613479 valid-loss:8.149495 valid-rmse:8.149495\n",
            "[INFO 23-11-05 01:25:05.5836 UTC gradient_boosted_trees.cc:1556] \tnum-trees:7 train-loss:8.610551 train-rmse:8.610551 valid-loss:8.543680 valid-rmse:8.543680\n",
            "[INFO 23-11-05 01:25:35.7428 UTC gradient_boosted_trees.cc:1556] \tnum-trees:87 train-loss:8.238639 train-rmse:8.238639 valid-loss:8.364584 valid-rmse:8.364584\n",
            "[INFO 23-11-05 01:26:06.6632 UTC gradient_boosted_trees.cc:1556] \tnum-trees:14 train-loss:8.571942 train-rmse:8.571942 valid-loss:8.530422 valid-rmse:8.530422\n",
            "[INFO 23-11-05 01:26:38.6183 UTC gradient_boosted_trees.cc:1556] \tnum-trees:59 train-loss:8.063734 train-rmse:8.063734 valid-loss:8.334062 valid-rmse:8.334062\n",
            "[INFO 23-11-05 01:27:10.6470 UTC gradient_boosted_trees.cc:1556] \tnum-trees:258 train-loss:7.603883 train-rmse:7.603883 valid-loss:8.146663 valid-rmse:8.146663\n",
            "[INFO 23-11-05 01:27:40.6609 UTC gradient_boosted_trees.cc:1556] \tnum-trees:102 train-loss:8.209615 train-rmse:8.209615 valid-loss:8.353853 valid-rmse:8.353853\n",
            "[INFO 23-11-05 01:28:11.0156 UTC gradient_boosted_trees.cc:1556] \tnum-trees:35 train-loss:7.948934 train-rmse:7.948934 valid-loss:8.307049 valid-rmse:8.307049\n",
            "[INFO 23-11-05 01:28:42.3777 UTC gradient_boosted_trees.cc:1556] \tnum-trees:182 train-loss:7.286379 train-rmse:7.286379 valid-loss:8.048712 valid-rmse:8.048712\n",
            "[INFO 23-11-05 01:29:16.7279 UTC gradient_boosted_trees.cc:1556] \tnum-trees:181 train-loss:8.132916 train-rmse:8.132916 valid-loss:8.317636 valid-rmse:8.317636\n",
            "[INFO 23-11-05 01:29:46.7329 UTC gradient_boosted_trees.cc:1556] \tnum-trees:185 train-loss:7.269159 train-rmse:7.269159 valid-loss:8.040289 valid-rmse:8.040289\n",
            "[INFO 23-11-05 01:30:17.2714 UTC gradient_boosted_trees.cc:1556] \tnum-trees:210 train-loss:6.232015 train-rmse:6.232015 valid-loss:7.884382 valid-rmse:7.884382\n",
            "[INFO 23-11-05 01:30:50.6044 UTC gradient_boosted_trees.cc:1556] \tnum-trees:191 train-loss:8.119243 train-rmse:8.119243 valid-loss:8.311090 valid-rmse:8.311090\n",
            "[INFO 23-11-05 01:31:24.0861 UTC gradient_boosted_trees.cc:1556] \tnum-trees:216 train-loss:7.105171 train-rmse:7.105171 valid-loss:8.039725 valid-rmse:8.039725\n",
            "[INFO 23-11-05 01:31:56.2940 UTC gradient_boosted_trees.cc:1556] \tnum-trees:191 train-loss:7.250434 train-rmse:7.250434 valid-loss:8.035172 valid-rmse:8.035172\n",
            "[INFO 23-11-05 01:32:27.6806 UTC gradient_boosted_trees.cc:1556] \tnum-trees:58 train-loss:8.432888 train-rmse:8.432888 valid-loss:8.473657 valid-rmse:8.473657\n",
            "[INFO 23-11-05 01:32:57.7839 UTC gradient_boosted_trees.cc:1556] \tnum-trees:140 train-loss:8.152952 train-rmse:8.152952 valid-loss:8.317859 valid-rmse:8.317859\n",
            "[INFO 23-11-05 01:33:28.1500 UTC gradient_boosted_trees.cc:1556] \tnum-trees:190 train-loss:7.188675 train-rmse:7.188675 valid-loss:7.960059 valid-rmse:7.960059\n",
            "[INFO 23-11-05 01:33:58.4482 UTC gradient_boosted_trees.cc:1556] \tnum-trees:211 train-loss:8.094400 train-rmse:8.094400 valid-loss:8.301911 valid-rmse:8.301911\n",
            "[INFO 23-11-05 01:34:29.0546 UTC gradient_boosted_trees.cc:1556] \tnum-trees:86 train-loss:7.214781 train-rmse:7.214781 valid-loss:8.061917 valid-rmse:8.061917\n",
            "[INFO 23-11-05 01:35:00.0714 UTC gradient_boosted_trees.cc:1556] \tnum-trees:226 train-loss:6.117436 train-rmse:6.117436 valid-loss:7.869488 valid-rmse:7.869488\n",
            "[INFO 23-11-05 01:35:31.0954 UTC gradient_boosted_trees.cc:1556] \tnum-trees:53 train-loss:7.741550 train-rmse:7.741550 valid-loss:8.242353 valid-rmse:8.242353\n",
            "[INFO 23-11-05 01:36:01.1717 UTC gradient_boosted_trees.cc:1556] \tnum-trees:162 train-loss:8.125515 train-rmse:8.125515 valid-loss:8.305548 valid-rmse:8.305548\n",
            "[INFO 23-11-05 01:36:32.9981 UTC gradient_boosted_trees.cc:1556] \tnum-trees:197 train-loss:7.159923 train-rmse:7.159923 valid-loss:7.946984 valid-rmse:7.946984\n",
            "[INFO 23-11-05 01:37:05.1828 UTC gradient_boosted_trees.cc:1556] \tnum-trees:233 train-loss:6.077483 train-rmse:6.077483 valid-loss:7.860458 valid-rmse:7.860458\n",
            "[INFO 23-11-05 01:37:38.7730 UTC gradient_boosted_trees.cc:1556] \tnum-trees:226 train-loss:7.069954 train-rmse:7.069954 valid-loss:8.032447 valid-rmse:8.032447\n",
            "[INFO 23-11-05 01:38:09.2221 UTC gradient_boosted_trees.cc:1556] \tnum-trees:98 train-loss:8.369227 train-rmse:8.369227 valid-loss:8.437244 valid-rmse:8.437244\n",
            "[INFO 23-11-05 01:38:39.4562 UTC gradient_boosted_trees.cc:1556] \tnum-trees:181 train-loss:8.101350 train-rmse:8.101350 valid-loss:8.290153 valid-rmse:8.290153\n",
            "[INFO 23-11-05 01:39:10.3876 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:6.039923 train-rmse:6.039923 valid-loss:7.857114 valid-rmse:7.857114\n",
            "[INFO 23-11-05 01:39:41.8395 UTC gradient_boosted_trees.cc:1556] \tnum-trees:105 train-loss:7.841573 train-rmse:7.841573 valid-loss:8.251803 valid-rmse:8.251803\n",
            "[INFO 23-11-05 01:40:14.2505 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:8.047327 train-rmse:8.047327 valid-loss:8.282885 valid-rmse:8.282885\n",
            "[INFO 23-11-05 01:40:44.9722 UTC gradient_boosted_trees.cc:1556] \tnum-trees:196 train-loss:8.083431 train-rmse:8.083431 valid-loss:8.276636 valid-rmse:8.276636\n",
            "[INFO 23-11-05 01:41:16.4920 UTC gradient_boosted_trees.cc:1556] \tnum-trees:67 train-loss:7.621930 train-rmse:7.621930 valid-loss:8.210643 valid-rmse:8.210643\n",
            "[INFO 23-11-05 01:41:48.4479 UTC gradient_boosted_trees.cc:1556] \tnum-trees:261 train-loss:8.035402 train-rmse:8.035402 valid-loss:8.284008 valid-rmse:8.284008\n",
            "[INFO 23-11-05 01:42:24.6924 UTC gradient_boosted_trees.cc:1556] \tnum-trees:128 train-loss:8.336473 train-rmse:8.336473 valid-loss:8.416729 valid-rmse:8.416729\n",
            "[INFO 23-11-05 01:42:55.9999 UTC gradient_boosted_trees.cc:1556] \tnum-trees:71 train-loss:7.591789 train-rmse:7.591789 valid-loss:8.200781 valid-rmse:8.200781\n",
            "[INFO 23-11-05 01:43:31.4939 UTC gradient_boosted_trees.cc:1556] \tnum-trees:103 train-loss:7.061571 train-rmse:7.061571 valid-loss:8.016388 valid-rmse:8.016388\n",
            "[INFO 23-11-05 01:44:03.8606 UTC gradient_boosted_trees.cc:1556] \tnum-trees:104 train-loss:7.055547 train-rmse:7.055547 valid-loss:8.015876 valid-rmse:8.015876\n",
            "[INFO 23-11-05 01:44:35.7007 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:7.558896 train-rmse:7.558896 valid-loss:8.196519 valid-rmse:8.196519\n",
            "[INFO 23-11-05 01:45:06.4415 UTC gradient_boosted_trees.cc:1556] \tnum-trees:260 train-loss:5.905879 train-rmse:5.905879 valid-loss:7.827652 valid-rmse:7.827652\n",
            "[INFO 23-11-05 01:45:36.8957 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:6.999929 train-rmse:6.999929 valid-loss:8.012208 valid-rmse:8.012208\n",
            "[INFO 23-11-05 01:46:09.8268 UTC gradient_boosted_trees.cc:1556] \tnum-trees:241 train-loss:6.997065 train-rmse:6.997065 valid-loss:8.008688 valid-rmse:8.008688\n",
            "[INFO 23-11-05 01:46:40.2656 UTC gradient_boosted_trees.cc:1556] \tnum-trees:292 train-loss:7.999040 train-rmse:7.999040 valid-loss:8.273577 valid-rmse:8.273577\n",
            "[INFO 23-11-05 01:47:06.7859 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.521971 train-rmse:7.521971 valid-loss:8.121060 valid-rmse:8.121060\n",
            "[INFO 23-11-05 01:47:06.7860 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 01:47:06.7860 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:8.121039 valid-rmse:8.121039\n",
            "[INFO 23-11-05 01:47:06.7886 UTC hyperparameters_optimizer.cc:582] [51/100] Score: -8.12104 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 16 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 01:47:06.7942 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 01:47:06.7943 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 01:47:06.8087 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 01:47:15.0381 UTC gradient_boosted_trees.cc:1556] \tnum-trees:162 train-loss:8.306486 train-rmse:8.306486 valid-loss:8.401001 valid-rmse:8.401001\n",
            "[INFO 23-11-05 01:47:37.6884 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.605479 train-rmse:8.605479 valid-loss:8.534800 valid-rmse:8.534800\n",
            "[INFO 23-11-05 01:47:45.4591 UTC gradient_boosted_trees.cc:1556] \tnum-trees:269 train-loss:5.850471 train-rmse:5.850471 valid-loss:7.827708 valid-rmse:7.827708\n",
            "[INFO 23-11-05 01:47:55.6825 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.990736 train-rmse:7.990736 valid-loss:8.270835 valid-rmse:8.270835\n",
            "[INFO 23-11-05 01:47:55.6825 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 01:47:55.6825 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:8.269998 valid-rmse:8.269998\n",
            "[INFO 23-11-05 01:47:55.6837 UTC hyperparameters_optimizer.cc:582] [52/100] Score: -8.27 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 01:47:55.6858 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 01:47:55.6858 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 01:47:55.6971 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 01:48:15.8284 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:7.014832 train-rmse:7.014832 valid-loss:8.019285 valid-rmse:8.019285\n",
            "[INFO 23-11-05 01:48:21.0718 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.614544 train-rmse:8.614544 valid-loss:8.531450 valid-rmse:8.531450\n",
            "[INFO 23-11-05 01:48:46.5990 UTC gradient_boosted_trees.cc:1556] \tnum-trees:2 train-loss:8.576087 train-rmse:8.576087 valid-loss:8.515791 valid-rmse:8.515791\n",
            "[INFO 23-11-05 01:49:22.3571 UTC gradient_boosted_trees.cc:1556] \tnum-trees:139 train-loss:7.691713 train-rmse:7.691713 valid-loss:8.192694 valid-rmse:8.192694\n",
            "[INFO 23-11-05 01:49:54.7391 UTC gradient_boosted_trees.cc:1556] \tnum-trees:88 train-loss:7.454509 train-rmse:7.454509 valid-loss:8.161001 valid-rmse:8.161001\n",
            "[INFO 23-11-05 01:50:24.7851 UTC gradient_boosted_trees.cc:1556] \tnum-trees:184 train-loss:8.289135 train-rmse:8.289135 valid-loss:8.387789 valid-rmse:8.387789\n",
            "[INFO 23-11-05 01:50:57.0718 UTC gradient_boosted_trees.cc:1556] \tnum-trees:269 train-loss:7.995852 train-rmse:7.995852 valid-loss:8.242320 valid-rmse:8.242320\n",
            "[INFO 23-11-05 01:51:30.4270 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:7.056465 train-rmse:7.056465 valid-loss:7.967539 valid-rmse:7.967539\n",
            "[INFO 23-11-05 01:52:04.9042 UTC gradient_boosted_trees.cc:1556] \tnum-trees:277 train-loss:7.988416 train-rmse:7.988416 valid-loss:8.239903 valid-rmse:8.239903\n",
            "[INFO 23-11-05 01:52:35.1476 UTC gradient_boosted_trees.cc:1556] \tnum-trees:250 train-loss:6.992311 train-rmse:6.992311 valid-loss:8.011180 valid-rmse:8.011180\n",
            "[INFO 23-11-05 01:53:06.0310 UTC gradient_boosted_trees.cc:1556] \tnum-trees:152 train-loss:7.652151 train-rmse:7.652151 valid-loss:8.180499 valid-rmse:8.180499\n",
            "[INFO 23-11-05 01:53:37.9822 UTC gradient_boosted_trees.cc:1556] \tnum-trees:288 train-loss:7.975024 train-rmse:7.975024 valid-loss:8.238076 valid-rmse:8.238076\n",
            "[INFO 23-11-05 01:54:08.3685 UTC gradient_boosted_trees.cc:1556] \tnum-trees:291 train-loss:5.693738 train-rmse:5.693738 valid-loss:7.792035 valid-rmse:7.792035\n",
            "[INFO 23-11-05 01:54:41.8160 UTC gradient_boosted_trees.cc:1556] \tnum-trees:255 train-loss:7.025489 train-rmse:7.025489 valid-loss:7.962169 valid-rmse:7.962169\n",
            "[INFO 23-11-05 01:55:17.9795 UTC gradient_boosted_trees.cc:1556] \tnum-trees:101 train-loss:7.375804 train-rmse:7.375804 valid-loss:8.123573 valid-rmse:8.123573\n",
            "[INFO 23-11-05 01:55:19.4486 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.960898 train-rmse:7.960898 valid-loss:8.228605 valid-rmse:8.228605\n",
            "[INFO 23-11-05 01:55:19.4486 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 01:55:19.4486 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.228605 valid-rmse:8.228605\n",
            "[INFO 23-11-05 01:55:19.4493 UTC hyperparameters_optimizer.cc:582] [53/100] Score: -8.22861 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 3 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 01:55:19.4504 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 01:55:19.4504 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 01:55:19.4625 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 01:55:47.2784 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.616047 train-rmse:8.616047 valid-loss:8.540732 valid-rmse:8.540732\n",
            "[INFO 23-11-05 01:55:49.4617 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:7.014551 train-rmse:7.014551 valid-loss:7.916250 valid-rmse:7.916250\n",
            "[INFO 23-11-05 01:56:20.2244 UTC gradient_boosted_trees.cc:1556] \tnum-trees:225 train-loss:8.262426 train-rmse:8.262426 valid-loss:8.376053 valid-rmse:8.376053\n",
            "[INFO 23-11-05 01:56:46.3666 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.646067 train-rmse:5.646067 valid-loss:7.785241 valid-rmse:7.785241\n",
            "[INFO 23-11-05 01:56:46.3666 UTC gradient_boosted_trees.cc:249] Truncates the model to 294 tree(s) i.e. 294  iteration(s).\n",
            "[INFO 23-11-05 01:56:46.3668 UTC gradient_boosted_trees.cc:312] Final model num-trees:294 valid-loss:7.784823 valid-rmse:7.784823\n",
            "[INFO 23-11-05 01:56:46.3756 UTC hyperparameters_optimizer.cc:582] [54/100] Score: -7.78482 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 256 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 01:56:46.3872 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 01:56:46.3873 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 01:56:46.4015 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 01:56:50.3403 UTC gradient_boosted_trees.cc:1556] \tnum-trees:165 train-loss:7.595672 train-rmse:7.595672 valid-loss:8.157239 valid-rmse:8.157239\n",
            "[INFO 23-11-05 01:57:14.9194 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.608805 train-rmse:8.608805 valid-loss:8.535394 valid-rmse:8.535394\n",
            "[INFO 23-11-05 01:57:20.4171 UTC gradient_boosted_trees.cc:1556] \tnum-trees:232 train-loss:8.257873 train-rmse:8.257873 valid-loss:8.374143 valid-rmse:8.374143\n",
            "[INFO 23-11-05 01:57:51.8298 UTC gradient_boosted_trees.cc:1556] \tnum-trees:269 train-loss:6.909707 train-rmse:6.909707 valid-loss:7.988571 valid-rmse:7.988571\n",
            "[INFO 23-11-05 01:58:29.1807 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:8.252139 train-rmse:8.252139 valid-loss:8.369681 valid-rmse:8.369681\n",
            "[INFO 23-11-05 01:58:59.5298 UTC gradient_boosted_trees.cc:1556] \tnum-trees:267 train-loss:6.992388 train-rmse:6.992388 valid-loss:7.958360 valid-rmse:7.958360\n",
            "[INFO 23-11-05 01:59:29.8720 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:8.247295 train-rmse:8.247295 valid-loss:8.365095 valid-rmse:8.365095\n",
            "[INFO 23-11-05 02:00:00.0873 UTC gradient_boosted_trees.cc:1556] \tnum-trees:10 train-loss:8.378700 train-rmse:8.378700 valid-loss:8.408826 valid-rmse:8.408826\n",
            "[INFO 23-11-05 02:00:30.6961 UTC gradient_boosted_trees.cc:1556] \tnum-trees:254 train-loss:8.243151 train-rmse:8.243151 valid-loss:8.361147 valid-rmse:8.361147\n",
            "[INFO 23-11-05 02:01:03.3054 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:7.298117 train-rmse:7.298117 valid-loss:8.102860 valid-rmse:8.102860\n",
            "[INFO 23-11-05 02:01:34.3004 UTC gradient_boosted_trees.cc:1556] \tnum-trees:257 train-loss:6.970492 train-rmse:6.970492 valid-loss:7.909592 valid-rmse:7.909592\n",
            "[INFO 23-11-05 02:02:06.0723 UTC gradient_boosted_trees.cc:1556] \tnum-trees:265 train-loss:8.236086 train-rmse:8.236086 valid-loss:8.358318 valid-rmse:8.358318\n",
            "[INFO 23-11-05 02:02:40.6533 UTC gradient_boosted_trees.cc:1556] \tnum-trees:269 train-loss:8.234039 train-rmse:8.234039 valid-loss:8.356593 valid-rmse:8.356593\n",
            "[INFO 23-11-05 02:03:11.7754 UTC gradient_boosted_trees.cc:1556] \tnum-trees:267 train-loss:6.947417 train-rmse:6.947417 valid-loss:7.998764 valid-rmse:7.998764\n",
            "[INFO 23-11-05 02:03:42.7970 UTC gradient_boosted_trees.cc:1556] \tnum-trees:189 train-loss:7.511608 train-rmse:7.511608 valid-loss:8.131375 valid-rmse:8.131375\n",
            "[INFO 23-11-05 02:04:15.0209 UTC gradient_boosted_trees.cc:1556] \tnum-trees:280 train-loss:8.228408 train-rmse:8.228408 valid-loss:8.351577 valid-rmse:8.351577\n",
            "[INFO 23-11-05 02:04:45.8677 UTC gradient_boosted_trees.cc:1556] \tnum-trees:124 train-loss:7.247998 train-rmse:7.247998 valid-loss:8.093150 valid-rmse:8.093150\n",
            "[INFO 23-11-05 02:05:20.3273 UTC gradient_boosted_trees.cc:1556] \tnum-trees:286 train-loss:6.860286 train-rmse:6.860286 valid-loss:7.979061 valid-rmse:7.979061\n",
            "[INFO 23-11-05 02:05:52.0199 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:6.937133 train-rmse:6.937133 valid-loss:7.904669 valid-rmse:7.904669\n",
            "[INFO 23-11-05 02:06:23.7009 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:8.219456 train-rmse:8.219456 valid-loss:8.348060 valid-rmse:8.348060\n",
            "[INFO 23-11-05 02:06:55.9855 UTC gradient_boosted_trees.cc:1556] \tnum-trees:273 train-loss:6.925907 train-rmse:6.925907 valid-loss:7.993141 valid-rmse:7.993141\n",
            "[INFO 23-11-05 02:07:06.6652 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:8.216479 train-rmse:8.216479 valid-loss:8.345526 valid-rmse:8.345526\n",
            "[INFO 23-11-05 02:07:06.6652 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 02:07:06.6652 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.345526 valid-rmse:8.345526\n",
            "[INFO 23-11-05 02:07:06.6660 UTC hyperparameters_optimizer.cc:582] [55/100] Score: -8.34553 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 3 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 02:07:06.6670 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 02:07:06.6670 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 02:07:06.6792 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 02:07:07.4098 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.608018 train-rmse:8.608018 valid-loss:8.539744 valid-rmse:8.539744\n",
            "[INFO 23-11-05 02:07:26.2406 UTC gradient_boosted_trees.cc:1556] \tnum-trees:34 train-loss:7.956509 train-rmse:7.956509 valid-loss:8.346320 valid-rmse:8.346320\n",
            "[INFO 23-11-05 02:07:56.4938 UTC gradient_boosted_trees.cc:1556] \tnum-trees:86 train-loss:7.480533 train-rmse:7.480533 valid-loss:8.165758 valid-rmse:8.165758\n",
            "[INFO 23-11-05 02:08:26.9855 UTC gradient_boosted_trees.cc:1556] \tnum-trees:138 train-loss:7.184296 train-rmse:7.184296 valid-loss:8.070543 valid-rmse:8.070543\n",
            "[INFO 23-11-05 02:08:57.1374 UTC gradient_boosted_trees.cc:1556] \tnum-trees:189 train-loss:6.964125 train-rmse:6.964125 valid-loss:7.980608 valid-rmse:7.980608\n",
            "[INFO 23-11-05 02:09:27.2814 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:6.740495 train-rmse:6.740495 valid-loss:7.901800 valid-rmse:7.901800\n",
            "[INFO 23-11-05 02:09:57.6104 UTC gradient_boosted_trees.cc:1556] \tnum-trees:291 train-loss:6.569242 train-rmse:6.569242 valid-loss:7.868658 valid-rmse:7.868658\n",
            "[INFO 23-11-05 02:10:02.9445 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.533965 train-rmse:6.533965 valid-loss:7.860631 valid-rmse:7.860631\n",
            "[INFO 23-11-05 02:10:02.9551 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 02:10:02.9556 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.860631 valid-rmse:7.860631\n",
            "[INFO 23-11-05 02:10:02.9664 UTC hyperparameters_optimizer.cc:582] [56/100] Score: -7.86063 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"AXIS_ALIGNED\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 02:10:02.9734 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 02:10:02.9828 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 02:10:02.9995 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 02:10:24.6645 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.590029 train-rmse:8.590029 valid-loss:8.525088 valid-rmse:8.525088\n",
            "[INFO 23-11-05 02:10:28.0264 UTC gradient_boosted_trees.cc:1556] \tnum-trees:153 train-loss:6.726897 train-rmse:6.726897 valid-loss:7.928390 valid-rmse:7.928390\n",
            "[INFO 23-11-05 02:10:44.1818 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.877383 train-rmse:6.877383 valid-loss:7.927668 valid-rmse:7.927668\n",
            "[INFO 23-11-05 02:10:44.1818 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 02:10:44.1818 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.927566 valid-rmse:7.927566\n",
            "[INFO 23-11-05 02:10:44.1864 UTC hyperparameters_optimizer.cc:582] [57/100] Score: -7.92757 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 6 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 02:10:44.1877 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 02:10:44.1878 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 02:10:44.2004 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 02:11:00.5514 UTC gradient_boosted_trees.cc:1556] \tnum-trees:154 train-loss:6.721771 train-rmse:6.721771 valid-loss:7.926594 valid-rmse:7.926594\n",
            "[INFO 23-11-05 02:11:12.1261 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.637218 train-rmse:8.637218 valid-loss:8.546173 valid-rmse:8.546173\n",
            "[INFO 23-11-05 02:11:24.4377 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.818217 train-rmse:6.818217 valid-loss:7.973647 valid-rmse:7.973647\n",
            "[INFO 23-11-05 02:11:24.4377 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 02:11:24.4377 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.973647 valid-rmse:7.973647\n",
            "[INFO 23-11-05 02:11:24.4412 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 02:11:24.4413 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 02:11:24.4419 UTC hyperparameters_optimizer.cc:582] [58/100] Score: -7.97365 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 16 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 02:11:24.4545 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 02:11:32.3966 UTC gradient_boosted_trees.cc:1556] \tnum-trees:155 train-loss:6.714793 train-rmse:6.714793 valid-loss:7.926470 valid-rmse:7.926470\n",
            "[INFO 23-11-05 02:11:49.9370 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.635787 train-rmse:8.635787 valid-loss:8.550958 valid-rmse:8.550958\n",
            "[INFO 23-11-05 02:12:04.9686 UTC gradient_boosted_trees.cc:1556] \tnum-trees:156 train-loss:6.708558 train-rmse:6.708558 valid-loss:7.927062 valid-rmse:7.927062\n",
            "[INFO 23-11-05 02:12:35.8782 UTC gradient_boosted_trees.cc:1556] \tnum-trees:285 train-loss:6.887659 train-rmse:6.887659 valid-loss:7.891096 valid-rmse:7.891096\n",
            "[INFO 23-11-05 02:13:06.0381 UTC gradient_boosted_trees.cc:1556] \tnum-trees:4 train-loss:8.590036 train-rmse:8.590036 valid-loss:8.525874 valid-rmse:8.525874\n",
            "[INFO 23-11-05 02:13:37.6001 UTC gradient_boosted_trees.cc:1556] \tnum-trees:10 train-loss:8.212384 train-rmse:8.212384 valid-loss:8.388592 valid-rmse:8.388592\n",
            "[INFO 23-11-05 02:14:08.3904 UTC gradient_boosted_trees.cc:1556] \tnum-trees:40 train-loss:8.005458 train-rmse:8.005458 valid-loss:8.268380 valid-rmse:8.268380\n",
            "[INFO 23-11-05 02:14:41.8728 UTC gradient_boosted_trees.cc:1556] \tnum-trees:148 train-loss:7.117645 train-rmse:7.117645 valid-loss:8.060744 valid-rmse:8.060744\n",
            "[INFO 23-11-05 02:15:13.2559 UTC gradient_boosted_trees.cc:1556] \tnum-trees:9 train-loss:8.522045 train-rmse:8.522045 valid-loss:8.499104 valid-rmse:8.499104\n",
            "[INFO 23-11-05 02:15:46.7434 UTC gradient_boosted_trees.cc:1556] \tnum-trees:287 train-loss:6.896702 train-rmse:6.896702 valid-loss:7.981731 valid-rmse:7.981731\n",
            "[INFO 23-11-05 02:16:18.0459 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:6.856059 train-rmse:6.856059 valid-loss:7.882258 valid-rmse:7.882258\n",
            "[INFO 23-11-05 02:16:50.2560 UTC gradient_boosted_trees.cc:1556] \tnum-trees:13 train-loss:8.469609 train-rmse:8.469609 valid-loss:8.483544 valid-rmse:8.483544\n",
            "[INFO 23-11-05 02:17:21.5007 UTC gradient_boosted_trees.cc:1556] \tnum-trees:237 train-loss:7.341611 train-rmse:7.341611 valid-loss:8.092890 valid-rmse:8.092890\n",
            "[INFO 23-11-05 02:17:52.8876 UTC gradient_boosted_trees.cc:1556] \tnum-trees:44 train-loss:7.710028 train-rmse:7.710028 valid-loss:8.241813 valid-rmse:8.241813\n",
            "[INFO 23-11-05 02:18:25.7850 UTC gradient_boosted_trees.cc:1556] \tnum-trees:157 train-loss:7.079776 train-rmse:7.079776 valid-loss:8.053524 valid-rmse:8.053524\n",
            "[INFO 23-11-05 02:18:42.3615 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.841818 train-rmse:6.841818 valid-loss:7.879538 valid-rmse:7.879538\n",
            "[INFO 23-11-05 02:18:42.3615 UTC gradient_boosted_trees.cc:249] Truncates the model to 296 tree(s) i.e. 296  iteration(s).\n",
            "[INFO 23-11-05 02:18:42.3615 UTC gradient_boosted_trees.cc:312] Final model num-trees:296 valid-loss:7.878903 valid-rmse:7.878903\n",
            "[INFO 23-11-05 02:18:42.3689 UTC hyperparameters_optimizer.cc:582] [59/100] Score: -7.8789 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 16 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 02:18:42.3698 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 02:18:42.3698 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 02:18:42.3877 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 02:18:58.7122 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.636476 train-rmse:8.636476 valid-loss:8.550441 valid-rmse:8.550441\n",
            "[INFO 23-11-05 02:19:01.4489 UTC gradient_boosted_trees.cc:1556] \tnum-trees:18 train-loss:8.413992 train-rmse:8.413992 valid-loss:8.455330 valid-rmse:8.455330\n",
            "[INFO 23-11-05 02:19:31.7828 UTC gradient_boosted_trees.cc:1556] \tnum-trees:3 train-loss:8.603011 train-rmse:8.603011 valid-loss:8.538020 valid-rmse:8.538020\n",
            "[INFO 23-11-05 02:20:04.8973 UTC gradient_boosted_trees.cc:1556] \tnum-trees:171 train-loss:6.611568 train-rmse:6.611568 valid-loss:7.901899 valid-rmse:7.901899\n",
            "[INFO 23-11-05 02:20:35.1214 UTC gradient_boosted_trees.cc:1556] \tnum-trees:21 train-loss:8.377631 train-rmse:8.377631 valid-loss:8.444335 valid-rmse:8.444335\n",
            "[INFO 23-11-05 02:21:08.3450 UTC gradient_boosted_trees.cc:1556] \tnum-trees:23 train-loss:8.364781 train-rmse:8.364781 valid-loss:8.436395 valid-rmse:8.436395\n",
            "[INFO 23-11-05 02:21:41.7153 UTC gradient_boosted_trees.cc:1556] \tnum-trees:174 train-loss:6.595154 train-rmse:6.595154 valid-loss:7.897764 valid-rmse:7.897764\n",
            "[INFO 23-11-05 02:22:11.8391 UTC gradient_boosted_trees.cc:1556] \tnum-trees:166 train-loss:7.040071 train-rmse:7.040071 valid-loss:8.044085 valid-rmse:8.044085\n",
            "[INFO 23-11-05 02:22:43.1838 UTC gradient_boosted_trees.cc:1556] \tnum-trees:256 train-loss:7.276682 train-rmse:7.276682 valid-loss:8.069885 valid-rmse:8.069885\n",
            "[INFO 23-11-05 02:23:13.1936 UTC gradient_boosted_trees.cc:1556] \tnum-trees:59 train-loss:7.877096 train-rmse:7.877096 valid-loss:8.227091 valid-rmse:8.227091\n",
            "[INFO 23-11-05 02:23:48.1529 UTC gradient_boosted_trees.cc:1556] \tnum-trees:38 train-loss:7.802813 train-rmse:7.802813 valid-loss:8.272836 valid-rmse:8.272836\n",
            "[INFO 23-11-05 02:23:52.3203 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.860664 train-rmse:6.860664 valid-loss:7.976002 valid-rmse:7.976002\n",
            "[INFO 23-11-05 02:23:52.3203 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 02:23:52.3203 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.975985 valid-rmse:7.975985\n",
            "[INFO 23-11-05 02:23:52.3334 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 02:23:52.3334 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 02:23:52.3347 UTC hyperparameters_optimizer.cc:582] [60/100] Score: -7.97599 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 02:23:52.3511 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 02:24:08.6281 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.623615 train-rmse:8.623615 valid-loss:8.541377 valid-rmse:8.541377\n",
            "[INFO 23-11-05 02:24:18.5941 UTC gradient_boosted_trees.cc:1556] \tnum-trees:29 train-loss:8.298532 train-rmse:8.298532 valid-loss:8.416513 valid-rmse:8.416513\n",
            "[INFO 23-11-05 02:24:48.6932 UTC gradient_boosted_trees.cc:1556] \tnum-trees:22 train-loss:8.370504 train-rmse:8.370504 valid-loss:8.453019 valid-rmse:8.453019\n",
            "[INFO 23-11-05 02:25:22.2093 UTC gradient_boosted_trees.cc:1556] \tnum-trees:24 train-loss:8.353968 train-rmse:8.353968 valid-loss:8.444031 valid-rmse:8.444031\n",
            "[INFO 23-11-05 02:25:55.0404 UTC gradient_boosted_trees.cc:1556] \tnum-trees:26 train-loss:8.334218 train-rmse:8.334218 valid-loss:8.436354 valid-rmse:8.436354\n",
            "[INFO 23-11-05 02:26:25.5919 UTC gradient_boosted_trees.cc:1556] \tnum-trees:269 train-loss:7.236289 train-rmse:7.236289 valid-loss:8.061529 valid-rmse:8.061529\n",
            "[INFO 23-11-05 02:27:00.3193 UTC gradient_boosted_trees.cc:1556] \tnum-trees:271 train-loss:7.231066 train-rmse:7.231066 valid-loss:8.057856 valid-rmse:8.057856\n",
            "[INFO 23-11-05 02:27:31.7318 UTC gradient_boosted_trees.cc:1556] \tnum-trees:38 train-loss:8.243209 train-rmse:8.243209 valid-loss:8.385020 valid-rmse:8.385020\n",
            "[INFO 23-11-05 02:28:02.5958 UTC gradient_boosted_trees.cc:1556] \tnum-trees:37 train-loss:8.235036 train-rmse:8.235036 valid-loss:8.399667 valid-rmse:8.399667\n",
            "[INFO 23-11-05 02:28:32.9015 UTC gradient_boosted_trees.cc:1556] \tnum-trees:17 train-loss:8.336126 train-rmse:8.336126 valid-loss:8.434634 valid-rmse:8.434634\n",
            "[INFO 23-11-05 02:29:05.8711 UTC gradient_boosted_trees.cc:1556] \tnum-trees:19 train-loss:8.314628 train-rmse:8.314628 valid-loss:8.431728 valid-rmse:8.431728\n",
            "[INFO 23-11-05 02:29:38.7976 UTC gradient_boosted_trees.cc:1556] \tnum-trees:54 train-loss:7.615259 train-rmse:7.615259 valid-loss:8.206310 valid-rmse:8.206310\n",
            "[INFO 23-11-05 02:30:08.8002 UTC gradient_boosted_trees.cc:1556] \tnum-trees:282 train-loss:7.200099 train-rmse:7.200099 valid-loss:8.049228 valid-rmse:8.049228\n",
            "[INFO 23-11-05 02:30:41.2992 UTC gradient_boosted_trees.cc:1556] \tnum-trees:43 train-loss:8.191676 train-rmse:8.191676 valid-loss:8.382546 valid-rmse:8.382546\n",
            "[INFO 23-11-05 02:31:14.6567 UTC gradient_boosted_trees.cc:1556] \tnum-trees:45 train-loss:8.178020 train-rmse:8.178020 valid-loss:8.381794 valid-rmse:8.381794\n",
            "[INFO 23-11-05 02:31:46.8649 UTC gradient_boosted_trees.cc:1556] \tnum-trees:48 train-loss:8.171913 train-rmse:8.171913 valid-loss:8.369935 valid-rmse:8.369935\n",
            "[INFO 23-11-05 02:32:20.3897 UTC gradient_boosted_trees.cc:1556] \tnum-trees:78 train-loss:7.747209 train-rmse:7.747209 valid-loss:8.187686 valid-rmse:8.187686\n",
            "[INFO 23-11-05 02:32:51.0565 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:7.342932 train-rmse:7.342932 valid-loss:8.155253 valid-rmse:8.155253\n",
            "[INFO 23-11-05 02:33:28.8171 UTC gradient_boosted_trees.cc:1556] \tnum-trees:193 train-loss:6.925575 train-rmse:6.925575 valid-loss:8.004843 valid-rmse:8.004843\n",
            "[INFO 23-11-05 02:34:02.5172 UTC gradient_boosted_trees.cc:1556] \tnum-trees:66 train-loss:7.488461 train-rmse:7.488461 valid-loss:8.169103 valid-rmse:8.169103\n",
            "[INFO 23-11-05 02:34:34.5173 UTC gradient_boosted_trees.cc:1556] \tnum-trees:108 train-loss:7.436270 train-rmse:7.436270 valid-loss:8.091599 valid-rmse:8.091599\n",
            "[INFO 23-11-05 02:35:05.1255 UTC gradient_boosted_trees.cc:1556] \tnum-trees:52 train-loss:8.130518 train-rmse:8.130518 valid-loss:8.369898 valid-rmse:8.369898\n",
            "[INFO 23-11-05 02:35:16.9495 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.148103 train-rmse:7.148103 valid-loss:8.029921 valid-rmse:8.029921\n",
            "[INFO 23-11-05 02:35:16.9495 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 02:35:16.9495 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.029921 valid-rmse:8.029921\n",
            "[INFO 23-11-05 02:35:16.9589 UTC hyperparameters_optimizer.cc:582] [61/100] Score: -8.02992 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 02:35:16.9600 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 02:35:16.9600 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 02:35:16.9767 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 02:35:37.0474 UTC gradient_boosted_trees.cc:1556] \tnum-trees:57 train-loss:8.116942 train-rmse:8.116942 valid-loss:8.351737 valid-rmse:8.351737\n",
            "[INFO 23-11-05 02:35:39.7811 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.615808 train-rmse:8.615808 valid-loss:8.534506 valid-rmse:8.534506\n",
            "[INFO 23-11-05 02:36:08.4446 UTC gradient_boosted_trees.cc:1556] \tnum-trees:86 train-loss:7.679387 train-rmse:7.679387 valid-loss:8.156754 valid-rmse:8.156754\n",
            "[INFO 23-11-05 02:36:39.2315 UTC gradient_boosted_trees.cc:1556] \tnum-trees:46 train-loss:8.136562 train-rmse:8.136562 valid-loss:8.350261 valid-rmse:8.350261\n",
            "[INFO 23-11-05 02:37:09.6997 UTC gradient_boosted_trees.cc:1556] \tnum-trees:66 train-loss:8.052794 train-rmse:8.052794 valid-loss:8.332564 valid-rmse:8.332564\n",
            "[INFO 23-11-05 02:37:40.8108 UTC gradient_boosted_trees.cc:1556] \tnum-trees:76 train-loss:7.409063 train-rmse:7.409063 valid-loss:8.146729 valid-rmse:8.146729\n",
            "[INFO 23-11-05 02:38:17.7416 UTC gradient_boosted_trees.cc:1556] \tnum-trees:70 train-loss:8.036161 train-rmse:8.036161 valid-loss:8.323795 valid-rmse:8.323795\n",
            "[INFO 23-11-05 02:38:51.6223 UTC gradient_boosted_trees.cc:1556] \tnum-trees:72 train-loss:8.027492 train-rmse:8.027492 valid-loss:8.319803 valid-rmse:8.319803\n",
            "[INFO 23-11-05 02:39:23.8039 UTC gradient_boosted_trees.cc:1556] \tnum-trees:119 train-loss:7.371930 train-rmse:7.371930 valid-loss:8.079578 valid-rmse:8.079578\n",
            "[INFO 23-11-05 02:39:56.3096 UTC gradient_boosted_trees.cc:1556] \tnum-trees:94 train-loss:7.621623 train-rmse:7.621623 valid-loss:8.132672 valid-rmse:8.132672\n",
            "[INFO 23-11-05 02:40:28.9291 UTC gradient_boosted_trees.cc:1556] \tnum-trees:209 train-loss:6.409680 train-rmse:6.409680 valid-loss:7.857611 valid-rmse:7.857611\n",
            "[INFO 23-11-05 02:41:00.5137 UTC gradient_boosted_trees.cc:1556] \tnum-trees:15 train-loss:8.250336 train-rmse:8.250336 valid-loss:8.388301 valid-rmse:8.388301\n",
            "[INFO 23-11-05 02:41:32.9112 UTC gradient_boosted_trees.cc:1556] \tnum-trees:211 train-loss:6.396733 train-rmse:6.396733 valid-loss:7.854491 valid-rmse:7.854491\n",
            "[INFO 23-11-05 02:42:02.9383 UTC gradient_boosted_trees.cc:1556] \tnum-trees:72 train-loss:8.036584 train-rmse:8.036584 valid-loss:8.322770 valid-rmse:8.322770\n",
            "[INFO 23-11-05 02:42:33.8833 UTC gradient_boosted_trees.cc:1556] \tnum-trees:67 train-loss:8.023632 train-rmse:8.023632 valid-loss:8.298733 valid-rmse:8.298733\n",
            "[INFO 23-11-05 02:43:04.0064 UTC gradient_boosted_trees.cc:1556] \tnum-trees:216 train-loss:6.819551 train-rmse:6.819551 valid-loss:7.979622 valid-rmse:7.979622\n",
            "[INFO 23-11-05 02:43:40.7775 UTC gradient_boosted_trees.cc:1556] \tnum-trees:22 train-loss:8.142650 train-rmse:8.142650 valid-loss:8.356707 valid-rmse:8.356707\n",
            "[INFO 23-11-05 02:44:12.9782 UTC gradient_boosted_trees.cc:1556] \tnum-trees:216 train-loss:6.368368 train-rmse:6.368368 valid-loss:7.847477 valid-rmse:7.847477\n",
            "[INFO 23-11-05 02:44:43.7637 UTC gradient_boosted_trees.cc:1556] \tnum-trees:104 train-loss:7.568538 train-rmse:7.568538 valid-loss:8.113451 valid-rmse:8.113451\n",
            "[INFO 23-11-05 02:45:16.6355 UTC gradient_boosted_trees.cc:1556] \tnum-trees:218 train-loss:6.357991 train-rmse:6.357991 valid-loss:7.844249 valid-rmse:7.844249\n",
            "[INFO 23-11-05 02:45:48.9932 UTC gradient_boosted_trees.cc:1556] \tnum-trees:219 train-loss:6.351281 train-rmse:6.351281 valid-loss:7.845632 valid-rmse:7.845632\n",
            "[INFO 23-11-05 02:46:20.2751 UTC gradient_boosted_trees.cc:1556] \tnum-trees:220 train-loss:6.342720 train-rmse:6.342720 valid-loss:7.842939 valid-rmse:7.842939\n",
            "[INFO 23-11-05 02:46:50.2965 UTC gradient_boosted_trees.cc:1556] \tnum-trees:136 train-loss:7.279230 train-rmse:7.279230 valid-loss:8.037770 valid-rmse:8.037770\n",
            "[INFO 23-11-05 02:47:20.6176 UTC gradient_boosted_trees.cc:1556] \tnum-trees:105 train-loss:7.084108 train-rmse:7.084108 valid-loss:8.094279 valid-rmse:8.094279\n",
            "[INFO 23-11-05 02:47:51.7018 UTC gradient_boosted_trees.cc:1556] \tnum-trees:79 train-loss:7.992556 train-rmse:7.992556 valid-loss:8.316476 valid-rmse:8.316476\n",
            "[INFO 23-11-05 02:48:24.2037 UTC gradient_boosted_trees.cc:1556] \tnum-trees:87 train-loss:7.960706 train-rmse:7.960706 valid-loss:8.303842 valid-rmse:8.303842\n",
            "[INFO 23-11-05 02:48:54.4132 UTC gradient_boosted_trees.cc:1556] \tnum-trees:230 train-loss:6.760909 train-rmse:6.760909 valid-loss:7.971536 valid-rmse:7.971536\n",
            "[INFO 23-11-05 02:49:26.6345 UTC gradient_boosted_trees.cc:1556] \tnum-trees:142 train-loss:7.250701 train-rmse:7.250701 valid-loss:8.027117 valid-rmse:8.027117\n",
            "[INFO 23-11-05 02:49:57.7694 UTC gradient_boosted_trees.cc:1556] \tnum-trees:111 train-loss:7.863832 train-rmse:7.863832 valid-loss:8.266528 valid-rmse:8.266528\n",
            "[INFO 23-11-05 02:50:32.3178 UTC gradient_boosted_trees.cc:1556] \tnum-trees:113 train-loss:7.857425 train-rmse:7.857425 valid-loss:8.265753 valid-rmse:8.265753\n",
            "[INFO 23-11-05 02:51:06.1423 UTC gradient_boosted_trees.cc:1556] \tnum-trees:113 train-loss:7.073443 train-rmse:7.073443 valid-loss:8.062734 valid-rmse:8.062734\n",
            "[INFO 23-11-05 02:51:37.0394 UTC gradient_boosted_trees.cc:1556] \tnum-trees:147 train-loss:7.228334 train-rmse:7.228334 valid-loss:8.023639 valid-rmse:8.023639\n",
            "[INFO 23-11-05 02:52:08.2118 UTC gradient_boosted_trees.cc:1556] \tnum-trees:88 train-loss:7.957305 train-rmse:7.957305 valid-loss:8.308794 valid-rmse:8.308794\n",
            "[INFO 23-11-05 02:52:38.5139 UTC gradient_boosted_trees.cc:1556] \tnum-trees:103 train-loss:7.870188 train-rmse:7.870188 valid-loss:8.249512 valid-rmse:8.249512\n",
            "[INFO 23-11-05 02:53:09.0472 UTC gradient_boosted_trees.cc:1556] \tnum-trees:98 train-loss:7.910403 train-rmse:7.910403 valid-loss:8.291971 valid-rmse:8.291971\n",
            "[INFO 23-11-05 02:53:41.0905 UTC gradient_boosted_trees.cc:1556] \tnum-trees:124 train-loss:7.813708 train-rmse:7.813708 valid-loss:8.246598 valid-rmse:8.246598\n",
            "[INFO 23-11-05 02:54:13.6833 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:6.704810 train-rmse:6.704810 valid-loss:7.970217 valid-rmse:7.970217\n",
            "[INFO 23-11-05 02:54:44.7474 UTC gradient_boosted_trees.cc:1556] \tnum-trees:125 train-loss:7.455395 train-rmse:7.455395 valid-loss:8.087813 valid-rmse:8.087813\n",
            "[INFO 23-11-05 02:55:18.5995 UTC gradient_boosted_trees.cc:1556] \tnum-trees:103 train-loss:7.886885 train-rmse:7.886885 valid-loss:8.284441 valid-rmse:8.284441\n",
            "[INFO 23-11-05 02:55:52.3036 UTC gradient_boosted_trees.cc:1556] \tnum-trees:126 train-loss:6.981896 train-rmse:6.981896 valid-loss:8.039537 valid-rmse:8.039537\n",
            "[INFO 23-11-05 02:56:23.6677 UTC gradient_boosted_trees.cc:1556] \tnum-trees:97 train-loss:7.915782 train-rmse:7.915782 valid-loss:8.293558 valid-rmse:8.293558\n",
            "[INFO 23-11-05 02:56:57.7262 UTC gradient_boosted_trees.cc:1556] \tnum-trees:129 train-loss:6.962807 train-rmse:6.962807 valid-loss:8.034459 valid-rmse:8.034459\n",
            "[INFO 23-11-05 02:57:29.3426 UTC gradient_boosted_trees.cc:1556] \tnum-trees:108 train-loss:7.863558 train-rmse:7.863558 valid-loss:8.277778 valid-rmse:8.277778\n",
            "[INFO 23-11-05 02:58:01.7674 UTC gradient_boosted_trees.cc:1556] \tnum-trees:127 train-loss:6.913913 train-rmse:6.913913 valid-loss:8.040536 valid-rmse:8.040536\n",
            "[INFO 23-11-05 02:58:32.7119 UTC gradient_boosted_trees.cc:1556] \tnum-trees:163 train-loss:7.149285 train-rmse:7.149285 valid-loss:8.000928 valid-rmse:8.000928\n",
            "[INFO 23-11-05 02:59:03.2460 UTC gradient_boosted_trees.cc:1556] \tnum-trees:134 train-loss:7.414389 train-rmse:7.414389 valid-loss:8.075769 valid-rmse:8.075769\n",
            "[INFO 23-11-05 02:59:35.7584 UTC gradient_boosted_trees.cc:1556] \tnum-trees:256 train-loss:6.647570 train-rmse:6.647570 valid-loss:7.954095 valid-rmse:7.954095\n",
            "[INFO 23-11-05 03:00:11.2825 UTC gradient_boosted_trees.cc:1556] \tnum-trees:65 train-loss:7.703650 train-rmse:7.703650 valid-loss:8.192499 valid-rmse:8.192499\n",
            "[INFO 23-11-05 03:00:42.7516 UTC gradient_boosted_trees.cc:1556] \tnum-trees:168 train-loss:7.130495 train-rmse:7.130495 valid-loss:7.994273 valid-rmse:7.994273\n",
            "[INFO 23-11-05 03:01:14.9335 UTC gradient_boosted_trees.cc:1556] \tnum-trees:260 train-loss:6.635808 train-rmse:6.635808 valid-loss:7.955315 valid-rmse:7.955315\n",
            "[INFO 23-11-05 03:01:48.1463 UTC gradient_boosted_trees.cc:1556] \tnum-trees:118 train-loss:7.819306 train-rmse:7.819306 valid-loss:8.261208 valid-rmse:8.261208\n",
            "[INFO 23-11-05 03:02:19.3569 UTC gradient_boosted_trees.cc:1556] \tnum-trees:144 train-loss:6.852590 train-rmse:6.852590 valid-loss:8.007530 valid-rmse:8.007530\n",
            "[INFO 23-11-05 03:02:53.0084 UTC gradient_boosted_trees.cc:1556] \tnum-trees:137 train-loss:6.842606 train-rmse:6.842606 valid-loss:8.017793 valid-rmse:8.017793\n",
            "[INFO 23-11-05 03:03:23.6978 UTC gradient_boosted_trees.cc:1556] \tnum-trees:158 train-loss:7.699514 train-rmse:7.699514 valid-loss:8.198707 valid-rmse:8.198707\n",
            "[INFO 23-11-05 03:03:56.5523 UTC gradient_boosted_trees.cc:1556] \tnum-trees:123 train-loss:7.795389 train-rmse:7.795389 valid-loss:8.253180 valid-rmse:8.253180\n",
            "[INFO 23-11-05 03:04:26.8497 UTC gradient_boosted_trees.cc:1556] \tnum-trees:76 train-loss:7.629621 train-rmse:7.629621 valid-loss:8.162086 valid-rmse:8.162086\n",
            "[INFO 23-11-05 03:05:00.0454 UTC gradient_boosted_trees.cc:1556] \tnum-trees:269 train-loss:6.607963 train-rmse:6.607963 valid-loss:7.949254 valid-rmse:7.949254\n",
            "[INFO 23-11-05 03:05:33.4537 UTC gradient_boosted_trees.cc:1556] \tnum-trees:256 train-loss:6.148464 train-rmse:6.148464 valid-loss:7.805983 valid-rmse:7.805983\n",
            "[INFO 23-11-05 03:06:05.2784 UTC gradient_boosted_trees.cc:1556] \tnum-trees:257 train-loss:6.141156 train-rmse:6.141156 valid-loss:7.803729 valid-rmse:7.803729\n",
            "[INFO 23-11-05 03:06:37.0731 UTC gradient_boosted_trees.cc:1556] \tnum-trees:258 train-loss:6.136583 train-rmse:6.136583 valid-loss:7.802886 valid-rmse:7.802886\n",
            "[INFO 23-11-05 03:07:08.4137 UTC gradient_boosted_trees.cc:1556] \tnum-trees:83 train-loss:7.577553 train-rmse:7.577553 valid-loss:8.133243 valid-rmse:8.133243\n",
            "[INFO 23-11-05 03:07:39.2943 UTC gradient_boosted_trees.cc:1556] \tnum-trees:173 train-loss:7.650311 train-rmse:7.650311 valid-loss:8.177373 valid-rmse:8.177373\n",
            "[INFO 23-11-05 03:08:12.2190 UTC gradient_boosted_trees.cc:1556] \tnum-trees:153 train-loss:7.330496 train-rmse:7.330496 valid-loss:8.044295 valid-rmse:8.044295\n",
            "[INFO 23-11-05 03:08:43.5495 UTC gradient_boosted_trees.cc:1556] \tnum-trees:278 train-loss:6.571385 train-rmse:6.571385 valid-loss:7.941374 valid-rmse:7.941374\n",
            "[INFO 23-11-05 03:09:13.5583 UTC gradient_boosted_trees.cc:1556] \tnum-trees:162 train-loss:7.665482 train-rmse:7.665482 valid-loss:8.175968 valid-rmse:8.175968\n",
            "[INFO 23-11-05 03:09:45.3499 UTC gradient_boosted_trees.cc:1556] \tnum-trees:125 train-loss:7.798347 train-rmse:7.798347 valid-loss:8.243320 valid-rmse:8.243320\n",
            "[INFO 23-11-05 03:10:17.8270 UTC gradient_boosted_trees.cc:1556] \tnum-trees:190 train-loss:7.038864 train-rmse:7.038864 valid-loss:7.969924 valid-rmse:7.969924\n",
            "[INFO 23-11-05 03:10:49.7456 UTC gradient_boosted_trees.cc:1556] \tnum-trees:139 train-loss:7.739556 train-rmse:7.739556 valid-loss:8.232490 valid-rmse:8.232490\n",
            "[INFO 23-11-05 03:11:24.1957 UTC gradient_boosted_trees.cc:1556] \tnum-trees:169 train-loss:6.692214 train-rmse:6.692214 valid-loss:7.977765 valid-rmse:7.977765\n",
            "[INFO 23-11-05 03:12:00.1578 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:6.087256 train-rmse:6.087256 valid-loss:7.787570 valid-rmse:7.787570\n",
            "[INFO 23-11-05 03:12:32.7219 UTC gradient_boosted_trees.cc:1556] \tnum-trees:269 train-loss:6.083774 train-rmse:6.083774 valid-loss:7.785848 valid-rmse:7.785848\n",
            "[INFO 23-11-05 03:13:04.9404 UTC gradient_boosted_trees.cc:1556] \tnum-trees:270 train-loss:6.078065 train-rmse:6.078065 valid-loss:7.786701 valid-rmse:7.786701\n",
            "[INFO 23-11-05 03:13:35.4766 UTC gradient_boosted_trees.cc:1556] \tnum-trees:159 train-loss:6.691042 train-rmse:6.691042 valid-loss:7.966688 valid-rmse:7.966688\n",
            "[INFO 23-11-05 03:14:05.4892 UTC gradient_boosted_trees.cc:1556] \tnum-trees:291 train-loss:6.518755 train-rmse:6.518755 valid-loss:7.929916 valid-rmse:7.929916\n",
            "[INFO 23-11-05 03:14:38.2038 UTC gradient_boosted_trees.cc:1556] \tnum-trees:200 train-loss:6.986896 train-rmse:6.986896 valid-loss:7.955694 valid-rmse:7.955694\n",
            "[INFO 23-11-05 03:15:12.7434 UTC gradient_boosted_trees.cc:1556] \tnum-trees:274 train-loss:6.049055 train-rmse:6.049055 valid-loss:7.776583 valid-rmse:7.776583\n",
            "[INFO 23-11-05 03:15:43.1474 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:6.503338 train-rmse:6.503338 valid-loss:7.927435 valid-rmse:7.927435\n",
            "[INFO 23-11-05 03:16:13.8857 UTC gradient_boosted_trees.cc:1556] \tnum-trees:187 train-loss:7.580816 train-rmse:7.580816 valid-loss:8.150062 valid-rmse:8.150062\n",
            "[INFO 23-11-05 03:16:45.1801 UTC gradient_boosted_trees.cc:1556] \tnum-trees:205 train-loss:7.557768 train-rmse:7.557768 valid-loss:8.155618 valid-rmse:8.155618\n",
            "[INFO 23-11-05 03:17:16.1772 UTC gradient_boosted_trees.cc:1556] \tnum-trees:206 train-loss:6.957587 train-rmse:6.957587 valid-loss:7.949858 valid-rmse:7.949858\n",
            "[INFO 23-11-05 03:17:47.0323 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.486804 train-rmse:6.486804 valid-loss:7.924564 valid-rmse:7.924564\n",
            "[INFO 23-11-05 03:17:47.0323 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 03:17:47.0323 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.923769 valid-rmse:7.923769\n",
            "[INFO 23-11-05 03:17:47.0406 UTC hyperparameters_optimizer.cc:582] [62/100] Score: -7.92377 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 03:17:47.0444 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 03:17:47.0449 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 03:17:47.0589 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 03:17:51.9530 UTC gradient_boosted_trees.cc:1556] \tnum-trees:142 train-loss:7.739538 train-rmse:7.739538 valid-loss:8.223848 valid-rmse:8.223848\n",
            "[INFO 23-11-05 03:17:54.9468 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.638092 train-rmse:8.638092 valid-loss:8.551674 valid-rmse:8.551674\n",
            "[INFO 23-11-05 03:18:22.9660 UTC gradient_boosted_trees.cc:1556] \tnum-trees:112 train-loss:7.390264 train-rmse:7.390264 valid-loss:8.083451 valid-rmse:8.083451\n",
            "[INFO 23-11-05 03:18:54.0001 UTC gradient_boosted_trees.cc:1556] \tnum-trees:170 train-loss:6.613204 train-rmse:6.613204 valid-loss:7.941436 valid-rmse:7.941436\n",
            "[INFO 23-11-05 03:19:26.0335 UTC gradient_boosted_trees.cc:1556] \tnum-trees:191 train-loss:6.547326 train-rmse:6.547326 valid-loss:7.927904 valid-rmse:7.927904\n",
            "[INFO 23-11-05 03:19:56.1408 UTC gradient_boosted_trees.cc:1556] \tnum-trees:116 train-loss:7.371629 train-rmse:7.371629 valid-loss:8.078474 valid-rmse:8.078474\n",
            "[INFO 23-11-05 03:20:26.6455 UTC gradient_boosted_trees.cc:1556] \tnum-trees:218 train-loss:7.515365 train-rmse:7.515365 valid-loss:8.140699 valid-rmse:8.140699\n",
            "[INFO 23-11-05 03:20:59.9218 UTC gradient_boosted_trees.cc:1556] \tnum-trees:220 train-loss:7.508552 train-rmse:7.508552 valid-loss:8.138748 valid-rmse:8.138748\n",
            "[INFO 23-11-05 03:21:33.0898 UTC gradient_boosted_trees.cc:1556] \tnum-trees:222 train-loss:7.501614 train-rmse:7.501614 valid-loss:8.136172 valid-rmse:8.136172\n",
            "[INFO 23-11-05 03:22:07.0817 UTC gradient_boosted_trees.cc:1556] \tnum-trees:224 train-loss:7.494752 train-rmse:7.494752 valid-loss:8.135309 valid-rmse:8.135309\n",
            "[INFO 23-11-05 03:22:38.8009 UTC gradient_boosted_trees.cc:1556] \tnum-trees:37 train-loss:8.357958 train-rmse:8.357958 valid-loss:8.419608 valid-rmse:8.419608\n",
            "[INFO 23-11-05 03:23:09.8558 UTC gradient_boosted_trees.cc:1556] \tnum-trees:41 train-loss:8.343113 train-rmse:8.343113 valid-loss:8.411250 valid-rmse:8.411250\n",
            "[INFO 23-11-05 03:23:41.0839 UTC gradient_boosted_trees.cc:1556] \tnum-trees:45 train-loss:8.328294 train-rmse:8.328294 valid-loss:8.400508 valid-rmse:8.400508\n",
            "[INFO 23-11-05 03:24:12.2354 UTC gradient_boosted_trees.cc:1556] \tnum-trees:49 train-loss:8.314322 train-rmse:8.314322 valid-loss:8.392816 valid-rmse:8.392816\n",
            "[INFO 23-11-05 03:24:42.3217 UTC gradient_boosted_trees.cc:1556] \tnum-trees:217 train-loss:7.492620 train-rmse:7.492620 valid-loss:8.112867 valid-rmse:8.112867\n",
            "[INFO 23-11-05 03:25:12.9315 UTC gradient_boosted_trees.cc:1556] \tnum-trees:207 train-loss:6.435517 train-rmse:6.435517 valid-loss:7.891356 valid-rmse:7.891356\n",
            "[INFO 23-11-05 03:25:45.2991 UTC gradient_boosted_trees.cc:1556] \tnum-trees:184 train-loss:6.532211 train-rmse:6.532211 valid-loss:7.921580 valid-rmse:7.921580\n",
            "[INFO 23-11-05 03:26:17.3820 UTC gradient_boosted_trees.cc:1556] \tnum-trees:210 train-loss:6.413684 train-rmse:6.413684 valid-loss:7.883721 valid-rmse:7.883721\n",
            "[INFO 23-11-05 03:26:48.1166 UTC gradient_boosted_trees.cc:1556] \tnum-trees:176 train-loss:7.623538 train-rmse:7.623538 valid-loss:8.189151 valid-rmse:8.189151\n",
            "[INFO 23-11-05 03:27:21.3480 UTC gradient_boosted_trees.cc:1556] \tnum-trees:73 train-loss:8.248600 train-rmse:8.248600 valid-loss:8.367783 valid-rmse:8.367783\n",
            "[INFO 23-11-05 03:27:51.6717 UTC gradient_boosted_trees.cc:1556] \tnum-trees:244 train-loss:7.430954 train-rmse:7.430954 valid-loss:8.114585 valid-rmse:8.114585\n",
            "[INFO 23-11-05 03:28:22.3594 UTC gradient_boosted_trees.cc:1556] \tnum-trees:195 train-loss:7.149587 train-rmse:7.149587 valid-loss:7.996644 valid-rmse:7.996644\n",
            "[INFO 23-11-05 03:28:54.8851 UTC gradient_boosted_trees.cc:1556] \tnum-trees:232 train-loss:7.448441 train-rmse:7.448441 valid-loss:8.096373 valid-rmse:8.096373\n",
            "[INFO 23-11-05 03:29:05.1423 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.920134 train-rmse:5.920134 valid-loss:7.741993 valid-rmse:7.741993\n",
            "[INFO 23-11-05 03:29:05.1424 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 03:29:05.1424 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.741993 valid-rmse:7.741993\n",
            "[INFO 23-11-05 03:29:05.1536 UTC hyperparameters_optimizer.cc:582] [63/100] Score: -7.74199 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 03:29:05.1545 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 03:29:05.1546 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 03:29:05.1704 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 03:29:12.7238 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.627463 train-rmse:8.627463 valid-loss:8.549892 valid-rmse:8.549892\n",
            "[INFO 23-11-05 03:29:27.6121 UTC gradient_boosted_trees.cc:1556] \tnum-trees:89 train-loss:8.212604 train-rmse:8.212604 valid-loss:8.343355 valid-rmse:8.343355\n",
            "[INFO 23-11-05 03:29:58.4861 UTC gradient_boosted_trees.cc:1556] \tnum-trees:7 train-loss:8.525668 train-rmse:8.525668 valid-loss:8.512881 valid-rmse:8.512881\n",
            "[INFO 23-11-05 03:30:29.0156 UTC gradient_boosted_trees.cc:1556] \tnum-trees:11 train-loss:8.471403 train-rmse:8.471403 valid-loss:8.500099 valid-rmse:8.500099\n",
            "[INFO 23-11-05 03:30:59.5365 UTC gradient_boosted_trees.cc:1556] \tnum-trees:15 train-loss:8.429585 train-rmse:8.429585 valid-loss:8.489455 valid-rmse:8.489455\n",
            "[INFO 23-11-05 03:31:30.4179 UTC gradient_boosted_trees.cc:1556] \tnum-trees:19 train-loss:8.392921 train-rmse:8.392921 valid-loss:8.471267 valid-rmse:8.471267\n",
            "[INFO 23-11-05 03:32:01.4182 UTC gradient_boosted_trees.cc:1556] \tnum-trees:23 train-loss:8.367683 train-rmse:8.367683 valid-loss:8.449230 valid-rmse:8.449230\n",
            "[INFO 23-11-05 03:32:32.0998 UTC gradient_boosted_trees.cc:1556] \tnum-trees:27 train-loss:8.341275 train-rmse:8.341275 valid-loss:8.433588 valid-rmse:8.433588\n",
            "[INFO 23-11-05 03:33:02.8851 UTC gradient_boosted_trees.cc:1556] \tnum-trees:31 train-loss:8.318874 train-rmse:8.318874 valid-loss:8.428347 valid-rmse:8.428347\n",
            "[INFO 23-11-05 03:33:33.4226 UTC gradient_boosted_trees.cc:1556] \tnum-trees:35 train-loss:8.299682 train-rmse:8.299682 valid-loss:8.418106 valid-rmse:8.418106\n",
            "[INFO 23-11-05 03:34:03.6577 UTC gradient_boosted_trees.cc:1556] \tnum-trees:39 train-loss:8.279428 train-rmse:8.279428 valid-loss:8.406027 valid-rmse:8.406027\n",
            "[INFO 23-11-05 03:34:34.0490 UTC gradient_boosted_trees.cc:1556] \tnum-trees:233 train-loss:6.271010 train-rmse:6.271010 valid-loss:7.852153 valid-rmse:7.852153\n",
            "[INFO 23-11-05 03:35:04.5659 UTC gradient_boosted_trees.cc:1556] \tnum-trees:269 train-loss:7.367130 train-rmse:7.367130 valid-loss:8.103024 valid-rmse:8.103024\n",
            "[INFO 23-11-05 03:35:35.2969 UTC gradient_boosted_trees.cc:1556] \tnum-trees:51 train-loss:8.228273 train-rmse:8.228273 valid-loss:8.389111 valid-rmse:8.389111\n",
            "[INFO 23-11-05 03:36:05.5652 UTC gradient_boosted_trees.cc:1556] \tnum-trees:140 train-loss:8.118041 train-rmse:8.118041 valid-loss:8.295481 valid-rmse:8.295481\n",
            "[INFO 23-11-05 03:36:36.1480 UTC gradient_boosted_trees.cc:1556] \tnum-trees:59 train-loss:8.201717 train-rmse:8.201717 valid-loss:8.382184 valid-rmse:8.382184\n",
            "[INFO 23-11-05 03:37:06.8825 UTC gradient_boosted_trees.cc:1556] \tnum-trees:63 train-loss:8.186520 train-rmse:8.186520 valid-loss:8.372581 valid-rmse:8.372581\n",
            "[INFO 23-11-05 03:37:37.6150 UTC gradient_boosted_trees.cc:1556] \tnum-trees:67 train-loss:8.173470 train-rmse:8.173470 valid-loss:8.372067 valid-rmse:8.372067\n",
            "[INFO 23-11-05 03:38:08.5048 UTC gradient_boosted_trees.cc:1556] \tnum-trees:71 train-loss:8.161025 train-rmse:8.161025 valid-loss:8.364278 valid-rmse:8.364278\n",
            "[INFO 23-11-05 03:38:38.8330 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:8.147015 train-rmse:8.147015 valid-loss:8.357234 valid-rmse:8.357234\n",
            "[INFO 23-11-05 03:39:09.4263 UTC gradient_boosted_trees.cc:1556] \tnum-trees:79 train-loss:8.132999 train-rmse:8.132999 valid-loss:8.351701 valid-rmse:8.351701\n",
            "[INFO 23-11-05 03:39:40.0662 UTC gradient_boosted_trees.cc:1556] \tnum-trees:83 train-loss:8.122090 train-rmse:8.122090 valid-loss:8.355248 valid-rmse:8.355248\n",
            "[INFO 23-11-05 03:40:10.2547 UTC gradient_boosted_trees.cc:1556] \tnum-trees:207 train-loss:7.529861 train-rmse:7.529861 valid-loss:8.157412 valid-rmse:8.157412\n",
            "[INFO 23-11-05 03:40:41.2336 UTC gradient_boosted_trees.cc:1556] \tnum-trees:91 train-loss:8.097914 train-rmse:8.097914 valid-loss:8.345963 valid-rmse:8.345963\n",
            "[INFO 23-11-05 03:41:11.7764 UTC gradient_boosted_trees.cc:1556] \tnum-trees:95 train-loss:8.086182 train-rmse:8.086182 valid-loss:8.345608 valid-rmse:8.345608\n",
            "[INFO 23-11-05 03:41:42.2163 UTC gradient_boosted_trees.cc:1556] \tnum-trees:99 train-loss:8.074511 train-rmse:8.074511 valid-loss:8.346478 valid-rmse:8.346478\n",
            "[INFO 23-11-05 03:42:12.9684 UTC gradient_boosted_trees.cc:1556] \tnum-trees:103 train-loss:8.063753 train-rmse:8.063753 valid-loss:8.334121 valid-rmse:8.334121\n",
            "[INFO 23-11-05 03:42:43.3116 UTC gradient_boosted_trees.cc:1556] \tnum-trees:194 train-loss:7.575267 train-rmse:7.575267 valid-loss:8.161605 valid-rmse:8.161605\n",
            "[INFO 23-11-05 03:43:14.1541 UTC gradient_boosted_trees.cc:1556] \tnum-trees:111 train-loss:8.046268 train-rmse:8.046268 valid-loss:8.329066 valid-rmse:8.329066\n",
            "[INFO 23-11-05 03:43:44.3789 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:8.031574 train-rmse:8.031574 valid-loss:8.323657 valid-rmse:8.323657\n",
            "[INFO 23-11-05 03:43:53.4017 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.278453 train-rmse:7.278453 valid-loss:8.082047 valid-rmse:8.082047\n",
            "[INFO 23-11-05 03:43:53.4017 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 03:43:53.4017 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.082047 valid-rmse:8.082047\n",
            "[INFO 23-11-05 03:43:53.4159 UTC gradient_boosted_trees.cc:[INFO 23-11-05 03:43:53.4169 UTC hyperparameters_optimizer.cc:582] [64/100] Score: -8.08205 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 03:43:53.4280 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 03:43:53.4525 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 03:44:15.1405 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:6.707744 train-rmse:6.707744 valid-loss:7.887125 valid-rmse:7.887125\n",
            "[INFO 23-11-05 03:44:24.5375 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.615744 train-rmse:8.615744 valid-loss:8.538966 valid-rmse:8.538966\n",
            "[INFO 23-11-05 03:44:45.9900 UTC gradient_boosted_trees.cc:1556] \tnum-trees:123 train-loss:8.016041 train-rmse:8.016041 valid-loss:8.321910 valid-rmse:8.321910\n",
            "[INFO 23-11-05 03:45:16.3665 UTC gradient_boosted_trees.cc:1556] \tnum-trees:230 train-loss:7.028876 train-rmse:7.028876 valid-loss:7.963521 valid-rmse:7.963521\n",
            "[INFO 23-11-05 03:45:46.4556 UTC gradient_boosted_trees.cc:1556] \tnum-trees:225 train-loss:6.335965 train-rmse:6.335965 valid-loss:7.874985 valid-rmse:7.874985\n",
            "[INFO 23-11-05 03:46:18.2561 UTC gradient_boosted_trees.cc:1556] \tnum-trees:135 train-loss:7.991024 train-rmse:7.991024 valid-loss:8.306404 valid-rmse:8.306404\n",
            "[INFO 23-11-05 03:46:49.0454 UTC gradient_boosted_trees.cc:1556] \tnum-trees:139 train-loss:7.978875 train-rmse:7.978875 valid-loss:8.304646 valid-rmse:8.304646\n",
            "[INFO 23-11-05 03:47:19.6927 UTC gradient_boosted_trees.cc:1556] \tnum-trees:143 train-loss:7.970795 train-rmse:7.970795 valid-loss:8.302361 valid-rmse:8.302361\n",
            "[INFO 23-11-05 03:47:50.4523 UTC gradient_boosted_trees.cc:1556] \tnum-trees:147 train-loss:7.965294 train-rmse:7.965294 valid-loss:8.299544 valid-rmse:8.299544\n",
            "[INFO 23-11-05 03:48:02.3816 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.273972 train-rmse:7.273972 valid-loss:8.035395 valid-rmse:8.035395\n",
            "[INFO 23-11-05 03:48:02.3816 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 03:48:02.3816 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.035395 valid-rmse:8.035395\n",
            "[INFO 23-11-05 03:48:02.3851 UTC hyperparameters_optimizer.cc:582] [65/100] Score: -8.03539 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 6 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 03:48:02.3862 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 03:48:02.3863 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 03:48:02.3994 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 03:48:02.9835 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.618778 train-rmse:8.618778 valid-loss:8.543349 valid-rmse:8.543349\n",
            "[INFO 23-11-05 03:48:20.5200 UTC gradient_boosted_trees.cc:1556] \tnum-trees:226 train-loss:7.472692 train-rmse:7.472692 valid-loss:8.138928 valid-rmse:8.138928\n",
            "[INFO 23-11-05 03:48:50.8773 UTC gradient_boosted_trees.cc:1556] \tnum-trees:109 train-loss:7.606035 train-rmse:7.606035 valid-loss:8.148092 valid-rmse:8.148092\n",
            "[INFO 23-11-05 03:49:21.1688 UTC gradient_boosted_trees.cc:1556] \tnum-trees:176 train-loss:7.309805 train-rmse:7.309805 valid-loss:8.032600 valid-rmse:8.032600\n",
            "[INFO 23-11-05 03:49:51.3548 UTC gradient_boosted_trees.cc:1556] \tnum-trees:245 train-loss:7.973917 train-rmse:7.973917 valid-loss:8.238099 valid-rmse:8.238099\n",
            "[INFO 23-11-05 03:50:17.2388 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.908404 train-rmse:6.908404 valid-loss:7.920594 valid-rmse:7.920594\n",
            "[INFO 23-11-05 03:50:17.2388 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 03:50:17.2388 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.920540 valid-rmse:7.920540\n",
            "[INFO 23-11-05 03:50:17.2444 UTC hyperparameters_optimizer.cc:582] [66/100] Score: -7.92054 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"AXIS_ALIGNED\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 256 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 03:50:17.2498 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 03:50:17.2498 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 03:50:17.2613 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 03:50:23.0175 UTC gradient_boosted_trees.cc:1556] \tnum-trees:249 train-loss:7.969754 train-rmse:7.969754 valid-loss:8.238292 valid-rmse:8.238292\n",
            "[INFO 23-11-05 03:50:29.6376 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.597229 train-rmse:8.597229 valid-loss:8.522758 valid-rmse:8.522758\n",
            "[INFO 23-11-05 03:50:53.7388 UTC gradient_boosted_trees.cc:1556] \tnum-trees:279 train-loss:6.005615 train-rmse:6.005615 valid-loss:7.812156 valid-rmse:7.812156\n",
            "[INFO 23-11-05 03:51:24.9730 UTC gradient_boosted_trees.cc:1556] \tnum-trees:175 train-loss:7.902590 train-rmse:7.902590 valid-loss:8.267121 valid-rmse:8.267121\n",
            "[INFO 23-11-05 03:51:55.6903 UTC gradient_boosted_trees.cc:1556] \tnum-trees:179 train-loss:7.894370 train-rmse:7.894370 valid-loss:8.268520 valid-rmse:8.268520\n",
            "[INFO 23-11-05 03:52:25.7618 UTC gradient_boosted_trees.cc:1556] \tnum-trees:199 train-loss:6.951756 train-rmse:6.951756 valid-loss:7.941172 valid-rmse:7.941172\n",
            "[INFO 23-11-05 03:52:57.1514 UTC gradient_boosted_trees.cc:1556] \tnum-trees:187 train-loss:7.878957 train-rmse:7.878957 valid-loss:8.264018 valid-rmse:8.264018\n",
            "[INFO 23-11-05 03:53:27.8837 UTC gradient_boosted_trees.cc:1556] \tnum-trees:15 train-loss:8.237962 train-rmse:8.237962 valid-loss:8.391924 valid-rmse:8.391924\n",
            "[INFO 23-11-05 03:53:58.8522 UTC gradient_boosted_trees.cc:1556] \tnum-trees:195 train-loss:7.863178 train-rmse:7.863178 valid-loss:8.253157 valid-rmse:8.253157\n",
            "[INFO 23-11-05 03:54:29.3292 UTC gradient_boosted_trees.cc:1556] \tnum-trees:199 train-loss:7.855713 train-rmse:7.855713 valid-loss:8.253125 valid-rmse:8.253125\n",
            "[INFO 23-11-05 03:54:59.6939 UTC gradient_boosted_trees.cc:1556] \tnum-trees:284 train-loss:7.927039 train-rmse:7.927039 valid-loss:8.212952 valid-rmse:8.212952\n",
            "[INFO 23-11-05 03:55:30.3695 UTC gradient_boosted_trees.cc:1556] \tnum-trees:207 train-loss:6.905200 train-rmse:6.905200 valid-loss:7.916410 valid-rmse:7.916410\n",
            "[INFO 23-11-05 03:56:00.5815 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:6.235151 train-rmse:6.235151 valid-loss:7.853544 valid-rmse:7.853544\n",
            "[INFO 23-11-05 03:56:32.4487 UTC gradient_boosted_trees.cc:1556] \tnum-trees:215 train-loss:7.822151 train-rmse:7.822151 valid-loss:8.245458 valid-rmse:8.245458\n",
            "[INFO 23-11-05 03:57:03.0448 UTC gradient_boosted_trees.cc:1556] \tnum-trees:219 train-loss:7.814737 train-rmse:7.814737 valid-loss:8.241901 valid-rmse:8.241901\n",
            "[INFO 23-11-05 03:57:04.5290 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.908959 train-rmse:7.908959 valid-loss:8.198901 valid-rmse:8.198901\n",
            "[INFO 23-11-05 03:57:04.5291 UTC gradient_boosted_trees.cc:249] Truncates the model to 297 tree(s) i.e. 297  iteration(s).\n",
            "[INFO 23-11-05 03:57:04.5291 UTC gradient_boosted_trees.cc:312] Final model num-trees:297 valid-loss:8.197421 valid-rmse:8.197421\n",
            "[INFO 23-11-05 03:57:04.5298 UTC hyperparameters_optimizer.cc:582] [67/100] Score: -8.19742 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 3 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 03:57:04.5309 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 03:57:04.5310 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 03:57:04.5425 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 03:57:33.7541 UTC gradient_boosted_trees.cc:1556] \tnum-trees:223 train-loss:7.809143 train-rmse:7.809143 valid-loss:8.239541 valid-rmse:8.239541\n",
            "[INFO 23-11-05 03:57:36.1436 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.635039 train-rmse:8.635039 valid-loss:8.551834 valid-rmse:8.551834\n",
            "[INFO 23-11-05 03:58:04.4493 UTC gradient_boosted_trees.cc:1556] \tnum-trees:227 train-loss:7.801462 train-rmse:7.801462 valid-loss:8.232422 valid-rmse:8.232422\n",
            "[INFO 23-11-05 03:58:06.6669 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.595241 train-rmse:6.595241 valid-loss:7.858105 valid-rmse:7.858105\n",
            "[INFO 23-11-05 03:58:06.6669 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 03:58:06.6669 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.858105 valid-rmse:7.858105\n",
            "[INFO 23-11-05 03:58:06.6713 UTC hyperparameters_optimizer.cc:582] [68/100] Score: -7.8581 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 03:58:06.6810 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 03:58:06.6810 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 03:58:06.6955 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 03:58:24.7443 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.889340 train-rmse:5.889340 valid-loss:7.797143 valid-rmse:7.797143\n",
            "[INFO 23-11-05 03:58:24.7443 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 03:58:24.7444 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.797143 valid-rmse:7.797143\n",
            "[INFO 23-11-05 03:58:24.7509 UTC hyperparameters_optimizer.cc:582] [69/100] Score: -7.79714 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 03:58:24.7520 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 03:58:24.7520 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 03:58:24.7720 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 03:58:26.1045 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.636594 train-rmse:8.636594 valid-loss:8.551984 valid-rmse:8.551984\n",
            "[INFO 23-11-05 03:58:33.1978 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.636082 train-rmse:8.636082 valid-loss:8.552082 valid-rmse:8.552082\n",
            "[INFO 23-11-05 03:58:35.5038 UTC gradient_boosted_trees.cc:1556] \tnum-trees:231 train-loss:7.794876 train-rmse:7.794876 valid-loss:8.232476 valid-rmse:8.232476\n",
            "[INFO 23-11-05 03:59:05.8709 UTC gradient_boosted_trees.cc:1556] \tnum-trees:235 train-loss:7.785603 train-rmse:7.785603 valid-loss:8.227678 valid-rmse:8.227678\n",
            "[INFO 23-11-05 03:59:36.2831 UTC gradient_boosted_trees.cc:1556] \tnum-trees:44 train-loss:7.962463 train-rmse:7.962463 valid-loss:8.288811 valid-rmse:8.288811\n",
            "[INFO 23-11-05 04:00:07.1398 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:7.767552 train-rmse:7.767552 valid-loss:8.225784 valid-rmse:8.225784\n",
            "[INFO 23-11-05 04:00:37.6288 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:7.761311 train-rmse:7.761311 valid-loss:8.223845 valid-rmse:8.223845\n",
            "[INFO 23-11-05 04:01:07.8307 UTC gradient_boosted_trees.cc:1556] \tnum-trees:252 train-loss:6.110325 train-rmse:6.110325 valid-loss:7.798903 valid-rmse:7.798903\n",
            "[INFO 23-11-05 04:01:39.0519 UTC gradient_boosted_trees.cc:1556] \tnum-trees:255 train-loss:7.744416 train-rmse:7.744416 valid-loss:8.211913 valid-rmse:8.211913\n",
            "[INFO 23-11-05 04:02:09.8112 UTC gradient_boosted_trees.cc:1556] \tnum-trees:259 train-loss:7.738759 train-rmse:7.738759 valid-loss:8.213010 valid-rmse:8.213010\n",
            "[INFO 23-11-05 04:02:40.7437 UTC gradient_boosted_trees.cc:1556] \tnum-trees:263 train-loss:7.732044 train-rmse:7.732044 valid-loss:8.202120 valid-rmse:8.202120\n",
            "[INFO 23-11-05 04:03:11.5036 UTC gradient_boosted_trees.cc:1556] \tnum-trees:267 train-loss:7.723917 train-rmse:7.723917 valid-loss:8.204261 valid-rmse:8.204261\n",
            "[INFO 23-11-05 04:03:42.2621 UTC gradient_boosted_trees.cc:1556] \tnum-trees:271 train-loss:7.718470 train-rmse:7.718470 valid-loss:8.207107 valid-rmse:8.207107\n",
            "[INFO 23-11-05 04:04:13.1531 UTC gradient_boosted_trees.cc:1556] \tnum-trees:275 train-loss:7.711260 train-rmse:7.711260 valid-loss:8.202671 valid-rmse:8.202671\n",
            "[INFO 23-11-05 04:04:43.6787 UTC gradient_boosted_trees.cc:1556] \tnum-trees:279 train-loss:7.703935 train-rmse:7.703935 valid-loss:8.204008 valid-rmse:8.204008\n",
            "[INFO 23-11-05 04:05:14.3930 UTC gradient_boosted_trees.cc:1556] \tnum-trees:283 train-loss:7.697524 train-rmse:7.697524 valid-loss:8.203802 valid-rmse:8.203802\n",
            "[INFO 23-11-05 04:05:45.1975 UTC gradient_boosted_trees.cc:1556] \tnum-trees:287 train-loss:7.690354 train-rmse:7.690354 valid-loss:8.200913 valid-rmse:8.200913\n",
            "[INFO 23-11-05 04:06:15.7400 UTC gradient_boosted_trees.cc:1556] \tnum-trees:267 train-loss:6.142221 train-rmse:6.142221 valid-loss:7.829048 valid-rmse:7.829048\n",
            "[INFO 23-11-05 04:06:47.2578 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:7.676139 train-rmse:7.676139 valid-loss:8.195068 valid-rmse:8.195068\n",
            "[INFO 23-11-05 04:07:18.0020 UTC gradient_boosted_trees.cc:1556] \tnum-trees:80 train-loss:7.716121 train-rmse:7.716121 valid-loss:8.228257 valid-rmse:8.228257\n",
            "[INFO 23-11-05 04:07:26.0810 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.667287 train-rmse:7.667287 valid-loss:8.190551 valid-rmse:8.190551\n",
            "[INFO 23-11-05 04:07:26.0810 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 04:07:26.0815 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.190551 valid-rmse:8.190551\n",
            "[INFO 23-11-05 04:07:26.0844 UTC hyperparameters_optimizer.cc:582] [70/100] Score: -8.19055 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 04:07:26.0864 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 04:07:26.0865 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 04:07:26.0988 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 04:07:37.9532 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.642549 train-rmse:8.642549 valid-loss:8.553695 valid-rmse:8.553695\n",
            "[INFO 23-11-05 04:07:49.6544 UTC gradient_boosted_trees.cc:1556] \tnum-trees:277 train-loss:6.866130 train-rmse:6.866130 valid-loss:7.918931 valid-rmse:7.918931\n",
            "[INFO 23-11-05 04:08:21.0547 UTC gradient_boosted_trees.cc:1556] \tnum-trees:85 train-loss:7.678306 train-rmse:7.678306 valid-loss:8.205270 valid-rmse:8.205270\n",
            "[INFO 23-11-05 04:08:52.5392 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:8.251011 train-rmse:8.251011 valid-loss:8.371780 valid-rmse:8.371780\n",
            "[INFO 23-11-05 04:09:24.1135 UTC gradient_boosted_trees.cc:1556] \tnum-trees:90 train-loss:7.644923 train-rmse:7.644923 valid-loss:8.207380 valid-rmse:8.207380\n",
            "[INFO 23-11-05 04:09:55.6576 UTC gradient_boosted_trees.cc:1556] \tnum-trees:276 train-loss:7.345595 train-rmse:7.345595 valid-loss:8.103143 valid-rmse:8.103143\n",
            "[INFO 23-11-05 04:10:26.1670 UTC gradient_boosted_trees.cc:1556] \tnum-trees:252 train-loss:7.415087 train-rmse:7.415087 valid-loss:8.120992 valid-rmse:8.120992\n",
            "[INFO 23-11-05 04:10:58.1108 UTC gradient_boosted_trees.cc:1556] \tnum-trees:90 train-loss:8.215825 train-rmse:8.215825 valid-loss:8.357682 valid-rmse:8.357682\n",
            "[INFO 23-11-05 04:11:30.8299 UTC gradient_boosted_trees.cc:1556] \tnum-trees:100 train-loss:7.578926 train-rmse:7.578926 valid-loss:8.206327 valid-rmse:8.206327\n",
            "[INFO 23-11-05 04:12:03.4856 UTC gradient_boosted_trees.cc:1556] \tnum-trees:281 train-loss:7.332375 train-rmse:7.332375 valid-loss:8.100014 valid-rmse:8.100014\n",
            "[INFO 23-11-05 04:12:34.1797 UTC gradient_boosted_trees.cc:1556] \tnum-trees:26 train-loss:8.465054 train-rmse:8.465054 valid-loss:8.486308 valid-rmse:8.486308\n",
            "[INFO 23-11-05 04:13:04.5298 UTC gradient_boosted_trees.cc:1556] \tnum-trees:288 train-loss:6.822566 train-rmse:6.822566 valid-loss:7.905051 valid-rmse:7.905051\n",
            "[INFO 23-11-05 04:13:36.3647 UTC gradient_boosted_trees.cc:1556] \tnum-trees:109 train-loss:8.184383 train-rmse:8.184383 valid-loss:8.342113 valid-rmse:8.342113\n",
            "[INFO 23-11-05 04:14:08.3895 UTC gradient_boosted_trees.cc:1556] \tnum-trees:49 train-loss:8.168237 train-rmse:8.168237 valid-loss:8.384632 valid-rmse:8.384632\n",
            "[INFO 23-11-05 04:14:38.8197 UTC gradient_boosted_trees.cc:1556] \tnum-trees:287 train-loss:7.319765 train-rmse:7.319765 valid-loss:8.096514 valid-rmse:8.096514\n",
            "[INFO 23-11-05 04:15:13.5860 UTC gradient_boosted_trees.cc:1556] \tnum-trees:262 train-loss:7.393799 train-rmse:7.393799 valid-loss:8.112876 valid-rmse:8.112876\n",
            "[INFO 23-11-05 04:15:43.9361 UTC gradient_boosted_trees.cc:1556] \tnum-trees:42 train-loss:8.394547 train-rmse:8.394547 valid-loss:8.463046 valid-rmse:8.463046\n",
            "[INFO 23-11-05 04:16:15.5470 UTC gradient_boosted_trees.cc:1556] \tnum-trees:128 train-loss:8.153191 train-rmse:8.153191 valid-loss:8.331952 valid-rmse:8.331952\n",
            "[INFO 23-11-05 04:16:46.0548 UTC gradient_boosted_trees.cc:1556] \tnum-trees:57 train-loss:8.126718 train-rmse:8.126718 valid-loss:8.374616 valid-rmse:8.374616\n",
            "[INFO 23-11-05 04:17:16.6610 UTC gradient_boosted_trees.cc:1556] \tnum-trees:63 train-loss:7.830633 train-rmse:7.830633 valid-loss:8.222013 valid-rmse:8.222013\n",
            "[INFO 23-11-05 04:17:47.4603 UTC gradient_boosted_trees.cc:1556] \tnum-trees:139 train-loss:8.138908 train-rmse:8.138908 valid-loss:8.323985 valid-rmse:8.323985\n",
            "[INFO 23-11-05 04:18:18.0070 UTC gradient_boosted_trees.cc:1556] \tnum-trees:55 train-loss:8.354169 train-rmse:8.354169 valid-loss:8.444982 valid-rmse:8.444982\n",
            "[INFO 23-11-05 04:18:49.5484 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.783607 train-rmse:6.783607 valid-loss:7.903327 valid-rmse:7.903327\n",
            "[INFO 23-11-05 04:18:49.5484 UTC gradient_boosted_trees.cc:249] Truncates the model to 298 tree(s) i.e. 298  iteration(s).\n",
            "[INFO 23-11-05 04:18:49.5485 UTC gradient_boosted_trees.cc:312] Final model num-trees:298 valid-loss:7.903126 valid-rmse:7.903126\n",
            "[INFO 23-11-05 04:18:49.5544 UTC hyperparameters_optimizer.cc:582] [71/100] Score: -7.90313 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 128 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 04:18:49.5615 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 04:18:49.5620 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 04:18:49.5738 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 04:18:49.8729 UTC gradient_boosted_trees.cc:1556] \tnum-trees:41 train-loss:8.185384 train-rmse:8.185384 valid-loss:8.376716 valid-rmse:8.376716\n",
            "[INFO 23-11-05 04:19:20.4083 UTC gradient_boosted_trees.cc:1556] \tnum-trees:137 train-loss:7.392683 train-rmse:7.392683 valid-loss:8.176207 valid-rmse:8.176207\n",
            "[INFO 23-11-05 04:19:20.7754 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.602795 train-rmse:8.602795 valid-loss:8.536078 valid-rmse:8.536078\n",
            "[INFO 23-11-05 04:19:51.2849 UTC gradient_boosted_trees.cc:1556] \tnum-trees:299 train-loss:7.288254 train-rmse:7.288254 valid-loss:8.086407 valid-rmse:8.086407\n",
            "[INFO 23-11-05 04:20:17.5540 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.287276 train-rmse:7.287276 valid-loss:8.086233 valid-rmse:8.086233\n",
            "[INFO 23-11-05 04:20:17.5540 UTC gradient_boosted_trees.cc:249] Truncates the model to 298 tree(s) i.e. 298  iteration(s).\n",
            "[INFO 23-11-05 04:20:17.5540 UTC gradient_boosted_trees.cc:312] Final model num-trees:298 valid-loss:8.086205 valid-rmse:8.086205\n",
            "[INFO 23-11-05 04:20:17.5601 UTC hyperparameters_optimizer.cc:582] [72/100] Score: -8.08621 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 04:20:17.5607 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 04:20:17.5607 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 04:20:17.5737 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 04:20:22.4027 UTC gradient_boosted_trees.cc:1556] \tnum-trees:68 train-loss:8.073905 train-rmse:8.073905 valid-loss:8.358056 valid-rmse:8.358056\n",
            "[INFO 23-11-05 04:20:37.3058 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.639019 train-rmse:8.639019 valid-loss:8.552977 valid-rmse:8.552977\n",
            "[INFO 23-11-05 04:20:52.7498 UTC gradient_boosted_trees.cc:1556] \tnum-trees:68 train-loss:8.319636 train-rmse:8.319636 valid-loss:8.431987 valid-rmse:8.431987\n",
            "[INFO 23-11-05 04:21:24.9489 UTC gradient_boosted_trees.cc:1556] \tnum-trees:165 train-loss:8.097295 train-rmse:8.097295 valid-loss:8.311318 valid-rmse:8.311318\n",
            "[INFO 23-11-05 04:21:58.2182 UTC gradient_boosted_trees.cc:1556] \tnum-trees:5 train-loss:8.583488 train-rmse:8.583488 valid-loss:8.531100 valid-rmse:8.531100\n",
            "[INFO 23-11-05 04:22:20.2646 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.988678 train-rmse:5.988678 valid-loss:7.800047 valid-rmse:7.800047\n",
            "[INFO 23-11-05 04:22:20.2646 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 04:22:20.2646 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.800047 valid-rmse:7.800047\n",
            "[INFO 23-11-05 04:22:20.2776 UTC hyperparameters_optimizer.cc:582] [73/100] Score: -7.80005 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 04:22:20.2785 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 04:22:20.2785 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 04:22:20.3021 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 04:22:29.1189 UTC gradient_boosted_trees.cc:1556] \tnum-trees:152 train-loss:7.308115 train-rmse:7.308115 valid-loss:8.159180 valid-rmse:8.159180\n",
            "[INFO 23-11-05 04:22:44.3911 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.619422 train-rmse:8.619422 valid-loss:8.538110 valid-rmse:8.538110\n",
            "[INFO 23-11-05 04:23:00.3896 UTC gradient_boosted_trees.cc:1556] \tnum-trees:76 train-loss:8.037562 train-rmse:8.037562 valid-loss:8.349321 valid-rmse:8.349321\n",
            "[INFO 23-11-05 04:23:30.7006 UTC gradient_boosted_trees.cc:1556] \tnum-trees:180 train-loss:8.077948 train-rmse:8.077948 valid-loss:8.295828 valid-rmse:8.295828\n",
            "[INFO 23-11-05 04:24:02.6366 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:8.283350 train-rmse:8.283350 valid-loss:8.418960 valid-rmse:8.418960\n",
            "[INFO 23-11-05 04:24:35.3877 UTC gradient_boosted_trees.cc:1556] \tnum-trees:162 train-loss:7.251245 train-rmse:7.251245 valid-loss:8.147366 valid-rmse:8.147366\n",
            "[INFO 23-11-05 04:25:08.2877 UTC gradient_boosted_trees.cc:1556] \tnum-trees:12 train-loss:8.194048 train-rmse:8.194048 valid-loss:8.370860 valid-rmse:8.370860\n",
            "[INFO 23-11-05 04:25:38.7189 UTC gradient_boosted_trees.cc:1556] \tnum-trees:167 train-loss:7.232220 train-rmse:7.232220 valid-loss:8.149393 valid-rmse:8.149393\n",
            "[INFO 23-11-05 04:26:09.8829 UTC gradient_boosted_trees.cc:1556] \tnum-trees:199 train-loss:8.051991 train-rmse:8.051991 valid-loss:8.283985 valid-rmse:8.283985\n",
            "[INFO 23-11-05 04:26:33.1224 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.840589 train-rmse:5.840589 valid-loss:7.757743 valid-rmse:7.757743\n",
            "[INFO 23-11-05 04:26:33.1224 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 04:26:33.1224 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.757743 valid-rmse:7.757743\n",
            "[INFO 23-11-05 04:26:33.1372 UTC hyperparameters_optimizer.cc:582] [74/100] Score: -7.75774 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 04:26:33.1572 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 04:26:33.1572 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 04:26:33.1725 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 04:26:40.1097 UTC gradient_boosted_trees.cc:1556] \tnum-trees:286 train-loss:7.339316 train-rmse:7.339316 valid-loss:8.095842 valid-rmse:8.095842\n",
            "[INFO 23-11-05 04:26:51.2111 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.621139 train-rmse:8.621139 valid-loss:8.535657 valid-rmse:8.535657\n",
            "[INFO 23-11-05 04:27:11.3340 UTC gradient_boosted_trees.cc:1556] \tnum-trees:12 train-loss:8.332561 train-rmse:8.332561 valid-loss:8.438116 valid-rmse:8.438116\n",
            "[INFO 23-11-05 04:27:42.0348 UTC gradient_boosted_trees.cc:1556] \tnum-trees:210 train-loss:8.039686 train-rmse:8.039686 valid-loss:8.278999 valid-rmse:8.278999\n",
            "[INFO 23-11-05 04:28:15.7409 UTC gradient_boosted_trees.cc:1556] \tnum-trees:214 train-loss:8.035297 train-rmse:8.035297 valid-loss:8.278427 valid-rmse:8.278427\n",
            "[INFO 23-11-05 04:28:47.8876 UTC gradient_boosted_trees.cc:1556] \tnum-trees:293 train-loss:6.525903 train-rmse:6.525903 valid-loss:7.806489 valid-rmse:7.806489\n",
            "[INFO 23-11-05 04:29:18.0546 UTC gradient_boosted_trees.cc:1556] \tnum-trees:9 train-loss:8.412663 train-rmse:8.412663 valid-loss:8.439400 valid-rmse:8.439400\n",
            "[INFO 23-11-05 04:29:48.2356 UTC gradient_boosted_trees.cc:1556] \tnum-trees:225 train-loss:8.022780 train-rmse:8.022780 valid-loss:8.271407 valid-rmse:8.271407\n",
            "[INFO 23-11-05 04:30:18.5030 UTC gradient_boosted_trees.cc:1556] \tnum-trees:189 train-loss:7.110030 train-rmse:7.110030 valid-loss:8.121286 valid-rmse:8.121286\n",
            "[INFO 23-11-05 04:30:50.2650 UTC gradient_boosted_trees.cc:1556] \tnum-trees:14 train-loss:8.324445 train-rmse:8.324445 valid-loss:8.401989 valid-rmse:8.401989\n",
            "[INFO 23-11-05 04:31:20.6021 UTC gradient_boosted_trees.cc:1556] \tnum-trees:236 train-loss:8.011171 train-rmse:8.011171 valid-loss:8.268706 valid-rmse:8.268706\n",
            "[INFO 23-11-05 04:31:29.2360 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.491542 train-rmse:6.491542 valid-loss:7.800869 valid-rmse:7.800869\n",
            "[INFO 23-11-05 04:31:29.2361 UTC gradient_boosted_trees.cc:249] Truncates the model to 297 tree(s) i.e. 297  iteration(s).\n",
            "[INFO 23-11-05 04:31:29.2361 UTC gradient_boosted_trees.cc:312] Final model num-trees:297 valid-loss:7.800326 valid-rmse:7.800326\n",
            "[INFO 23-11-05 04:31:29.2449 UTC hyperparameters_optimizer.cc:582] [75/100] Score: -7.80033 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 256 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 04:31:29.2478 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 04:31:29.2482 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 04:31:29.2621 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 04:31:39.2675 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.628669 train-rmse:8.628669 valid-loss:8.547915 valid-rmse:8.547915\n",
            "[INFO 23-11-05 04:31:54.4882 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:8.007419 train-rmse:8.007419 valid-loss:8.266406 valid-rmse:8.266406\n",
            "[INFO 23-11-05 04:32:26.5109 UTC gradient_boosted_trees.cc:1556] \tnum-trees:199 train-loss:7.057849 train-rmse:7.057849 valid-loss:8.102633 valid-rmse:8.102633\n",
            "[INFO 23-11-05 04:32:56.8897 UTC gradient_boosted_trees.cc:1556] \tnum-trees:299 train-loss:7.312989 train-rmse:7.312989 valid-loss:8.085131 valid-rmse:8.085131\n",
            "[INFO 23-11-05 04:33:25.8602 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.309014 train-rmse:7.309014 valid-loss:8.083887 valid-rmse:8.083887\n",
            "[INFO 23-11-05 04:33:25.8602 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 04:33:25.8603 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.083887 valid-rmse:8.083887\n",
            "[INFO 23-11-05 04:33:25.8716 UTC hyperparameters_optimizer.cc:582] [76/100] Score: -8.08389 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 04:33:25.8729 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 04:33:25.8729 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 04:33:25.8906 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 04:33:27.0543 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:7.994766 train-rmse:7.994766 valid-loss:8.262659 valid-rmse:8.262659\n",
            "[INFO 23-11-05 04:33:49.2809 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.640060 train-rmse:8.640060 valid-loss:8.550422 valid-rmse:8.550422\n",
            "[INFO 23-11-05 04:33:58.6646 UTC gradient_boosted_trees.cc:1556] \tnum-trees:41 train-loss:8.248937 train-rmse:8.248937 valid-loss:8.389582 valid-rmse:8.389582\n",
            "[INFO 23-11-05 04:34:30.6548 UTC gradient_boosted_trees.cc:1556] \tnum-trees:137 train-loss:8.187725 train-rmse:8.187725 valid-loss:8.370054 valid-rmse:8.370054\n",
            "[INFO 23-11-05 04:35:00.8530 UTC gradient_boosted_trees.cc:1556] \tnum-trees:21 train-loss:8.368642 train-rmse:8.368642 valid-loss:8.439322 valid-rmse:8.439322\n",
            "[INFO 23-11-05 04:35:31.0562 UTC gradient_boosted_trees.cc:1556] \tnum-trees:24 train-loss:8.347152 train-rmse:8.347152 valid-loss:8.431984 valid-rmse:8.431984\n",
            "[INFO 23-11-05 04:36:01.1103 UTC gradient_boosted_trees.cc:1556] \tnum-trees:27 train-loss:8.326619 train-rmse:8.326619 valid-loss:8.425115 valid-rmse:8.425115\n",
            "[INFO 23-11-05 04:36:31.5268 UTC gradient_boosted_trees.cc:1556] \tnum-trees:30 train-loss:8.310373 train-rmse:8.310373 valid-loss:8.406947 valid-rmse:8.406947\n",
            "[INFO 23-11-05 04:37:02.2467 UTC gradient_boosted_trees.cc:1556] \tnum-trees:33 train-loss:8.293715 train-rmse:8.293715 valid-loss:8.407046 valid-rmse:8.407046\n",
            "[INFO 23-11-05 04:37:32.7851 UTC gradient_boosted_trees.cc:1556] \tnum-trees:36 train-loss:8.281394 train-rmse:8.281394 valid-loss:8.407609 valid-rmse:8.407609\n",
            "[INFO 23-11-05 04:38:03.1127 UTC gradient_boosted_trees.cc:1556] \tnum-trees:39 train-loss:8.266990 train-rmse:8.266990 valid-loss:8.403328 valid-rmse:8.403328\n",
            "[INFO 23-11-05 04:38:33.2740 UTC gradient_boosted_trees.cc:1556] \tnum-trees:42 train-loss:8.253440 train-rmse:8.253440 valid-loss:8.397055 valid-rmse:8.397055\n",
            "[INFO 23-11-05 04:39:03.4626 UTC gradient_boosted_trees.cc:1556] \tnum-trees:45 train-loss:8.239535 train-rmse:8.239535 valid-loss:8.386823 valid-rmse:8.386823\n",
            "[INFO 23-11-05 04:39:33.6397 UTC gradient_boosted_trees.cc:1556] \tnum-trees:48 train-loss:8.227941 train-rmse:8.227941 valid-loss:8.378204 valid-rmse:8.378204\n",
            "[INFO 23-11-05 04:40:03.9530 UTC gradient_boosted_trees.cc:1556] \tnum-trees:51 train-loss:8.216991 train-rmse:8.216991 valid-loss:8.367885 valid-rmse:8.367885\n",
            "[INFO 23-11-05 04:40:17.2368 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.945391 train-rmse:7.945391 valid-loss:8.245144 valid-rmse:8.245144\n",
            "[INFO 23-11-05 04:40:17.2368 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 04:40:17.2368 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:8.244384 valid-rmse:8.244384\n",
            "[INFO 23-11-05 04:40:17.2389 UTC hyperparameters_optimizer.cc:582] [77/100] Score: -8.24438 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 3 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 04:40:17.2429 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 04:40:17.2429 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 04:40:17.2686 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 04:40:34.1803 UTC gradient_boosted_trees.cc:1556] \tnum-trees:54 train-loss:8.203562 train-rmse:8.203562 valid-loss:8.365088 valid-rmse:8.365088\n",
            "[INFO 23-11-05 04:40:49.1446 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.632794 train-rmse:8.632794 valid-loss:8.548349 valid-rmse:8.548349\n",
            "[INFO 23-11-05 04:41:04.4343 UTC gradient_boosted_trees.cc:1556] \tnum-trees:57 train-loss:8.193521 train-rmse:8.193521 valid-loss:8.356068 valid-rmse:8.356068\n",
            "[INFO 23-11-05 04:41:34.6983 UTC gradient_boosted_trees.cc:1556] \tnum-trees:60 train-loss:8.185295 train-rmse:8.185295 valid-loss:8.346970 valid-rmse:8.346970\n",
            "[INFO 23-11-05 04:42:05.0452 UTC gradient_boosted_trees.cc:1556] \tnum-trees:63 train-loss:8.175505 train-rmse:8.175505 valid-loss:8.342018 valid-rmse:8.342018\n",
            "[INFO 23-11-05 04:42:35.1632 UTC gradient_boosted_trees.cc:1556] \tnum-trees:178 train-loss:8.131401 train-rmse:8.131401 valid-loss:8.341573 valid-rmse:8.341573\n",
            "[INFO 23-11-05 04:43:05.7803 UTC gradient_boosted_trees.cc:1556] \tnum-trees:69 train-loss:8.155207 train-rmse:8.155207 valid-loss:8.334830 valid-rmse:8.334830\n",
            "[INFO 23-11-05 04:43:36.3928 UTC gradient_boosted_trees.cc:1556] \tnum-trees:72 train-loss:8.145961 train-rmse:8.145961 valid-loss:8.332950 valid-rmse:8.332950\n",
            "[INFO 23-11-05 04:44:06.6180 UTC gradient_boosted_trees.cc:1556] \tnum-trees:75 train-loss:8.136392 train-rmse:8.136392 valid-loss:8.329196 valid-rmse:8.329196\n",
            "[INFO 23-11-05 04:44:37.3971 UTC gradient_boosted_trees.cc:1556] \tnum-trees:78 train-loss:8.129439 train-rmse:8.129439 valid-loss:8.329854 valid-rmse:8.329854\n",
            "[INFO 23-11-05 04:45:07.6661 UTC gradient_boosted_trees.cc:1556] \tnum-trees:81 train-loss:8.115266 train-rmse:8.115266 valid-loss:8.322892 valid-rmse:8.322892\n",
            "[INFO 23-11-05 04:45:38.6057 UTC gradient_boosted_trees.cc:1556] \tnum-trees:84 train-loss:8.107924 train-rmse:8.107924 valid-loss:8.314688 valid-rmse:8.314688\n",
            "[INFO 23-11-05 04:46:09.3878 UTC gradient_boosted_trees.cc:1556] \tnum-trees:87 train-loss:8.096578 train-rmse:8.096578 valid-loss:8.312757 valid-rmse:8.312757\n",
            "[INFO 23-11-05 04:46:39.8656 UTC gradient_boosted_trees.cc:1556] \tnum-trees:266 train-loss:6.773544 train-rmse:6.773544 valid-loss:8.044030 valid-rmse:8.044030\n",
            "[INFO 23-11-05 04:47:10.3780 UTC gradient_boosted_trees.cc:1556] \tnum-trees:35 train-loss:8.321337 train-rmse:8.321337 valid-loss:8.414076 valid-rmse:8.414076\n",
            "[INFO 23-11-05 04:47:41.4320 UTC gradient_boosted_trees.cc:1556] \tnum-trees:68 train-loss:7.882493 train-rmse:7.882493 valid-loss:8.282520 valid-rmse:8.282520\n",
            "[INFO 23-11-05 04:48:12.0157 UTC gradient_boosted_trees.cc:1556] \tnum-trees:99 train-loss:8.058686 train-rmse:8.058686 valid-loss:8.307648 valid-rmse:8.307648\n",
            "[INFO 23-11-05 04:48:43.0279 UTC gradient_boosted_trees.cc:1556] \tnum-trees:102 train-loss:8.051871 train-rmse:8.051871 valid-loss:8.301942 valid-rmse:8.301942\n",
            "[INFO 23-11-05 04:49:13.7083 UTC gradient_boosted_trees.cc:1556] \tnum-trees:105 train-loss:8.043040 train-rmse:8.043040 valid-loss:8.295472 valid-rmse:8.295472\n",
            "[INFO 23-11-05 04:49:44.5943 UTC gradient_boosted_trees.cc:1556] \tnum-trees:108 train-loss:8.034643 train-rmse:8.034643 valid-loss:8.288952 valid-rmse:8.288952\n",
            "[INFO 23-11-05 04:50:14.6285 UTC gradient_boosted_trees.cc:1556] \tnum-trees:217 train-loss:8.080861 train-rmse:8.080861 valid-loss:8.316429 valid-rmse:8.316429\n",
            "[INFO 23-11-05 04:50:45.3945 UTC gradient_boosted_trees.cc:1556] \tnum-trees:91 train-loss:8.009747 train-rmse:8.009747 valid-loss:8.328170 valid-rmse:8.328170\n",
            "[INFO 23-11-05 04:51:16.5779 UTC gradient_boosted_trees.cc:1556] \tnum-trees:117 train-loss:8.007450 train-rmse:8.007450 valid-loss:8.277289 valid-rmse:8.277289\n",
            "[INFO 23-11-05 04:51:47.4054 UTC gradient_boosted_trees.cc:1556] \tnum-trees:94 train-loss:7.999821 train-rmse:7.999821 valid-loss:8.324644 valid-rmse:8.324644\n",
            "[INFO 23-11-05 04:52:18.1529 UTC gradient_boosted_trees.cc:1556] \tnum-trees:123 train-loss:7.993279 train-rmse:7.993279 valid-loss:8.273487 valid-rmse:8.273487\n",
            "[INFO 23-11-05 04:52:49.1226 UTC gradient_boosted_trees.cc:1556] \tnum-trees:126 train-loss:7.985976 train-rmse:7.985976 valid-loss:8.271540 valid-rmse:8.271540\n",
            "[INFO 23-11-05 04:53:20.1189 UTC gradient_boosted_trees.cc:1556] \tnum-trees:129 train-loss:7.973997 train-rmse:7.973997 valid-loss:8.268894 valid-rmse:8.268894\n",
            "[INFO 23-11-05 04:53:49.3822 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.640050 train-rmse:6.640050 valid-loss:8.010052 valid-rmse:8.010052\n",
            "[INFO 23-11-05 04:53:49.3823 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 04:53:49.3823 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:8.008121 valid-rmse:8.008121\n",
            "[INFO 23-11-05 04:53:49.3874 UTC hyperparameters_optimizer.cc:582] [78/100] Score: -8.00812 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 6 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 04:53:49.3908 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 04:53:49.3909 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 04:53:49.4072 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 04:53:50.1788 UTC gradient_boosted_trees.cc:1556] \tnum-trees:100 train-loss:7.978428 train-rmse:7.978428 valid-loss:8.312410 valid-rmse:8.312410\n",
            "[INFO 23-11-05 04:54:02.2172 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.644937 train-rmse:8.644937 valid-loss:8.554485 valid-rmse:8.554485\n",
            "[INFO 23-11-05 04:54:21.5389 UTC gradient_boosted_trees.cc:1556] \tnum-trees:238 train-loss:8.055408 train-rmse:8.055408 valid-loss:8.305500 valid-rmse:8.305500\n",
            "[INFO 23-11-05 04:54:52.4965 UTC gradient_boosted_trees.cc:1556] \tnum-trees:138 train-loss:7.954684 train-rmse:7.954684 valid-loss:8.260788 valid-rmse:8.260788\n",
            "[INFO 23-11-05 04:55:23.2319 UTC gradient_boosted_trees.cc:1556] \tnum-trees:141 train-loss:7.946732 train-rmse:7.946732 valid-loss:8.252468 valid-rmse:8.252468\n",
            "[INFO 23-11-05 04:55:54.2257 UTC gradient_boosted_trees.cc:1556] \tnum-trees:144 train-loss:7.942360 train-rmse:7.942360 valid-loss:8.251024 valid-rmse:8.251024\n",
            "[INFO 23-11-05 04:56:24.9971 UTC gradient_boosted_trees.cc:1556] \tnum-trees:147 train-loss:7.936647 train-rmse:7.936647 valid-loss:8.250246 valid-rmse:8.250246\n",
            "[INFO 23-11-05 04:56:55.7988 UTC gradient_boosted_trees.cc:1556] \tnum-trees:150 train-loss:7.928556 train-rmse:7.928556 valid-loss:8.247456 valid-rmse:8.247456\n",
            "[INFO 23-11-05 04:57:25.9017 UTC gradient_boosted_trees.cc:1556] \tnum-trees:17 train-loss:8.537920 train-rmse:8.537920 valid-loss:8.499791 valid-rmse:8.499791\n",
            "[INFO 23-11-05 04:57:56.1071 UTC gradient_boosted_trees.cc:1556] \tnum-trees:181 train-loss:7.644664 train-rmse:7.644664 valid-loss:8.208378 valid-rmse:8.208378\n",
            "[INFO 23-11-05 04:58:27.9521 UTC gradient_boosted_trees.cc:1556] \tnum-trees:159 train-loss:7.904754 train-rmse:7.904754 valid-loss:8.241309 valid-rmse:8.241309\n",
            "[INFO 23-11-05 04:58:58.4243 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:7.918427 train-rmse:7.918427 valid-loss:8.297657 valid-rmse:8.297657\n",
            "[INFO 23-11-05 04:59:29.1655 UTC gradient_boosted_trees.cc:1556] \tnum-trees:165 train-loss:7.890939 train-rmse:7.890939 valid-loss:8.237524 valid-rmse:8.237524\n",
            "[INFO 23-11-05 04:59:59.9924 UTC gradient_boosted_trees.cc:1556] \tnum-trees:118 train-loss:7.908582 train-rmse:7.908582 valid-loss:8.291776 valid-rmse:8.291776\n",
            "[INFO 23-11-05 05:00:30.9794 UTC gradient_boosted_trees.cc:1556] \tnum-trees:171 train-loss:7.879138 train-rmse:7.879138 valid-loss:8.224030 valid-rmse:8.224030\n",
            "[INFO 23-11-05 05:01:01.2021 UTC gradient_boosted_trees.cc:1556] \tnum-trees:121 train-loss:7.898190 train-rmse:7.898190 valid-loss:8.287605 valid-rmse:8.287605\n",
            "[INFO 23-11-05 05:01:32.2836 UTC gradient_boosted_trees.cc:1556] \tnum-trees:177 train-loss:7.863897 train-rmse:7.863897 valid-loss:8.215183 valid-rmse:8.215183\n",
            "[INFO 23-11-05 05:02:02.7562 UTC gradient_boosted_trees.cc:1556] \tnum-trees:124 train-loss:7.889816 train-rmse:7.889816 valid-loss:8.283424 valid-rmse:8.283424\n",
            "[INFO 23-11-05 05:02:33.2317 UTC gradient_boosted_trees.cc:1556] \tnum-trees:195 train-loss:7.602561 train-rmse:7.602561 valid-loss:8.195258 valid-rmse:8.195258\n",
            "[INFO 23-11-05 05:03:03.7878 UTC gradient_boosted_trees.cc:1556] \tnum-trees:186 train-loss:7.842763 train-rmse:7.842763 valid-loss:8.207864 valid-rmse:8.207864\n",
            "[INFO 23-11-05 05:03:33.9598 UTC gradient_boosted_trees.cc:1556] \tnum-trees:189 train-loss:7.835293 train-rmse:7.835293 valid-loss:8.205376 valid-rmse:8.205376\n",
            "[INFO 23-11-05 05:04:04.5005 UTC gradient_boosted_trees.cc:1556] \tnum-trees:130 train-loss:7.868317 train-rmse:7.868317 valid-loss:8.274394 valid-rmse:8.274394\n",
            "[INFO 23-11-05 05:04:34.9680 UTC gradient_boosted_trees.cc:1556] \tnum-trees:195 train-loss:7.827394 train-rmse:7.827394 valid-loss:8.202370 valid-rmse:8.202370\n",
            "[INFO 23-11-05 05:05:05.4501 UTC gradient_boosted_trees.cc:1556] \tnum-trees:198 train-loss:7.819898 train-rmse:7.819898 valid-loss:8.196175 valid-rmse:8.196175\n",
            "[INFO 23-11-05 05:05:36.0500 UTC gradient_boosted_trees.cc:1556] \tnum-trees:201 train-loss:7.815053 train-rmse:7.815053 valid-loss:8.195352 valid-rmse:8.195352\n",
            "[INFO 23-11-05 05:06:06.6527 UTC gradient_boosted_trees.cc:1556] \tnum-trees:204 train-loss:7.808498 train-rmse:7.808498 valid-loss:8.192674 valid-rmse:8.192674\n",
            "[INFO 23-11-05 05:06:32.0557 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.989188 train-rmse:7.989188 valid-loss:8.274125 valid-rmse:8.274125\n",
            "[INFO 23-11-05 05:06:32.0557 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 05:06:32.0557 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:8.274000 valid-rmse:8.274000\n",
            "[INFO 23-11-05 05:06:32.0570 UTC hyperparameters_optimizer.cc:582] [79/100] Score: -8.274 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 05:06:32.0589 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 05:06:32.0589 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 05:06:32.0705 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 05:06:37.2523 UTC gradient_boosted_trees.cc:1556] \tnum-trees:207 train-loss:7.803454 train-rmse:7.803454 valid-loss:8.192325 valid-rmse:8.192325\n",
            "[INFO 23-11-05 05:06:47.4663 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.585729 train-rmse:8.585729 valid-loss:8.522403 valid-rmse:8.522403\n",
            "[INFO 23-11-05 05:07:07.7580 UTC gradient_boosted_trees.cc:1556] \tnum-trees:210 train-loss:7.797402 train-rmse:7.797402 valid-loss:8.186234 valid-rmse:8.186234\n",
            "[INFO 23-11-05 05:07:37.7744 UTC gradient_boosted_trees.cc:1556] \tnum-trees:65 train-loss:8.359048 train-rmse:8.359048 valid-loss:8.435061 valid-rmse:8.435061\n",
            "[INFO 23-11-05 05:08:08.2330 UTC gradient_boosted_trees.cc:1556] \tnum-trees:93 train-loss:7.017253 train-rmse:7.017253 valid-loss:8.038409 valid-rmse:8.038409\n",
            "[INFO 23-11-05 05:08:38.5150 UTC gradient_boosted_trees.cc:1556] \tnum-trees:219 train-loss:7.782133 train-rmse:7.782133 valid-loss:8.177939 valid-rmse:8.177939\n",
            "[INFO 23-11-05 05:09:08.9497 UTC gradient_boosted_trees.cc:1556] \tnum-trees:222 train-loss:7.776929 train-rmse:7.776929 valid-loss:8.177086 valid-rmse:8.177086\n",
            "[INFO 23-11-05 05:09:39.4088 UTC gradient_boosted_trees.cc:1556] \tnum-trees:225 train-loss:7.769561 train-rmse:7.769561 valid-loss:8.172311 valid-rmse:8.172311\n",
            "[INFO 23-11-05 05:10:09.5984 UTC gradient_boosted_trees.cc:1556] \tnum-trees:228 train-loss:7.763732 train-rmse:7.763732 valid-loss:8.172968 valid-rmse:8.172968\n",
            "[INFO 23-11-05 05:10:40.0580 UTC gradient_boosted_trees.cc:1556] \tnum-trees:231 train-loss:7.758100 train-rmse:7.758100 valid-loss:8.171219 valid-rmse:8.171219\n",
            "[INFO 23-11-05 05:11:10.3573 UTC gradient_boosted_trees.cc:1556] \tnum-trees:234 train-loss:7.752658 train-rmse:7.752658 valid-loss:8.170902 valid-rmse:8.170902\n",
            "[INFO 23-11-05 05:11:40.7634 UTC gradient_boosted_trees.cc:1556] \tnum-trees:237 train-loss:7.747746 train-rmse:7.747746 valid-loss:8.167704 valid-rmse:8.167704\n",
            "[INFO 23-11-05 05:12:10.9948 UTC gradient_boosted_trees.cc:1556] \tnum-trees:165 train-loss:7.286961 train-rmse:7.286961 valid-loss:8.030626 valid-rmse:8.030626\n",
            "[INFO 23-11-05 05:12:41.4891 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:7.738888 train-rmse:7.738888 valid-loss:8.163171 valid-rmse:8.163171\n",
            "[INFO 23-11-05 05:13:11.8881 UTC gradient_boosted_trees.cc:1556] \tnum-trees:227 train-loss:7.515325 train-rmse:7.515325 valid-loss:8.169261 valid-rmse:8.169261\n",
            "[INFO 23-11-05 05:13:42.1850 UTC gradient_boosted_trees.cc:1556] \tnum-trees:249 train-loss:7.725747 train-rmse:7.725747 valid-loss:8.155563 valid-rmse:8.155563\n",
            "[INFO 23-11-05 05:14:12.7395 UTC gradient_boosted_trees.cc:1556] \tnum-trees:252 train-loss:7.720486 train-rmse:7.720486 valid-loss:8.154928 valid-rmse:8.154928\n",
            "[INFO 23-11-05 05:14:43.2478 UTC gradient_boosted_trees.cc:1556] \tnum-trees:255 train-loss:7.713964 train-rmse:7.713964 valid-loss:8.148278 valid-rmse:8.148278\n",
            "[INFO 23-11-05 05:15:13.4950 UTC gradient_boosted_trees.cc:1556] \tnum-trees:258 train-loss:7.709598 train-rmse:7.709598 valid-loss:8.144910 valid-rmse:8.144910\n",
            "[INFO 23-11-05 05:15:44.1815 UTC gradient_boosted_trees.cc:1556] \tnum-trees:261 train-loss:7.704059 train-rmse:7.704059 valid-loss:8.142360 valid-rmse:8.142360\n",
            "[INFO 23-11-05 05:16:14.9553 UTC gradient_boosted_trees.cc:1556] \tnum-trees:264 train-loss:7.699996 train-rmse:7.699996 valid-loss:8.141060 valid-rmse:8.141060\n",
            "[INFO 23-11-05 05:16:45.0168 UTC gradient_boosted_trees.cc:1556] \tnum-trees:267 train-loss:7.695349 train-rmse:7.695349 valid-loss:8.137559 valid-rmse:8.137559\n",
            "[INFO 23-11-05 05:17:15.6019 UTC gradient_boosted_trees.cc:1556] \tnum-trees:270 train-loss:7.692200 train-rmse:7.692200 valid-loss:8.136102 valid-rmse:8.136102\n",
            "[INFO 23-11-05 05:17:45.9991 UTC gradient_boosted_trees.cc:1556] \tnum-trees:273 train-loss:7.688651 train-rmse:7.688651 valid-loss:8.133169 valid-rmse:8.133169\n",
            "[INFO 23-11-05 05:18:16.2511 UTC gradient_boosted_trees.cc:1556] \tnum-trees:276 train-loss:7.681949 train-rmse:7.681949 valid-loss:8.132836 valid-rmse:8.132836\n",
            "[INFO 23-11-05 05:18:46.7216 UTC gradient_boosted_trees.cc:1556] \tnum-trees:279 train-loss:7.673288 train-rmse:7.673288 valid-loss:8.126754 valid-rmse:8.126754\n",
            "[INFO 23-11-05 05:19:17.1101 UTC gradient_boosted_trees.cc:1556] \tnum-trees:282 train-loss:7.670025 train-rmse:7.670025 valid-loss:8.125006 valid-rmse:8.125006\n",
            "[INFO 23-11-05 05:19:47.5387 UTC gradient_boosted_trees.cc:1556] \tnum-trees:117 train-loss:7.982199 train-rmse:7.982199 valid-loss:8.283819 valid-rmse:8.283819\n",
            "[INFO 23-11-05 05:20:18.0669 UTC gradient_boosted_trees.cc:1556] \tnum-trees:288 train-loss:7.661798 train-rmse:7.661798 valid-loss:8.126806 valid-rmse:8.126806\n",
            "[INFO 23-11-05 05:20:48.5371 UTC gradient_boosted_trees.cc:1556] \tnum-trees:291 train-loss:7.657859 train-rmse:7.657859 valid-loss:8.123968 valid-rmse:8.123968\n",
            "[INFO 23-11-05 05:21:18.9651 UTC gradient_boosted_trees.cc:1556] \tnum-trees:294 train-loss:7.653182 train-rmse:7.653182 valid-loss:8.123600 valid-rmse:8.123600\n",
            "[INFO 23-11-05 05:21:49.0668 UTC gradient_boosted_trees.cc:1556] \tnum-trees:297 train-loss:7.647699 train-rmse:7.647699 valid-loss:8.119820 valid-rmse:8.119820\n",
            "[INFO 23-11-05 05:22:19.3072 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.641546 train-rmse:7.641546 valid-loss:8.118062 valid-rmse:8.118062\n",
            "[INFO 23-11-05 05:22:19.3098 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 05:22:19.3113 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.118062 valid-rmse:8.118062\n",
            "[INFO 23-11-05 05:22:19.3155 UTC hyperparameters_optimizer.cc:582] [80/100] Score: -8.11806 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 05:22:19.3162 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 05:22:19.3162 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 05:22:19.3411 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 05:22:26.5931 UTC gradient_boosted_trees.cc:1556] \tnum-trees:184 train-loss:7.705504 train-rmse:7.705504 valid-loss:8.193962 valid-rmse:8.193962\n",
            "[INFO 23-11-05 05:22:44.4607 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.582082 train-rmse:8.582082 valid-loss:8.536853 valid-rmse:8.536853\n",
            "[INFO 23-11-05 05:22:58.6965 UTC gradient_boosted_trees.cc:1556] \tnum-trees:121 train-loss:6.769563 train-rmse:6.769563 valid-loss:7.980597 valid-rmse:7.980597\n",
            "[INFO 23-11-05 05:23:30.0174 UTC gradient_boosted_trees.cc:1556] \tnum-trees:149 train-loss:7.277603 train-rmse:7.277603 valid-loss:8.069847 valid-rmse:8.069847\n",
            "[INFO 23-11-05 05:24:01.1616 UTC gradient_boosted_trees.cc:1556] \tnum-trees:4 train-loss:8.420840 train-rmse:8.420840 valid-loss:8.458368 valid-rmse:8.458368\n",
            "[INFO 23-11-05 05:24:32.4560 UTC gradient_boosted_trees.cc:1556] \tnum-trees:261 train-loss:7.427978 train-rmse:7.427978 valid-loss:8.140572 valid-rmse:8.140572\n",
            "[INFO 23-11-05 05:25:03.3217 UTC gradient_boosted_trees.cc:1556] \tnum-trees:130 train-loss:7.942640 train-rmse:7.942640 valid-loss:8.262359 valid-rmse:8.262359\n",
            "[INFO 23-11-05 05:25:34.6422 UTC gradient_boosted_trees.cc:1556] \tnum-trees:154 train-loss:7.253808 train-rmse:7.253808 valid-loss:8.059144 valid-rmse:8.059144\n",
            "[INFO 23-11-05 05:26:06.6623 UTC gradient_boosted_trees.cc:1556] \tnum-trees:151 train-loss:8.212478 train-rmse:8.212478 valid-loss:8.363977 valid-rmse:8.363977\n",
            "[INFO 23-11-05 05:26:40.4366 UTC gradient_boosted_trees.cc:1556] \tnum-trees:134 train-loss:7.932836 train-rmse:7.932836 valid-loss:8.258341 valid-rmse:8.258341\n",
            "[INFO 23-11-05 05:27:11.0784 UTC gradient_boosted_trees.cc:1556] \tnum-trees:156 train-loss:8.204311 train-rmse:8.204311 valid-loss:8.363791 valid-rmse:8.363791\n",
            "[INFO 23-11-05 05:27:45.2923 UTC gradient_boosted_trees.cc:1556] \tnum-trees:130 train-loss:6.703271 train-rmse:6.703271 valid-loss:7.962651 valid-rmse:7.962651\n",
            "[INFO 23-11-05 05:28:15.3339 UTC gradient_boosted_trees.cc:1556] \tnum-trees:161 train-loss:8.198900 train-rmse:8.198900 valid-loss:8.358886 valid-rmse:8.358886\n",
            "[INFO 23-11-05 05:28:45.8142 UTC gradient_boosted_trees.cc:1556] \tnum-trees:199 train-loss:7.228834 train-rmse:7.228834 valid-loss:8.109244 valid-rmse:8.109244\n",
            "[INFO 23-11-05 05:29:15.8248 UTC gradient_boosted_trees.cc:1556] \tnum-trees:204 train-loss:7.654653 train-rmse:7.654653 valid-loss:8.176333 valid-rmse:8.176333\n",
            "[INFO 23-11-05 05:29:50.6708 UTC gradient_boosted_trees.cc:1556] \tnum-trees:277 train-loss:7.388444 train-rmse:7.388444 valid-loss:8.124826 valid-rmse:8.124826\n",
            "[INFO 23-11-05 05:30:23.4552 UTC gradient_boosted_trees.cc:1556] \tnum-trees:171 train-loss:8.186530 train-rmse:8.186530 valid-loss:8.350480 valid-rmse:8.350480\n",
            "[INFO 23-11-05 05:30:53.9673 UTC gradient_boosted_trees.cc:1556] \tnum-trees:167 train-loss:7.183794 train-rmse:7.183794 valid-loss:8.048244 valid-rmse:8.048244\n",
            "[INFO 23-11-05 05:31:27.8935 UTC gradient_boosted_trees.cc:1556] \tnum-trees:176 train-loss:8.180311 train-rmse:8.180311 valid-loss:8.347056 valid-rmse:8.347056\n",
            "[INFO 23-11-05 05:32:00.4588 UTC gradient_boosted_trees.cc:1556] \tnum-trees:212 train-loss:7.634067 train-rmse:7.634067 valid-loss:8.170012 valid-rmse:8.170012\n",
            "[INFO 23-11-05 05:32:32.4686 UTC gradient_boosted_trees.cc:1556] \tnum-trees:181 train-loss:8.174887 train-rmse:8.174887 valid-loss:8.346510 valid-rmse:8.346510\n",
            "[INFO 23-11-05 05:33:04.9903 UTC gradient_boosted_trees.cc:1556] \tnum-trees:25 train-loss:7.981647 train-rmse:7.981647 valid-loss:8.324991 valid-rmse:8.324991\n",
            "[INFO 23-11-05 05:33:36.5054 UTC gradient_boosted_trees.cc:1556] \tnum-trees:141 train-loss:6.625671 train-rmse:6.625671 valid-loss:7.948514 valid-rmse:7.948514\n",
            "[INFO 23-11-05 05:34:08.8928 UTC gradient_boosted_trees.cc:1556] \tnum-trees:290 train-loss:7.350824 train-rmse:7.350824 valid-loss:8.111406 valid-rmse:8.111406\n",
            "[INFO 23-11-05 05:34:41.0163 UTC gradient_boosted_trees.cc:1556] \tnum-trees:143 train-loss:6.611764 train-rmse:6.611764 valid-loss:7.946690 valid-rmse:7.946690\n",
            "[INFO 23-11-05 05:35:12.9613 UTC gradient_boosted_trees.cc:1556] \tnum-trees:144 train-loss:6.604638 train-rmse:6.604638 valid-loss:7.944167 valid-rmse:7.944167\n",
            "[INFO 23-11-05 05:35:45.0902 UTC gradient_boosted_trees.cc:1556] \tnum-trees:223 train-loss:7.608695 train-rmse:7.608695 valid-loss:8.162147 valid-rmse:8.162147\n",
            "[INFO 23-11-05 05:36:16.1229 UTC gradient_boosted_trees.cc:1556] \tnum-trees:180 train-loss:7.124732 train-rmse:7.124732 valid-loss:8.033008 valid-rmse:8.033008\n",
            "[INFO 23-11-05 05:36:47.4730 UTC gradient_boosted_trees.cc:1556] \tnum-trees:225 train-loss:7.105357 train-rmse:7.105357 valid-loss:8.077010 valid-rmse:8.077010\n",
            "[INFO 23-11-05 05:37:20.3489 UTC gradient_boosted_trees.cc:1556] \tnum-trees:148 train-loss:6.576167 train-rmse:6.576167 valid-loss:7.938972 valid-rmse:7.938972\n",
            "[INFO 23-11-05 05:37:30.6030 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.325761 train-rmse:7.325761 valid-loss:8.102188 valid-rmse:8.102188\n",
            "[INFO 23-11-05 05:37:30.6030 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 05:37:30.6030 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.102188 valid-rmse:8.102188\n",
            "[INFO 23-11-05 05:37:30.6108 UTC hyperparameters_optimizer.cc:582] [81/100] Score: -8.10219 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 128 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 05:37:30.6144 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 05:37:30.6145 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 05:37:30.6278 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 05:37:50.8920 UTC gradient_boosted_trees.cc:1556] \tnum-trees:36 train-loss:7.810310 train-rmse:7.810310 valid-loss:8.246988 valid-rmse:8.246988\n",
            "[INFO 23-11-05 05:38:07.6855 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.604388 train-rmse:8.604388 valid-loss:8.537115 valid-rmse:8.537115\n",
            "[INFO 23-11-05 05:38:21.4961 UTC gradient_boosted_trees.cc:1556] \tnum-trees:208 train-loss:8.141731 train-rmse:8.141731 valid-loss:8.328244 valid-rmse:8.328244\n",
            "[INFO 23-11-05 05:38:55.7773 UTC gradient_boosted_trees.cc:1556] \tnum-trees:151 train-loss:6.544326 train-rmse:6.544326 valid-loss:7.927137 valid-rmse:7.927137\n",
            "[INFO 23-11-05 05:39:25.9045 UTC gradient_boosted_trees.cc:1556] \tnum-trees:213 train-loss:8.136872 train-rmse:8.136872 valid-loss:8.327059 valid-rmse:8.327059\n",
            "[INFO 23-11-05 05:39:57.8010 UTC gradient_boosted_trees.cc:1556] \tnum-trees:189 train-loss:7.089600 train-rmse:7.089600 valid-loss:8.021067 valid-rmse:8.021067\n",
            "[INFO 23-11-05 05:40:29.3195 UTC gradient_boosted_trees.cc:1556] \tnum-trees:42 train-loss:7.738020 train-rmse:7.738020 valid-loss:8.205808 valid-rmse:8.205808\n",
            "[INFO 23-11-05 05:41:06.0490 UTC gradient_boosted_trees.cc:1556] \tnum-trees:170 train-loss:7.837461 train-rmse:7.837461 valid-loss:8.216150 valid-rmse:8.216150\n",
            "[INFO 23-11-05 05:41:36.1610 UTC gradient_boosted_trees.cc:1556] \tnum-trees:193 train-loss:7.066895 train-rmse:7.066895 valid-loss:8.017543 valid-rmse:8.017543\n",
            "[INFO 23-11-05 05:42:10.5592 UTC gradient_boosted_trees.cc:1556] \tnum-trees:242 train-loss:7.563079 train-rmse:7.563079 valid-loss:8.147731 valid-rmse:8.147731\n",
            "[INFO 23-11-05 05:42:41.4389 UTC gradient_boosted_trees.cc:1556] \tnum-trees:47 train-loss:7.686264 train-rmse:7.686264 valid-loss:8.195916 valid-rmse:8.195916\n",
            "[INFO 23-11-05 05:43:11.5193 UTC gradient_boosted_trees.cc:1556] \tnum-trees:245 train-loss:7.557072 train-rmse:7.557072 valid-loss:8.147329 valid-rmse:8.147329\n",
            "[INFO 23-11-05 05:43:43.8379 UTC gradient_boosted_trees.cc:1556] \tnum-trees:233 train-loss:8.113385 train-rmse:8.113385 valid-loss:8.314698 valid-rmse:8.314698\n",
            "[INFO 23-11-05 05:44:14.8605 UTC gradient_boosted_trees.cc:1556] \tnum-trees:11 train-loss:8.243717 train-rmse:8.243717 valid-loss:8.404331 valid-rmse:8.404331\n",
            "[INFO 23-11-05 05:44:48.3239 UTC gradient_boosted_trees.cc:1556] \tnum-trees:238 train-loss:8.106712 train-rmse:8.106712 valid-loss:8.313902 valid-rmse:8.313902\n",
            "[INFO 23-11-05 05:45:18.8686 UTC gradient_boosted_trees.cc:1556] \tnum-trees:202 train-loss:7.029307 train-rmse:7.029307 valid-loss:8.010079 valid-rmse:8.010079\n",
            "[INFO 23-11-05 05:45:49.7484 UTC gradient_boosted_trees.cc:1556] \tnum-trees:254 train-loss:6.988539 train-rmse:6.988539 valid-loss:8.042929 valid-rmse:8.042929\n",
            "[INFO 23-11-05 05:46:20.8638 UTC gradient_boosted_trees.cc:1556] \tnum-trees:183 train-loss:7.811245 train-rmse:7.811245 valid-loss:8.204331 valid-rmse:8.204331\n",
            "[INFO 23-11-05 05:46:53.9341 UTC gradient_boosted_trees.cc:1556] \tnum-trees:166 train-loss:6.421810 train-rmse:6.421810 valid-loss:7.908451 valid-rmse:7.908451\n",
            "[INFO 23-11-05 05:47:26.6798 UTC gradient_boosted_trees.cc:1556] \tnum-trees:167 train-loss:6.417068 train-rmse:6.417068 valid-loss:7.907802 valid-rmse:7.907802\n",
            "[INFO 23-11-05 05:47:57.8942 UTC gradient_boosted_trees.cc:1556] \tnum-trees:187 train-loss:7.801294 train-rmse:7.801294 valid-loss:8.200034 valid-rmse:8.200034\n",
            "[INFO 23-11-05 05:48:30.9769 UTC gradient_boosted_trees.cc:1556] \tnum-trees:169 train-loss:6.405978 train-rmse:6.405978 valid-loss:7.903214 valid-rmse:7.903214\n",
            "[INFO 23-11-05 05:49:03.1345 UTC gradient_boosted_trees.cc:1556] \tnum-trees:170 train-loss:6.398745 train-rmse:6.398745 valid-loss:7.899577 valid-rmse:7.899577\n",
            "[INFO 23-11-05 05:49:33.5145 UTC gradient_boosted_trees.cc:1556] \tnum-trees:266 train-loss:6.935820 train-rmse:6.935820 valid-loss:8.033189 valid-rmse:8.033189\n",
            "[INFO 23-11-05 05:50:05.3349 UTC gradient_boosted_trees.cc:1556] \tnum-trees:64 train-loss:7.491797 train-rmse:7.491797 valid-loss:8.139673 valid-rmse:8.139673\n",
            "[INFO 23-11-05 05:50:35.4169 UTC gradient_boosted_trees.cc:1556] \tnum-trees:265 train-loss:8.077562 train-rmse:8.077562 valid-loss:8.294796 valid-rmse:8.294796\n",
            "[INFO 23-11-05 05:51:06.3219 UTC gradient_boosted_trees.cc:1556] \tnum-trees:214 train-loss:7.389310 train-rmse:7.389310 valid-loss:8.100871 valid-rmse:8.100871\n",
            "[INFO 23-11-05 05:51:36.3400 UTC gradient_boosted_trees.cc:1556] \tnum-trees:23 train-loss:7.966671 train-rmse:7.966671 valid-loss:8.318014 valid-rmse:8.318014\n",
            "[INFO 23-11-05 05:52:08.2807 UTC gradient_boosted_trees.cc:1556] \tnum-trees:239 train-loss:6.992099 train-rmse:6.992099 valid-loss:7.932061 valid-rmse:7.932061\n",
            "[INFO 23-11-05 05:52:39.2309 UTC gradient_boosted_trees.cc:1556] \tnum-trees:273 train-loss:7.492256 train-rmse:7.492256 valid-loss:8.123650 valid-rmse:8.123650\n",
            "[INFO 23-11-05 05:53:09.6560 UTC gradient_boosted_trees.cc:1556] \tnum-trees:221 train-loss:6.940691 train-rmse:6.940691 valid-loss:7.988717 valid-rmse:7.988717\n",
            "[INFO 23-11-05 05:53:40.0756 UTC gradient_boosted_trees.cc:1556] \tnum-trees:276 train-loss:7.484333 train-rmse:7.484333 valid-loss:8.121908 valid-rmse:8.121908\n",
            "[INFO 23-11-05 05:54:12.8803 UTC gradient_boosted_trees.cc:1556] \tnum-trees:282 train-loss:8.057935 train-rmse:8.057935 valid-loss:8.288244 valid-rmse:8.288244\n",
            "[INFO 23-11-05 05:54:48.1991 UTC gradient_boosted_trees.cc:1556] \tnum-trees:204 train-loss:7.764582 train-rmse:7.764582 valid-loss:8.183939 valid-rmse:8.183939\n",
            "[INFO 23-11-05 05:55:20.6825 UTC gradient_boosted_trees.cc:1556] \tnum-trees:76 train-loss:7.390883 train-rmse:7.390883 valid-loss:8.118235 valid-rmse:8.118235\n",
            "[INFO 23-11-05 05:55:54.6097 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:6.970609 train-rmse:6.970609 valid-loss:7.923902 valid-rmse:7.923902\n",
            "[INFO 23-11-05 05:56:26.2156 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:6.963373 train-rmse:6.963373 valid-loss:7.920362 valid-rmse:7.920362\n",
            "[INFO 23-11-05 05:56:58.8573 UTC gradient_boosted_trees.cc:1556] \tnum-trees:248 train-loss:6.961107 train-rmse:6.961107 valid-loss:7.919672 valid-rmse:7.919672\n",
            "[INFO 23-11-05 05:57:31.5228 UTC gradient_boosted_trees.cc:1556] \tnum-trees:249 train-loss:6.958824 train-rmse:6.958824 valid-loss:7.919287 valid-rmse:7.919287\n",
            "[INFO 23-11-05 05:58:03.3500 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:8.039238 train-rmse:8.039238 valid-loss:8.284008 valid-rmse:8.284008\n",
            "[INFO 23-11-05 05:58:03.3500 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 05:58:03.3500 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:8.283678 valid-rmse:8.283678\n",
            "[INFO 23-11-05 05:58:03.3514 UTC hyperparameters_optimizer.cc:582] [82/100] Score: -8.28368 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 05:58:03.3524 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 05:58:03.3524 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 05:58:03.3642 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 05:58:03.9364 UTC gradient_boosted_trees.cc:1556] \tnum-trees:250 train-loss:6.954431 train-rmse:6.954431 valid-loss:7.917125 valid-rmse:7.917125\n",
            "[INFO 23-11-05 05:58:34.7489 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.585631 train-rmse:8.585631 valid-loss:8.532502 valid-rmse:8.532502\n",
            "[INFO 23-11-05 05:58:36.1765 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:6.948961 train-rmse:6.948961 valid-loss:7.916465 valid-rmse:7.916465\n",
            "[INFO 23-11-05 05:59:07.7108 UTC gradient_boosted_trees.cc:1556] \tnum-trees:292 train-loss:7.448242 train-rmse:7.448242 valid-loss:8.109008 valid-rmse:8.109008\n",
            "[INFO 23-11-05 05:59:38.7818 UTC gradient_boosted_trees.cc:1556] \tnum-trees:200 train-loss:6.338378 train-rmse:6.338378 valid-loss:7.930490 valid-rmse:7.930490\n",
            "[INFO 23-11-05 06:00:09.1727 UTC gradient_boosted_trees.cc:1556] \tnum-trees:87 train-loss:7.286696 train-rmse:7.286696 valid-loss:8.084025 valid-rmse:8.084025\n",
            "[INFO 23-11-05 06:00:12.6009 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.804947 train-rmse:6.804947 valid-loss:7.998986 valid-rmse:7.998986\n",
            "[INFO 23-11-05 06:00:12.6009 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 06:00:12.6010 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.996695 valid-rmse:7.996695\n",
            "[INFO 23-11-05 06:00:12.6055 UTC hyperparameters_optimizer.cc:582] [83/100] Score: -7.99669 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 06:00:12.6113 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 06:00:12.6113 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 06:00:12.6230 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 06:00:28.2518 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.621759 train-rmse:8.621759 valid-loss:8.539131 valid-rmse:8.539131\n",
            "[INFO 23-11-05 06:00:39.8655 UTC gradient_boosted_trees.cc:1556] \tnum-trees:5 train-loss:8.371857 train-rmse:8.371857 valid-loss:8.447409 valid-rmse:8.447409\n",
            "[INFO 23-11-05 06:01:09.9284 UTC gradient_boosted_trees.cc:1556] \tnum-trees:152 train-loss:7.524400 train-rmse:7.524400 valid-loss:8.163522 valid-rmse:8.163522\n",
            "[INFO 23-11-05 06:01:41.8530 UTC gradient_boosted_trees.cc:1556] \tnum-trees:153 train-loss:7.520742 train-rmse:7.520742 valid-loss:8.161354 valid-rmse:8.161354\n",
            "[INFO 23-11-05 06:01:50.1161 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.433874 train-rmse:7.433874 valid-loss:8.107015 valid-rmse:8.107015\n",
            "[INFO 23-11-05 06:01:50.1161 UTC gradient_boosted_trees.cc:249] Truncates the model to 297 tree(s) i.e. 297  iteration(s).\n",
            "[INFO 23-11-05 06:01:50.1161 UTC gradient_boosted_trees.cc:312] Final model num-trees:297 valid-loss:8.106990 valid-rmse:8.106990\n",
            "[INFO 23-11-05 06:01:50.1226 UTC hyperparameters_optimizer.cc:582] [84/100] Score: -8.10699 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 6 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 06:01:50.1274 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 06:01:50.1274 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 06:01:50.1404 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 06:02:07.5113 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.590549 train-rmse:8.590549 valid-loss:8.533072 valid-rmse:8.533072\n",
            "[INFO 23-11-05 06:02:13.3920 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:6.850772 train-rmse:6.850772 valid-loss:7.964524 valid-rmse:7.964524\n",
            "[INFO 23-11-05 06:02:45.2331 UTC gradient_boosted_trees.cc:1556] \tnum-trees:93 train-loss:7.236471 train-rmse:7.236471 valid-loss:8.074183 valid-rmse:8.074183\n",
            "[INFO 23-11-05 06:03:16.4513 UTC gradient_boosted_trees.cc:1556] \tnum-trees:10 train-loss:8.195975 train-rmse:8.195975 valid-loss:8.385613 valid-rmse:8.385613\n",
            "[INFO 23-11-05 06:03:46.9283 UTC gradient_boosted_trees.cc:1556] \tnum-trees:43 train-loss:7.650136 train-rmse:7.650136 valid-loss:8.223772 valid-rmse:8.223772\n",
            "[INFO 23-11-05 06:04:17.0298 UTC gradient_boosted_trees.cc:1556] \tnum-trees:248 train-loss:6.835340 train-rmse:6.835340 valid-loss:7.959145 valid-rmse:7.959145\n",
            "[INFO 23-11-05 06:04:49.2247 UTC gradient_boosted_trees.cc:1556] \tnum-trees:220 train-loss:6.201752 train-rmse:6.201752 valid-loss:7.921759 valid-rmse:7.921759\n",
            "[INFO 23-11-05 06:05:21.0681 UTC gradient_boosted_trees.cc:1556] \tnum-trees:222 train-loss:6.192065 train-rmse:6.192065 valid-loss:7.923209 valid-rmse:7.923209\n",
            "[INFO 23-11-05 06:05:52.6947 UTC gradient_boosted_trees.cc:1556] \tnum-trees:224 train-loss:6.177725 train-rmse:6.177725 valid-loss:7.912959 valid-rmse:7.912959\n",
            "[INFO 23-11-05 06:06:24.1301 UTC gradient_boosted_trees.cc:1556] \tnum-trees:226 train-loss:6.163882 train-rmse:6.163882 valid-loss:7.911170 valid-rmse:7.911170\n",
            "[INFO 23-11-05 06:06:54.6504 UTC gradient_boosted_trees.cc:1556] \tnum-trees:234 train-loss:7.706912 train-rmse:7.706912 valid-loss:8.164042 valid-rmse:8.164042\n",
            "[INFO 23-11-05 06:07:29.6813 UTC gradient_boosted_trees.cc:1556] \tnum-trees:49 train-loss:7.570661 train-rmse:7.570661 valid-loss:8.199323 valid-rmse:8.199323\n",
            "[INFO 23-11-05 06:08:00.0717 UTC gradient_boosted_trees.cc:1556] \tnum-trees:105 train-loss:7.163840 train-rmse:7.163840 valid-loss:8.057691 valid-rmse:8.057691\n",
            "[INFO 23-11-05 06:08:31.6517 UTC gradient_boosted_trees.cc:1556] \tnum-trees:238 train-loss:7.698709 train-rmse:7.698709 valid-loss:8.161051 valid-rmse:8.161051\n",
            "[INFO 23-11-05 06:09:04.2684 UTC gradient_boosted_trees.cc:1556] \tnum-trees:24 train-loss:8.058768 train-rmse:8.058768 valid-loss:8.358258 valid-rmse:8.358258\n",
            "[INFO 23-11-05 06:09:37.1479 UTC gradient_boosted_trees.cc:1556] \tnum-trees:261 train-loss:6.791523 train-rmse:6.791523 valid-loss:7.942341 valid-rmse:7.942341\n",
            "[INFO 23-11-05 06:10:07.4637 UTC gradient_boosted_trees.cc:1556] \tnum-trees:242 train-loss:7.691691 train-rmse:7.691691 valid-loss:8.158229 valid-rmse:8.158229\n",
            "[INFO 23-11-05 06:10:37.6801 UTC gradient_boosted_trees.cc:1556] \tnum-trees:29 train-loss:7.995194 train-rmse:7.995194 valid-loss:8.371051 valid-rmse:8.371051\n",
            "[INFO 23-11-05 06:11:13.1365 UTC gradient_boosted_trees.cc:1556] \tnum-trees:55 train-loss:7.494648 train-rmse:7.494648 valid-loss:8.179670 valid-rmse:8.179670\n",
            "[INFO 23-11-05 06:11:43.8190 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:7.683805 train-rmse:7.683805 valid-loss:8.157622 valid-rmse:8.157622\n",
            "[INFO 23-11-05 06:12:22.4910 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:7.099608 train-rmse:7.099608 valid-loss:8.037455 valid-rmse:8.037455\n",
            "[INFO 23-11-05 06:12:54.8499 UTC gradient_boosted_trees.cc:1556] \tnum-trees:269 train-loss:6.758410 train-rmse:6.758410 valid-loss:7.930313 valid-rmse:7.930313\n",
            "[INFO 23-11-05 06:13:28.9773 UTC gradient_boosted_trees.cc:1556] \tnum-trees:50 train-loss:7.940241 train-rmse:7.940241 valid-loss:8.262396 valid-rmse:8.262396\n",
            "[INFO 23-11-05 06:14:00.3503 UTC gradient_boosted_trees.cc:1556] \tnum-trees:52 train-loss:7.927886 train-rmse:7.927886 valid-loss:8.253012 valid-rmse:8.253012\n",
            "[INFO 23-11-05 06:14:30.8079 UTC gradient_boosted_trees.cc:1556] \tnum-trees:253 train-loss:7.665140 train-rmse:7.665140 valid-loss:8.151435 valid-rmse:8.151435\n",
            "[INFO 23-11-05 06:15:04.8502 UTC gradient_boosted_trees.cc:1556] \tnum-trees:56 train-loss:7.897462 train-rmse:7.897462 valid-loss:8.237361 valid-rmse:8.237361\n",
            "[INFO 23-11-05 06:15:35.1310 UTC gradient_boosted_trees.cc:1556] \tnum-trees:45 train-loss:7.826771 train-rmse:7.826771 valid-loss:8.314220 valid-rmse:8.314220\n",
            "[INFO 23-11-05 06:16:08.4901 UTC gradient_boosted_trees.cc:1556] \tnum-trees:257 train-loss:7.657701 train-rmse:7.657701 valid-loss:8.149083 valid-rmse:8.149083\n",
            "[INFO 23-11-05 06:16:40.5887 UTC gradient_boosted_trees.cc:1556] \tnum-trees:35 train-loss:7.752048 train-rmse:7.752048 valid-loss:8.239805 valid-rmse:8.239805\n",
            "[INFO 23-11-05 06:17:13.0908 UTC gradient_boosted_trees.cc:1556] \tnum-trees:36 train-loss:7.745985 train-rmse:7.745985 valid-loss:8.239223 valid-rmse:8.239223\n",
            "[INFO 23-11-05 06:17:44.7109 UTC gradient_boosted_trees.cc:1556] \tnum-trees:261 train-loss:7.647515 train-rmse:7.647515 valid-loss:8.145483 valid-rmse:8.145483\n",
            "[INFO 23-11-05 06:18:14.9040 UTC gradient_boosted_trees.cc:1556] \tnum-trees:282 train-loss:6.710453 train-rmse:6.710453 valid-loss:7.918288 valid-rmse:7.918288\n",
            "[INFO 23-11-05 06:18:48.9579 UTC gradient_boosted_trees.cc:1556] \tnum-trees:274 train-loss:5.887344 train-rmse:5.887344 valid-loss:7.872238 valid-rmse:7.872238\n",
            "[INFO 23-11-05 06:19:20.9173 UTC gradient_boosted_trees.cc:1556] \tnum-trees:265 train-loss:7.637552 train-rmse:7.637552 valid-loss:8.142686 valid-rmse:8.142686\n",
            "[INFO 23-11-05 06:19:51.5921 UTC gradient_boosted_trees.cc:1556] \tnum-trees:132 train-loss:6.953037 train-rmse:6.953037 valid-loss:8.011955 valid-rmse:8.011955\n",
            "[INFO 23-11-05 06:20:26.0139 UTC gradient_boosted_trees.cc:1556] \tnum-trees:76 train-loss:7.768941 train-rmse:7.768941 valid-loss:8.194391 valid-rmse:8.194391\n",
            "[INFO 23-11-05 06:20:57.7025 UTC gradient_boosted_trees.cc:1556] \tnum-trees:269 train-loss:7.632261 train-rmse:7.632261 valid-loss:8.140517 valid-rmse:8.140517\n",
            "[INFO 23-11-05 06:21:28.8712 UTC gradient_boosted_trees.cc:1556] \tnum-trees:64 train-loss:7.674521 train-rmse:7.674521 valid-loss:8.252202 valid-rmse:8.252202\n",
            "[INFO 23-11-05 06:22:02.1547 UTC gradient_boosted_trees.cc:1556] \tnum-trees:82 train-loss:7.725884 train-rmse:7.725884 valid-loss:8.176617 valid-rmse:8.176617\n",
            "[INFO 23-11-05 06:22:33.8957 UTC gradient_boosted_trees.cc:1556] \tnum-trees:273 train-loss:7.624682 train-rmse:7.624682 valid-loss:8.138081 valid-rmse:8.138081\n",
            "[INFO 23-11-05 06:23:04.9645 UTC gradient_boosted_trees.cc:1556] \tnum-trees:74 train-loss:7.295504 train-rmse:7.295504 valid-loss:8.112491 valid-rmse:8.112491\n",
            "[INFO 23-11-05 06:23:35.2327 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:6.666745 train-rmse:6.666745 valid-loss:7.911908 valid-rmse:7.911908\n",
            "[INFO 23-11-05 06:24:10.6462 UTC gradient_boosted_trees.cc:1556] \tnum-trees:277 train-loss:7.617503 train-rmse:7.617503 valid-loss:8.134505 valid-rmse:8.134505\n",
            "[INFO 23-11-05 06:24:44.2025 UTC gradient_boosted_trees.cc:1556] \tnum-trees:237 train-loss:5.953907 train-rmse:5.953907 valid-loss:7.828797 valid-rmse:7.828797\n",
            "[INFO 23-11-05 06:25:09.1404 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.787670 train-rmse:6.787670 valid-loss:7.871864 valid-rmse:7.871864\n",
            "[INFO 23-11-05 06:25:09.1410 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 06:25:09.1411 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.871864 valid-rmse:7.871864\n",
            "[INFO 23-11-05 06:25:09.1466 UTC hyperparameters_optimizer.cc:582] [85/100] Score: -7.87186 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 06:25:09.1499 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 06:25:09.1506 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 06:25:09.1750 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 06:25:14.2963 UTC gradient_boosted_trees.cc:1556] \tnum-trees:299 train-loss:6.653492 train-rmse:6.653492 valid-loss:7.908869 valid-rmse:7.908869\n",
            "[INFO 23-11-05 06:25:36.1966 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.731241 train-rmse:5.731241 valid-loss:7.872574 valid-rmse:7.872574\n",
            "[INFO 23-11-05 06:25:36.1967 UTC gradient_boosted_trees.cc:249] Truncates the model to 296 tree(s) i.e. 296  iteration(s).\n",
            "[INFO 23-11-05 06:25:36.1967 UTC gradient_boosted_trees.cc:312] Final model num-trees:296 valid-loss:7.864676 valid-rmse:7.864676\n",
            "[INFO 23-11-05 06:25:36.2054 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 06:25:36.2055 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 06:25:36.2057 UTC hyperparameters_optimizer.cc:582] [86/100] Score: -7.86468 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 32 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 06:25:36.2229 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 06:25:37.3644 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.615834 train-rmse:8.615834 valid-loss:8.538647 valid-rmse:8.538647\n",
            "[INFO 23-11-05 06:25:39.4373 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.650557 train-rmse:6.650557 valid-loss:7.907629 valid-rmse:7.907629\n",
            "[INFO 23-11-05 06:25:39.4467 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 06:25:39.4494 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.907629 valid-rmse:7.907629\n",
            "[INFO 23-11-05 06:25:39.4536 UTC hyperparameters_optimizer.cc:582] [87/100] Score: -7.90763 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 06:25:39.4579 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 06:25:39.4581 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 06:25:39.4707 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 06:25:47.0780 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.643537 train-rmse:8.643537 valid-loss:8.553751 valid-rmse:8.553751\n",
            "[INFO 23-11-05 06:25:47.2884 UTC gradient_boosted_trees.cc:1556] \tnum-trees:281 train-loss:7.611922 train-rmse:7.611922 valid-loss:8.131286 valid-rmse:8.131286\n",
            "[INFO 23-11-05 06:25:55.5161 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.586408 train-rmse:8.586408 valid-loss:8.536572 valid-rmse:8.536572\n",
            "[INFO 23-11-05 06:26:17.9295 UTC gradient_boosted_trees.cc:1556] \tnum-trees:5 train-loss:8.612304 train-rmse:8.612304 valid-loss:8.534174 valid-rmse:8.534174\n",
            "[INFO 23-11-05 06:26:48.5321 UTC gradient_boosted_trees.cc:1556] \tnum-trees:9 train-loss:8.583125 train-rmse:8.583125 valid-loss:8.522165 valid-rmse:8.522165\n",
            "[INFO 23-11-05 06:27:18.9342 UTC gradient_boosted_trees.cc:1556] \tnum-trees:13 train-loss:8.556503 train-rmse:8.556503 valid-loss:8.509925 valid-rmse:8.509925\n",
            "[INFO 23-11-05 06:27:49.3573 UTC gradient_boosted_trees.cc:1556] \tnum-trees:17 train-loss:8.532351 train-rmse:8.532351 valid-loss:8.501494 valid-rmse:8.501494\n",
            "[INFO 23-11-05 06:28:19.8182 UTC gradient_boosted_trees.cc:1556] \tnum-trees:21 train-loss:8.512543 train-rmse:8.512543 valid-loss:8.493356 valid-rmse:8.493356\n",
            "[INFO 23-11-05 06:28:50.6488 UTC gradient_boosted_trees.cc:1556] \tnum-trees:25 train-loss:8.492768 train-rmse:8.492768 valid-loss:8.484903 valid-rmse:8.484903\n",
            "[INFO 23-11-05 06:29:21.0445 UTC gradient_boosted_trees.cc:1556] \tnum-trees:285 train-loss:7.207357 train-rmse:7.207357 valid-loss:8.036620 valid-rmse:8.036620\n",
            "[INFO 23-11-05 06:29:51.2644 UTC gradient_boosted_trees.cc:1556] \tnum-trees:10 train-loss:8.335558 train-rmse:8.335558 valid-loss:8.414423 valid-rmse:8.414423\n",
            "[INFO 23-11-05 06:30:22.0421 UTC gradient_boosted_trees.cc:1556] \tnum-trees:37 train-loss:8.445641 train-rmse:8.445641 valid-loss:8.464504 valid-rmse:8.464504\n",
            "[INFO 23-11-05 06:30:52.3978 UTC gradient_boosted_trees.cc:1556] \tnum-trees:41 train-loss:8.431108 train-rmse:8.431108 valid-loss:8.458996 valid-rmse:8.458996\n",
            "[INFO 23-11-05 06:31:22.8166 UTC gradient_boosted_trees.cc:1556] \tnum-trees:45 train-loss:8.416829 train-rmse:8.416829 valid-loss:8.456208 valid-rmse:8.456208\n",
            "[INFO 23-11-05 06:31:53.7218 UTC gradient_boosted_trees.cc:1556] \tnum-trees:49 train-loss:8.404500 train-rmse:8.404500 valid-loss:8.445992 valid-rmse:8.445992\n",
            "[INFO 23-11-05 06:32:24.0409 UTC gradient_boosted_trees.cc:1556] \tnum-trees:53 train-loss:8.394553 train-rmse:8.394553 valid-loss:8.440813 valid-rmse:8.440813\n",
            "[INFO 23-11-05 06:32:54.3411 UTC gradient_boosted_trees.cc:1556] \tnum-trees:57 train-loss:8.383469 train-rmse:8.383469 valid-loss:8.437097 valid-rmse:8.437097\n",
            "[INFO 23-11-05 06:33:25.0485 UTC gradient_boosted_trees.cc:1556] \tnum-trees:163 train-loss:6.770168 train-rmse:6.770168 valid-loss:7.972867 valid-rmse:7.972867\n",
            "[INFO 23-11-05 06:33:26.2868 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.580809 train-rmse:7.580809 valid-loss:8.122192 valid-rmse:8.122192\n",
            "[INFO 23-11-05 06:33:26.2869 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 06:33:26.2869 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.122192 valid-rmse:8.122192\n",
            "[INFO 23-11-05 06:33:26.2909 UTC hyperparameters_optimizer.cc:582] [88/100] Score: -8.12219 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 6 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 06:33:26.2942 UTC gradient_boosted_trees.cc:470] Default loss set to SQUARED_ERROR\n",
            "[INFO 23-11-05 06:33:26.2943 UTC gradient_boosted_trees.cc:1097] Training gradient boosted tree on 40000 example(s) and 49 feature(s).\n",
            "[INFO 23-11-05 06:33:26.3057 UTC gradient_boosted_trees.cc:1140] 36030 examples used for training and 3970 examples used for validation\n",
            "[INFO 23-11-05 06:33:54.3900 UTC gradient_boosted_trees.cc:1554] \tnum-trees:1 train-loss:8.637260 train-rmse:8.637260 valid-loss:8.550670 valid-rmse:8.550670\n",
            "[INFO 23-11-05 06:33:55.1197 UTC gradient_boosted_trees.cc:1556] \tnum-trees:126 train-loss:7.489922 train-rmse:7.489922 valid-loss:8.103372 valid-rmse:8.103372\n",
            "[INFO 23-11-05 06:34:26.2410 UTC gradient_boosted_trees.cc:1556] \tnum-trees:69 train-loss:8.354526 train-rmse:8.354526 valid-loss:8.429663 valid-rmse:8.429663\n",
            "[INFO 23-11-05 06:34:56.7767 UTC gradient_boosted_trees.cc:1556] \tnum-trees:73 train-loss:8.344560 train-rmse:8.344560 valid-loss:8.425576 valid-rmse:8.425576\n",
            "[INFO 23-11-05 06:35:27.1450 UTC gradient_boosted_trees.cc:1556] \tnum-trees:77 train-loss:8.335189 train-rmse:8.335189 valid-loss:8.416685 valid-rmse:8.416685\n",
            "[INFO 23-11-05 06:35:57.6646 UTC gradient_boosted_trees.cc:1556] \tnum-trees:81 train-loss:8.328534 train-rmse:8.328534 valid-loss:8.411668 valid-rmse:8.411668\n",
            "[INFO 23-11-05 06:36:27.7288 UTC gradient_boosted_trees.cc:1556] \tnum-trees:24 train-loss:8.086518 train-rmse:8.086518 valid-loss:8.334926 valid-rmse:8.334926\n",
            "[INFO 23-11-05 06:36:57.7416 UTC gradient_boosted_trees.cc:1556] \tnum-trees:260 train-loss:5.824778 train-rmse:5.824778 valid-loss:7.801531 valid-rmse:7.801531\n",
            "[INFO 23-11-05 06:37:22.8083 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.169325 train-rmse:7.169325 valid-loss:8.028261 valid-rmse:8.028261\n",
            "[INFO 23-11-05 06:37:22.8083 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 06:37:22.8084 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.028261 valid-rmse:8.028261\n",
            "[INFO 23-11-05 06:37:22.8137 UTC hyperparameters_optimizer.cc:582] [89/100] Score: -8.02826 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 06:37:28.9193 UTC gradient_boosted_trees.cc:1556] \tnum-trees:93 train-loss:8.306822 train-rmse:8.306822 valid-loss:8.396719 valid-rmse:8.396719\n",
            "[INFO 23-11-05 06:38:00.4778 UTC gradient_boosted_trees.cc:1556] \tnum-trees:262 train-loss:5.814327 train-rmse:5.814327 valid-loss:7.800163 valid-rmse:7.800163\n",
            "[INFO 23-11-05 06:38:30.8190 UTC gradient_boosted_trees.cc:1556] \tnum-trees:119 train-loss:7.318838 train-rmse:7.318838 valid-loss:8.143986 valid-rmse:8.143986\n",
            "[INFO 23-11-05 06:39:01.0653 UTC gradient_boosted_trees.cc:1556] \tnum-trees:264 train-loss:5.802944 train-rmse:5.802944 valid-loss:7.799565 valid-rmse:7.799565\n",
            "[INFO 23-11-05 06:39:32.4407 UTC gradient_boosted_trees.cc:1556] \tnum-trees:265 train-loss:5.797937 train-rmse:5.797937 valid-loss:7.800058 valid-rmse:7.800058\n",
            "[INFO 23-11-05 06:40:03.6360 UTC gradient_boosted_trees.cc:1556] \tnum-trees:266 train-loss:5.791260 train-rmse:5.791260 valid-loss:7.799288 valid-rmse:7.799288\n",
            "[INFO 23-11-05 06:40:33.9028 UTC gradient_boosted_trees.cc:1556] \tnum-trees:151 train-loss:7.359242 train-rmse:7.359242 valid-loss:8.063553 valid-rmse:8.063553\n",
            "[INFO 23-11-05 06:41:04.8204 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:5.781296 train-rmse:5.781296 valid-loss:7.797848 valid-rmse:7.797848\n",
            "[INFO 23-11-05 06:41:34.8852 UTC gradient_boosted_trees.cc:1556] \tnum-trees:126 train-loss:8.252990 train-rmse:8.252990 valid-loss:8.365779 valid-rmse:8.365779\n",
            "[INFO 23-11-05 06:42:06.5673 UTC gradient_boosted_trees.cc:1556] \tnum-trees:157 train-loss:7.327672 train-rmse:7.327672 valid-loss:8.056711 valid-rmse:8.056711\n",
            "[INFO 23-11-05 06:42:37.4000 UTC gradient_boosted_trees.cc:1556] \tnum-trees:133 train-loss:7.228845 train-rmse:7.228845 valid-loss:8.125377 valid-rmse:8.125377\n",
            "[INFO 23-11-05 06:43:08.1825 UTC gradient_boosted_trees.cc:1556] \tnum-trees:272 train-loss:5.760118 train-rmse:5.760118 valid-loss:7.793827 valid-rmse:7.793827\n",
            "[INFO 23-11-05 06:43:38.2739 UTC gradient_boosted_trees.cc:1556] \tnum-trees:107 train-loss:7.017627 train-rmse:7.017627 valid-loss:8.042471 valid-rmse:8.042471\n",
            "[INFO 23-11-05 06:44:10.6067 UTC gradient_boosted_trees.cc:1556] \tnum-trees:274 train-loss:5.750669 train-rmse:5.750669 valid-loss:7.793415 valid-rmse:7.793415\n",
            "[INFO 23-11-05 06:44:42.1187 UTC gradient_boosted_trees.cc:1556] \tnum-trees:275 train-loss:5.747869 train-rmse:5.747869 valid-loss:7.792871 valid-rmse:7.792871\n",
            "[INFO 23-11-05 06:45:12.7155 UTC gradient_boosted_trees.cc:1556] \tnum-trees:276 train-loss:5.739882 train-rmse:5.739882 valid-loss:7.793205 valid-rmse:7.793205\n",
            "[INFO 23-11-05 06:45:44.2670 UTC gradient_boosted_trees.cc:1556] \tnum-trees:277 train-loss:5.735638 train-rmse:5.735638 valid-loss:7.791204 valid-rmse:7.791204\n",
            "[INFO 23-11-05 06:46:14.4001 UTC gradient_boosted_trees.cc:1556] \tnum-trees:63 train-loss:7.350989 train-rmse:7.350989 valid-loss:8.091209 valid-rmse:8.091209\n",
            "[INFO 23-11-05 06:46:45.0327 UTC gradient_boosted_trees.cc:1556] \tnum-trees:91 train-loss:7.271504 train-rmse:7.271504 valid-loss:8.102616 valid-rmse:8.102616\n",
            "[INFO 23-11-05 06:47:16.7757 UTC gradient_boosted_trees.cc:1556] \tnum-trees:92 train-loss:7.260533 train-rmse:7.260533 valid-loss:8.095741 valid-rmse:8.095741\n",
            "[INFO 23-11-05 06:47:46.7829 UTC gradient_boosted_trees.cc:1556] \tnum-trees:93 train-loss:7.246271 train-rmse:7.246271 valid-loss:8.095231 valid-rmse:8.095231\n",
            "[INFO 23-11-05 06:48:17.1867 UTC gradient_boosted_trees.cc:1556] \tnum-trees:180 train-loss:8.182503 train-rmse:8.182503 valid-loss:8.333854 valid-rmse:8.333854\n",
            "[INFO 23-11-05 06:48:47.9800 UTC gradient_boosted_trees.cc:1556] \tnum-trees:283 train-loss:5.693580 train-rmse:5.693580 valid-loss:7.780759 valid-rmse:7.780759\n",
            "[INFO 23-11-05 06:49:20.4099 UTC gradient_boosted_trees.cc:1556] \tnum-trees:96 train-loss:7.219404 train-rmse:7.219404 valid-loss:8.078426 valid-rmse:8.078426\n",
            "[INFO 23-11-05 06:49:51.8476 UTC gradient_boosted_trees.cc:1556] \tnum-trees:97 train-loss:7.212679 train-rmse:7.212679 valid-loss:8.081454 valid-rmse:8.081454\n",
            "[INFO 23-11-05 06:50:22.5120 UTC gradient_boosted_trees.cc:1556] \tnum-trees:54 train-loss:7.742333 train-rmse:7.742333 valid-loss:8.219561 valid-rmse:8.219561\n",
            "[INFO 23-11-05 06:50:53.9769 UTC gradient_boosted_trees.cc:1556] \tnum-trees:201 train-loss:8.156851 train-rmse:8.156851 valid-loss:8.325782 valid-rmse:8.325782\n",
            "[INFO 23-11-05 06:51:24.0695 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:7.195984 train-rmse:7.195984 valid-loss:8.062914 valid-rmse:8.062914\n",
            "[INFO 23-11-05 06:51:55.5664 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:7.192279 train-rmse:7.192279 valid-loss:8.060982 valid-rmse:8.060982\n",
            "[INFO 23-11-05 06:52:25.8641 UTC gradient_boosted_trees.cc:1556] \tnum-trees:82 train-loss:7.166747 train-rmse:7.166747 valid-loss:8.025343 valid-rmse:8.025343\n",
            "[INFO 23-11-05 06:52:58.5962 UTC gradient_boosted_trees.cc:1556] \tnum-trees:249 train-loss:7.184961 train-rmse:7.184961 valid-loss:8.057505 valid-rmse:8.057505\n",
            "[INFO 23-11-05 06:53:29.8359 UTC gradient_boosted_trees.cc:1556] \tnum-trees:250 train-loss:7.181716 train-rmse:7.181716 valid-loss:8.055808 valid-rmse:8.055808\n",
            "[INFO 23-11-05 06:54:00.7000 UTC gradient_boosted_trees.cc:1556] \tnum-trees:226 train-loss:8.128750 train-rmse:8.128750 valid-loss:8.314169 valid-rmse:8.314169\n",
            "[INFO 23-11-05 06:54:32.2705 UTC gradient_boosted_trees.cc:1556] \tnum-trees:63 train-loss:7.648295 train-rmse:7.648295 valid-loss:8.187528 valid-rmse:8.187528\n",
            "[INFO 23-11-05 06:55:03.8227 UTC gradient_boosted_trees.cc:1556] \tnum-trees:253 train-loss:7.172430 train-rmse:7.172430 valid-loss:8.053383 valid-rmse:8.053383\n",
            "[INFO 23-11-05 06:55:35.5005 UTC gradient_boosted_trees.cc:1556] \tnum-trees:254 train-loss:7.170131 train-rmse:7.170131 valid-loss:8.052429 valid-rmse:8.052429\n",
            "[INFO 23-11-05 06:56:07.2316 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:8.110267 train-rmse:8.110267 valid-loss:8.312388 valid-rmse:8.312388\n",
            "[INFO 23-11-05 06:56:37.2653 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:8.106284 train-rmse:8.106284 valid-loss:8.307815 valid-rmse:8.307815\n",
            "[INFO 23-11-05 06:57:10.5865 UTC gradient_boosted_trees.cc:1556] \tnum-trees:257 train-loss:7.162623 train-rmse:7.162623 valid-loss:8.051640 valid-rmse:8.051640\n",
            "[INFO 23-11-05 06:57:29.7810 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.596861 train-rmse:5.596861 valid-loss:7.760779 valid-rmse:7.760779\n",
            "[INFO 23-11-05 06:57:29.7810 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 06:57:29.7810 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.760779 valid-rmse:7.760779\n",
            "[INFO 23-11-05 06:57:29.7921 UTC hyperparameters_optimizer.cc:582] [90/100] Score: -7.76078 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 4 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 06:57:41.4727 UTC gradient_boosted_trees.cc:1556] \tnum-trees:130 train-loss:6.866447 train-rmse:6.866447 valid-loss:8.019125 valid-rmse:8.019125\n",
            "[INFO 23-11-05 06:58:13.4746 UTC gradient_boosted_trees.cc:1556] \tnum-trees:219 train-loss:7.035355 train-rmse:7.035355 valid-loss:7.998957 valid-rmse:7.998957\n",
            "[INFO 23-11-05 06:58:44.3566 UTC gradient_boosted_trees.cc:1556] \tnum-trees:221 train-loss:7.029205 train-rmse:7.029205 valid-loss:7.998804 valid-rmse:7.998804\n",
            "[INFO 23-11-05 06:59:14.8377 UTC gradient_boosted_trees.cc:1556] \tnum-trees:115 train-loss:7.049592 train-rmse:7.049592 valid-loss:8.036174 valid-rmse:8.036174\n",
            "[INFO 23-11-05 06:59:46.1722 UTC gradient_boosted_trees.cc:1556] \tnum-trees:116 train-loss:7.041161 train-rmse:7.041161 valid-loss:8.034460 valid-rmse:8.034460\n",
            "[INFO 23-11-05 07:00:17.3420 UTC gradient_boosted_trees.cc:1556] \tnum-trees:227 train-loss:7.003947 train-rmse:7.003947 valid-loss:7.992493 valid-rmse:7.992493\n",
            "[INFO 23-11-05 07:00:48.0718 UTC gradient_boosted_trees.cc:1556] \tnum-trees:229 train-loss:6.994003 train-rmse:6.994003 valid-loss:7.988266 valid-rmse:7.988266\n",
            "[INFO 23-11-05 07:01:18.6773 UTC gradient_boosted_trees.cc:1556] \tnum-trees:231 train-loss:6.984617 train-rmse:6.984617 valid-loss:7.986867 valid-rmse:7.986867\n",
            "[INFO 23-11-05 07:01:49.7412 UTC gradient_boosted_trees.cc:1556] \tnum-trees:233 train-loss:6.978600 train-rmse:6.978600 valid-loss:7.983839 valid-rmse:7.983839\n",
            "[INFO 23-11-05 07:02:20.8423 UTC gradient_boosted_trees.cc:1556] \tnum-trees:235 train-loss:6.970009 train-rmse:6.970009 valid-loss:7.982023 valid-rmse:7.982023\n",
            "[INFO 23-11-05 07:02:51.7547 UTC gradient_boosted_trees.cc:1556] \tnum-trees:237 train-loss:6.961036 train-rmse:6.961036 valid-loss:7.981972 valid-rmse:7.981972\n",
            "[INFO 23-11-05 07:03:08.1354 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:8.051598 train-rmse:8.051598 valid-loss:8.284941 valid-rmse:8.284941\n",
            "[INFO 23-11-05 07:03:08.1354 UTC gradient_boosted_trees.cc:249] Truncates the model to 294 tree(s) i.e. 294  iteration(s).\n",
            "[INFO 23-11-05 07:03:08.1364 UTC gradient_boosted_trees.cc:312] Final model num-trees:294 valid-loss:8.283595 valid-rmse:8.283595\n",
            "[INFO 23-11-05 07:03:08.1392 UTC hyperparameters_optimizer.cc:582] [91/100] Score: -8.2836 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 4 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 07:03:22.1168 UTC gradient_boosted_trees.cc:1556] \tnum-trees:239 train-loss:6.951309 train-rmse:6.951309 valid-loss:7.977873 valid-rmse:7.977873\n",
            "[INFO 23-11-05 07:03:52.4850 UTC gradient_boosted_trees.cc:1556] \tnum-trees:241 train-loss:6.942183 train-rmse:6.942183 valid-loss:7.972901 valid-rmse:7.972901\n",
            "[INFO 23-11-05 07:04:22.5799 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:6.932954 train-rmse:6.932954 valid-loss:7.969974 valid-rmse:7.969974\n",
            "[INFO 23-11-05 07:04:53.3410 UTC gradient_boosted_trees.cc:1556] \tnum-trees:245 train-loss:6.927849 train-rmse:6.927849 valid-loss:7.970475 valid-rmse:7.970475\n",
            "[INFO 23-11-05 07:05:23.7489 UTC gradient_boosted_trees.cc:1556] \tnum-trees:247 train-loss:6.918293 train-rmse:6.918293 valid-loss:7.970506 valid-rmse:7.970506\n",
            "[INFO 23-11-05 07:05:54.1593 UTC gradient_boosted_trees.cc:1556] \tnum-trees:249 train-loss:6.910654 train-rmse:6.910654 valid-loss:7.965856 valid-rmse:7.965856\n",
            "[INFO 23-11-05 07:06:24.6286 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:6.902681 train-rmse:6.902681 valid-loss:7.964900 valid-rmse:7.964900\n",
            "[INFO 23-11-05 07:06:55.5602 UTC gradient_boosted_trees.cc:1556] \tnum-trees:217 train-loss:6.783486 train-rmse:6.783486 valid-loss:8.035463 valid-rmse:8.035463\n",
            "[INFO 23-11-05 07:07:28.4168 UTC gradient_boosted_trees.cc:1556] \tnum-trees:129 train-loss:6.766707 train-rmse:6.766707 valid-loss:7.924688 valid-rmse:7.924688\n",
            "[INFO 23-11-05 07:08:00.3444 UTC gradient_boosted_trees.cc:1556] \tnum-trees:221 train-loss:6.764220 train-rmse:6.764220 valid-loss:8.038926 valid-rmse:8.038926\n",
            "[INFO 23-11-05 07:08:30.5266 UTC gradient_boosted_trees.cc:1556] \tnum-trees:148 train-loss:6.752144 train-rmse:6.752144 valid-loss:7.981067 valid-rmse:7.981067\n",
            "[INFO 23-11-05 07:09:01.3129 UTC gradient_boosted_trees.cc:1556] \tnum-trees:225 train-loss:6.748671 train-rmse:6.748671 valid-loss:8.031178 valid-rmse:8.031178\n",
            "[INFO 23-11-05 07:09:34.0332 UTC gradient_boosted_trees.cc:1556] \tnum-trees:135 train-loss:6.913057 train-rmse:6.913057 valid-loss:8.009214 valid-rmse:8.009214\n",
            "[INFO 23-11-05 07:10:04.9064 UTC gradient_boosted_trees.cc:1556] \tnum-trees:136 train-loss:6.909458 train-rmse:6.909458 valid-loss:8.010049 valid-rmse:8.010049\n",
            "[INFO 23-11-05 07:10:35.8964 UTC gradient_boosted_trees.cc:1556] \tnum-trees:231 train-loss:6.718803 train-rmse:6.718803 valid-loss:8.029614 valid-rmse:8.029614\n",
            "[INFO 23-11-05 07:11:08.6416 UTC gradient_boosted_trees.cc:1556] \tnum-trees:284 train-loss:7.097844 train-rmse:7.097844 valid-loss:8.035198 valid-rmse:8.035198\n",
            "[INFO 23-11-05 07:11:39.5766 UTC gradient_boosted_trees.cc:1556] \tnum-trees:285 train-loss:7.094249 train-rmse:7.094249 valid-loss:8.035138 valid-rmse:8.035138\n",
            "[INFO 23-11-05 07:12:09.7125 UTC gradient_boosted_trees.cc:1556] \tnum-trees:286 train-loss:7.090583 train-rmse:7.090583 valid-loss:8.033807 valid-rmse:8.033807\n",
            "[INFO 23-11-05 07:12:40.3310 UTC gradient_boosted_trees.cc:1556] \tnum-trees:287 train-loss:7.086514 train-rmse:7.086514 valid-loss:8.032614 valid-rmse:8.032614\n",
            "[INFO 23-11-05 07:13:10.9361 UTC gradient_boosted_trees.cc:1556] \tnum-trees:288 train-loss:7.083514 train-rmse:7.083514 valid-loss:8.031615 valid-rmse:8.031615\n",
            "[INFO 23-11-05 07:13:41.4306 UTC gradient_boosted_trees.cc:1556] \tnum-trees:289 train-loss:7.080747 train-rmse:7.080747 valid-loss:8.031067 valid-rmse:8.031067\n",
            "[INFO 23-11-05 07:14:11.7387 UTC gradient_boosted_trees.cc:1556] \tnum-trees:290 train-loss:7.078623 train-rmse:7.078623 valid-loss:8.029020 valid-rmse:8.029020\n",
            "[INFO 23-11-05 07:14:41.7875 UTC gradient_boosted_trees.cc:1556] \tnum-trees:291 train-loss:7.075275 train-rmse:7.075275 valid-loss:8.028710 valid-rmse:8.028710\n",
            "[INFO 23-11-05 07:15:12.3087 UTC gradient_boosted_trees.cc:1556] \tnum-trees:146 train-loss:6.846695 train-rmse:6.846695 valid-loss:8.001428 valid-rmse:8.001428\n",
            "[INFO 23-11-05 07:15:43.0299 UTC gradient_boosted_trees.cc:1556] \tnum-trees:250 train-loss:6.625829 train-rmse:6.625829 valid-loss:8.003016 valid-rmse:8.003016\n",
            "[INFO 23-11-05 07:16:14.3956 UTC gradient_boosted_trees.cc:1556] \tnum-trees:148 train-loss:6.838121 train-rmse:6.838121 valid-loss:8.002296 valid-rmse:8.002296\n",
            "[INFO 23-11-05 07:16:45.4346 UTC gradient_boosted_trees.cc:1556] \tnum-trees:149 train-loss:6.831692 train-rmse:6.831692 valid-loss:7.998713 valid-rmse:7.998713\n",
            "[INFO 23-11-05 07:17:15.4541 UTC gradient_boosted_trees.cc:1556] \tnum-trees:267 train-loss:6.163476 train-rmse:6.163476 valid-loss:7.867385 valid-rmse:7.867385\n",
            "[INFO 23-11-05 07:17:46.5249 UTC gradient_boosted_trees.cc:1556] \tnum-trees:151 train-loss:6.814469 train-rmse:6.814469 valid-loss:7.994173 valid-rmse:7.994173\n",
            "[INFO 23-11-05 07:18:16.8680 UTC gradient_boosted_trees.cc:1556] \tnum-trees:152 train-loss:6.808103 train-rmse:6.808103 valid-loss:7.992569 valid-rmse:7.992569\n",
            "[INFO 23-11-05 07:18:47.3526 UTC gradient_boosted_trees.cc:1556] \tnum-trees:153 train-loss:6.800075 train-rmse:6.800075 valid-loss:7.992919 valid-rmse:7.992919\n",
            "[INFO 23-11-05 07:18:51.8835 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.727244 train-rmse:6.727244 valid-loss:7.934773 valid-rmse:7.934773\n",
            "[INFO 23-11-05 07:18:51.8835 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 07:18:51.8835 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.934773 valid-rmse:7.934773\n",
            "[INFO 23-11-05 07:18:51.8869 UTC hyperparameters_optimizer.cc:582] [92/100] Score: -7.93477 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 2 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 128 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 07:19:17.5293 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.050493 train-rmse:7.050493 valid-loss:8.021335 valid-rmse:8.021335\n",
            "[INFO 23-11-05 07:19:17.5293 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 07:19:17.5293 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.021335 valid-rmse:8.021335\n",
            "[INFO 23-11-05 07:19:17.5364 UTC hyperparameters_optimizer.cc:582] [93/100] Score: -8.02133 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 64 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 07:19:17.7954 UTC gradient_boosted_trees.cc:1556] \tnum-trees:154 train-loss:6.795056 train-rmse:6.795056 valid-loss:7.991995 valid-rmse:7.991995\n",
            "[INFO 23-11-05 07:19:48.7438 UTC gradient_boosted_trees.cc:1556] \tnum-trees:118 train-loss:7.252167 train-rmse:7.252167 valid-loss:8.052771 valid-rmse:8.052771\n",
            "[INFO 23-11-05 07:20:19.4571 UTC gradient_boosted_trees.cc:1556] \tnum-trees:170 train-loss:6.449936 train-rmse:6.449936 valid-loss:7.851914 valid-rmse:7.851914\n",
            "[INFO 23-11-05 07:20:56.8450 UTC gradient_boosted_trees.cc:1556] \tnum-trees:276 train-loss:6.109113 train-rmse:6.109113 valid-loss:7.853361 valid-rmse:7.853361\n",
            "[INFO 23-11-05 07:21:33.5773 UTC gradient_boosted_trees.cc:1556] \tnum-trees:174 train-loss:6.419229 train-rmse:6.419229 valid-loss:7.845610 valid-rmse:7.845610\n",
            "[INFO 23-11-05 07:22:07.6027 UTC gradient_boosted_trees.cc:1556] \tnum-trees:107 train-loss:7.766141 train-rmse:7.766141 valid-loss:8.234014 valid-rmse:8.234014\n",
            "[INFO 23-11-05 07:22:42.8852 UTC gradient_boosted_trees.cc:1556] \tnum-trees:172 train-loss:6.610643 train-rmse:6.610643 valid-loss:7.940870 valid-rmse:7.940870\n",
            "[INFO 23-11-05 07:23:16.1914 UTC gradient_boosted_trees.cc:1556] \tnum-trees:162 train-loss:6.746993 train-rmse:6.746993 valid-loss:7.983495 valid-rmse:7.983495\n",
            "[INFO 23-11-05 07:23:46.5884 UTC gradient_boosted_trees.cc:1556] \tnum-trees:163 train-loss:6.743496 train-rmse:6.743496 valid-loss:7.982607 valid-rmse:7.982607\n",
            "[INFO 23-11-05 07:24:16.5988 UTC gradient_boosted_trees.cc:1556] \tnum-trees:128 train-loss:7.194457 train-rmse:7.194457 valid-loss:8.034483 valid-rmse:8.034483\n",
            "[INFO 23-11-05 07:24:46.7822 UTC gradient_boosted_trees.cc:1556] \tnum-trees:165 train-loss:6.737367 train-rmse:6.737367 valid-loss:7.982820 valid-rmse:7.982820\n",
            "[INFO 23-11-05 07:25:16.7995 UTC gradient_boosted_trees.cc:1556] \tnum-trees:166 train-loss:6.730335 train-rmse:6.730335 valid-loss:7.979074 valid-rmse:7.979074\n",
            "[INFO 23-11-05 07:25:47.1322 UTC gradient_boosted_trees.cc:1556] \tnum-trees:288 train-loss:6.046166 train-rmse:6.046166 valid-loss:7.853892 valid-rmse:7.853892\n",
            "[INFO 23-11-05 07:26:17.2123 UTC gradient_boosted_trees.cc:1556] \tnum-trees:289 train-loss:6.464673 train-rmse:6.464673 valid-loss:7.980452 valid-rmse:7.980452\n",
            "[INFO 23-11-05 07:26:48.5755 UTC gradient_boosted_trees.cc:1556] \tnum-trees:291 train-loss:6.455353 train-rmse:6.455353 valid-loss:7.980701 valid-rmse:7.980701\n",
            "[INFO 23-11-05 07:27:20.8961 UTC gradient_boosted_trees.cc:1556] \tnum-trees:180 train-loss:6.566644 train-rmse:6.566644 valid-loss:7.932638 valid-rmse:7.932638\n",
            "[INFO 23-11-05 07:27:55.0222 UTC gradient_boosted_trees.cc:1556] \tnum-trees:195 train-loss:6.270743 train-rmse:6.270743 valid-loss:7.811704 valid-rmse:7.811704\n",
            "[INFO 23-11-05 07:28:31.5570 UTC gradient_boosted_trees.cc:1556] \tnum-trees:182 train-loss:6.558206 train-rmse:6.558206 valid-loss:7.930763 valid-rmse:7.930763\n",
            "[INFO 23-11-05 07:29:06.7264 UTC gradient_boosted_trees.cc:1556] \tnum-trees:183 train-loss:6.552200 train-rmse:6.552200 valid-loss:7.930313 valid-rmse:7.930313\n",
            "[INFO 23-11-05 07:29:14.2364 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.420773 train-rmse:6.420773 valid-loss:7.981367 valid-rmse:7.981367\n",
            "[INFO 23-11-05 07:29:14.2364 UTC gradient_boosted_trees.cc:249] Truncates the model to 294 tree(s) i.e. 294  iteration(s).\n",
            "[INFO 23-11-05 07:29:14.2365 UTC gradient_boosted_trees.cc:312] Final model num-trees:294 valid-loss:7.976542 valid-rmse:7.976542\n",
            "[INFO 23-11-05 07:29:14.2389 UTC hyperparameters_optimizer.cc:582] [94/100] Score: -7.97654 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 16 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.6 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.2 } }\n",
            "[INFO 23-11-05 07:29:38.2948 UTC gradient_boosted_trees.cc:1556] \tnum-trees:140 train-loss:7.118701 train-rmse:7.118701 valid-loss:8.015758 valid-rmse:8.015758\n",
            "[INFO 23-11-05 07:30:11.8789 UTC gradient_boosted_trees.cc:1556] \tnum-trees:299 train-loss:5.977550 train-rmse:5.977550 valid-loss:7.848943 valid-rmse:7.848943\n",
            "[INFO 23-11-05 07:30:36.4046 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.973723 train-rmse:5.973723 valid-loss:7.847418 valid-rmse:7.847418\n",
            "[INFO 23-11-05 07:30:36.4047 UTC gradient_boosted_trees.cc:249] Truncates the model to 296 tree(s) i.e. 296  iteration(s).\n",
            "[INFO 23-11-05 07:30:36.4047 UTC gradient_boosted_trees.cc:312] Final model num-trees:296 valid-loss:7.846816 valid-rmse:7.846816\n",
            "[INFO 23-11-05 07:30:36.4079 UTC hyperparameters_optimizer.cc:582] [95/100] Score: -7.84682 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 3 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"CONTINUOUS\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 256 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 07:30:44.7968 UTC gradient_boosted_trees.cc:1556] \tnum-trees:177 train-loss:6.680117 train-rmse:6.680117 valid-loss:7.974692 valid-rmse:7.974692\n",
            "[INFO 23-11-05 07:31:18.3781 UTC gradient_boosted_trees.cc:1556] \tnum-trees:128 train-loss:7.672060 train-rmse:7.672060 valid-loss:8.195692 valid-rmse:8.195692\n",
            "[INFO 23-11-05 07:31:49.2423 UTC gradient_boosted_trees.cc:1556] \tnum-trees:145 train-loss:7.086779 train-rmse:7.086779 valid-loss:8.006752 valid-rmse:8.006752\n",
            "[INFO 23-11-05 07:32:24.0049 UTC gradient_boosted_trees.cc:1556] \tnum-trees:210 train-loss:6.171829 train-rmse:6.171829 valid-loss:7.801157 valid-rmse:7.801157\n",
            "[INFO 23-11-05 07:32:59.3642 UTC gradient_boosted_trees.cc:1556] \tnum-trees:212 train-loss:6.161187 train-rmse:6.161187 valid-loss:7.799826 valid-rmse:7.799826\n",
            "[INFO 23-11-05 07:33:32.8229 UTC gradient_boosted_trees.cc:1556] \tnum-trees:149 train-loss:7.063518 train-rmse:7.063518 valid-loss:7.999190 valid-rmse:7.999190\n",
            "[INFO 23-11-05 07:34:07.7351 UTC gradient_boosted_trees.cc:1556] \tnum-trees:184 train-loss:6.641504 train-rmse:6.641504 valid-loss:7.968300 valid-rmse:7.968300\n",
            "[INFO 23-11-05 07:34:44.8632 UTC gradient_boosted_trees.cc:1556] \tnum-trees:218 train-loss:6.131056 train-rmse:6.131056 valid-loss:7.792939 valid-rmse:7.792939\n",
            "[INFO 23-11-05 07:35:18.0397 UTC gradient_boosted_trees.cc:1556] \tnum-trees:153 train-loss:7.045463 train-rmse:7.045463 valid-loss:7.995343 valid-rmse:7.995343\n",
            "[INFO 23-11-05 07:35:54.3212 UTC gradient_boosted_trees.cc:1556] \tnum-trees:222 train-loss:6.106553 train-rmse:6.106553 valid-loss:7.787595 valid-rmse:7.787595\n",
            "[INFO 23-11-05 07:36:28.0840 UTC gradient_boosted_trees.cc:1556] \tnum-trees:196 train-loss:6.477740 train-rmse:6.477740 valid-loss:7.911978 valid-rmse:7.911978\n",
            "[INFO 23-11-05 07:37:00.9351 UTC gradient_boosted_trees.cc:1556] \tnum-trees:190 train-loss:6.597621 train-rmse:6.597621 valid-loss:7.955661 valid-rmse:7.955661\n",
            "[INFO 23-11-05 07:37:36.4676 UTC gradient_boosted_trees.cc:1556] \tnum-trees:198 train-loss:6.466508 train-rmse:6.466508 valid-loss:7.906956 valid-rmse:7.906956\n",
            "[INFO 23-11-05 07:38:10.2666 UTC gradient_boosted_trees.cc:1556] \tnum-trees:199 train-loss:6.463591 train-rmse:6.463591 valid-loss:7.904960 valid-rmse:7.904960\n",
            "[INFO 23-11-05 07:38:44.3595 UTC gradient_boosted_trees.cc:1556] \tnum-trees:200 train-loss:6.459511 train-rmse:6.459511 valid-loss:7.903473 valid-rmse:7.903473\n",
            "[INFO 23-11-05 07:39:18.2756 UTC gradient_boosted_trees.cc:1556] \tnum-trees:201 train-loss:6.456201 train-rmse:6.456201 valid-loss:7.902664 valid-rmse:7.902664\n",
            "[INFO 23-11-05 07:39:51.4918 UTC gradient_boosted_trees.cc:1556] \tnum-trees:148 train-loss:7.593227 train-rmse:7.593227 valid-loss:8.167061 valid-rmse:8.167061\n",
            "[INFO 23-11-05 07:40:22.9601 UTC gradient_boosted_trees.cc:1556] \tnum-trees:197 train-loss:6.559033 train-rmse:6.559033 valid-loss:7.947363 valid-rmse:7.947363\n",
            "[INFO 23-11-05 07:40:54.3075 UTC gradient_boosted_trees.cc:1556] \tnum-trees:166 train-loss:6.988035 train-rmse:6.988035 valid-loss:7.978551 valid-rmse:7.978551\n",
            "[INFO 23-11-05 07:41:28.3124 UTC gradient_boosted_trees.cc:1556] \tnum-trees:241 train-loss:5.987906 train-rmse:5.987906 valid-loss:7.771064 valid-rmse:7.771064\n",
            "[INFO 23-11-05 07:42:00.0252 UTC gradient_boosted_trees.cc:1556] \tnum-trees:153 train-loss:7.573415 train-rmse:7.573415 valid-loss:8.162055 valid-rmse:8.162055\n",
            "[INFO 23-11-05 07:42:36.4009 UTC gradient_boosted_trees.cc:1556] \tnum-trees:170 train-loss:6.959160 train-rmse:6.959160 valid-loss:7.967639 valid-rmse:7.967639\n",
            "[INFO 23-11-05 07:43:13.5638 UTC gradient_boosted_trees.cc:1556] \tnum-trees:208 train-loss:6.420582 train-rmse:6.420582 valid-loss:7.891399 valid-rmse:7.891399\n",
            "[INFO 23-11-05 07:43:44.1731 UTC gradient_boosted_trees.cc:1556] \tnum-trees:204 train-loss:6.513900 train-rmse:6.513900 valid-loss:7.939099 valid-rmse:7.939099\n",
            "[INFO 23-11-05 07:44:19.3293 UTC gradient_boosted_trees.cc:1556] \tnum-trees:174 train-loss:6.934518 train-rmse:6.934518 valid-loss:7.966637 valid-rmse:7.966637\n",
            "[INFO 23-11-05 07:44:54.0042 UTC gradient_boosted_trees.cc:1556] \tnum-trees:211 train-loss:6.405568 train-rmse:6.405568 valid-loss:7.886365 valid-rmse:7.886365\n",
            "[INFO 23-11-05 07:45:28.3233 UTC gradient_boosted_trees.cc:1556] \tnum-trees:212 train-loss:6.403233 train-rmse:6.403233 valid-loss:7.886162 valid-rmse:7.886162\n",
            "[INFO 23-11-05 07:46:01.6367 UTC gradient_boosted_trees.cc:1556] \tnum-trees:178 train-loss:6.914152 train-rmse:6.914152 valid-loss:7.960634 valid-rmse:7.960634\n",
            "[INFO 23-11-05 07:46:35.6980 UTC gradient_boosted_trees.cc:1556] \tnum-trees:214 train-loss:6.392338 train-rmse:6.392338 valid-loss:7.884343 valid-rmse:7.884343\n",
            "[INFO 23-11-05 07:47:05.9667 UTC gradient_boosted_trees.cc:1556] \tnum-trees:211 train-loss:6.485573 train-rmse:6.485573 valid-loss:7.931601 valid-rmse:7.931601\n",
            "[INFO 23-11-05 07:47:38.9657 UTC gradient_boosted_trees.cc:1556] \tnum-trees:262 train-loss:5.864971 train-rmse:5.864971 valid-loss:7.762889 valid-rmse:7.762889\n",
            "[INFO 23-11-05 07:48:11.0245 UTC gradient_boosted_trees.cc:1556] \tnum-trees:183 train-loss:6.882919 train-rmse:6.882919 valid-loss:7.946079 valid-rmse:7.946079\n",
            "[INFO 23-11-05 07:48:46.8949 UTC gradient_boosted_trees.cc:1556] \tnum-trees:169 train-loss:7.505727 train-rmse:7.505727 valid-loss:8.138982 valid-rmse:8.138982\n",
            "[INFO 23-11-05 07:49:22.5640 UTC gradient_boosted_trees.cc:1556] \tnum-trees:219 train-loss:6.363720 train-rmse:6.363720 valid-loss:7.879610 valid-rmse:7.879610\n",
            "[INFO 23-11-05 07:49:53.3956 UTC gradient_boosted_trees.cc:1556] \tnum-trees:187 train-loss:6.864790 train-rmse:6.864790 valid-loss:7.939760 valid-rmse:7.939760\n",
            "[INFO 23-11-05 07:50:26.8938 UTC gradient_boosted_trees.cc:1556] \tnum-trees:218 train-loss:6.452744 train-rmse:6.452744 valid-loss:7.925933 valid-rmse:7.925933\n",
            "[INFO 23-11-05 07:51:03.3458 UTC gradient_boosted_trees.cc:1556] \tnum-trees:222 train-loss:6.350306 train-rmse:6.350306 valid-loss:7.875251 valid-rmse:7.875251\n",
            "[INFO 23-11-05 07:51:36.2096 UTC gradient_boosted_trees.cc:1556] \tnum-trees:191 train-loss:6.840673 train-rmse:6.840673 valid-loss:7.929832 valid-rmse:7.929832\n",
            "[INFO 23-11-05 07:52:11.2517 UTC gradient_boosted_trees.cc:1556] \tnum-trees:224 train-loss:6.341189 train-rmse:6.341189 valid-loss:7.874118 valid-rmse:7.874118\n",
            "[INFO 23-11-05 07:52:41.5369 UTC gradient_boosted_trees.cc:1556] \tnum-trees:279 train-loss:5.784766 train-rmse:5.784766 valid-loss:7.750911 valid-rmse:7.750911\n",
            "[INFO 23-11-05 07:53:16.8415 UTC gradient_boosted_trees.cc:1556] \tnum-trees:281 train-loss:5.776555 train-rmse:5.776555 valid-loss:7.749959 valid-rmse:7.749959\n",
            "[INFO 23-11-05 07:53:48.6560 UTC gradient_boosted_trees.cc:1556] \tnum-trees:225 train-loss:6.409433 train-rmse:6.409433 valid-loss:7.919470 valid-rmse:7.919470\n",
            "[INFO 23-11-05 07:54:18.7518 UTC gradient_boosted_trees.cc:1556] \tnum-trees:182 train-loss:7.458169 train-rmse:7.458169 valid-loss:8.125706 valid-rmse:8.125706\n",
            "[INFO 23-11-05 07:54:58.7856 UTC gradient_boosted_trees.cc:1556] \tnum-trees:229 train-loss:6.310896 train-rmse:6.310896 valid-loss:7.862151 valid-rmse:7.862151\n",
            "[INFO 23-11-05 07:55:28.9405 UTC gradient_boosted_trees.cc:1556] \tnum-trees:200 train-loss:6.799223 train-rmse:6.799223 valid-loss:7.912534 valid-rmse:7.912534\n",
            "[INFO 23-11-05 07:56:00.2403 UTC gradient_boosted_trees.cc:1556] \tnum-trees:186 train-loss:7.441478 train-rmse:7.441478 valid-loss:8.120943 valid-rmse:8.120943\n",
            "[INFO 23-11-05 07:56:31.7506 UTC gradient_boosted_trees.cc:1556] \tnum-trees:292 train-loss:5.723362 train-rmse:5.723362 valid-loss:7.748451 valid-rmse:7.748451\n",
            "[INFO 23-11-05 07:57:07.4360 UTC gradient_boosted_trees.cc:1556] \tnum-trees:294 train-loss:5.710974 train-rmse:5.710974 valid-loss:7.744637 valid-rmse:7.744637\n",
            "[INFO 23-11-05 07:57:37.7453 UTC gradient_boosted_trees.cc:1556] \tnum-trees:205 train-loss:6.776089 train-rmse:6.776089 valid-loss:7.901628 valid-rmse:7.901628\n",
            "[INFO 23-11-05 07:58:09.3517 UTC gradient_boosted_trees.cc:1556] \tnum-trees:191 train-loss:7.420166 train-rmse:7.420166 valid-loss:8.109884 valid-rmse:8.109884\n",
            "[INFO 23-11-05 07:58:51.4923 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.671821 train-rmse:5.671821 valid-loss:7.739028 valid-rmse:7.739028\n",
            "[INFO 23-11-05 07:58:51.4923 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 07:58:51.4923 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.739028 valid-rmse:7.739028\n",
            "[INFO 23-11-05 07:58:51.4976 UTC hyperparameters_optimizer.cc:582] [96/100] Score: -7.73903 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 1 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"NONE\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 256 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.8 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 7 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 07:58:53.2920 UTC gradient_boosted_trees.cc:1556] \tnum-trees:236 train-loss:6.276385 train-rmse:6.276385 valid-loss:7.852783 valid-rmse:7.852783\n",
            "[INFO 23-11-05 07:59:25.7479 UTC gradient_boosted_trees.cc:1556] \tnum-trees:194 train-loss:7.408421 train-rmse:7.408421 valid-loss:8.105579 valid-rmse:8.105579\n",
            "[INFO 23-11-05 07:59:59.6726 UTC gradient_boosted_trees.cc:1556] \tnum-trees:238 train-loss:6.262218 train-rmse:6.262218 valid-loss:7.849990 valid-rmse:7.849990\n",
            "[INFO 23-11-05 08:00:32.3490 UTC gradient_boosted_trees.cc:1556] \tnum-trees:239 train-loss:6.340579 train-rmse:6.340579 valid-loss:7.898780 valid-rmse:7.898780\n",
            "[INFO 23-11-05 08:01:06.5783 UTC gradient_boosted_trees.cc:1556] \tnum-trees:240 train-loss:6.255676 train-rmse:6.255676 valid-loss:7.850399 valid-rmse:7.850399\n",
            "[INFO 23-11-05 08:01:40.2419 UTC gradient_boosted_trees.cc:1556] \tnum-trees:241 train-loss:6.252152 train-rmse:6.252152 valid-loss:7.850762 valid-rmse:7.850762\n",
            "[INFO 23-11-05 08:02:13.6275 UTC gradient_boosted_trees.cc:1556] \tnum-trees:242 train-loss:6.248255 train-rmse:6.248255 valid-loss:7.848566 valid-rmse:7.848566\n",
            "[INFO 23-11-05 08:02:46.0959 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:6.241940 train-rmse:6.241940 valid-loss:7.846429 valid-rmse:7.846429\n",
            "[INFO 23-11-05 08:03:19.7455 UTC gradient_boosted_trees.cc:1556] \tnum-trees:244 train-loss:6.237361 train-rmse:6.237361 valid-loss:7.844880 valid-rmse:7.844880\n",
            "[INFO 23-11-05 08:03:52.0121 UTC gradient_boosted_trees.cc:1556] \tnum-trees:245 train-loss:6.230732 train-rmse:6.230732 valid-loss:7.841613 valid-rmse:7.841613\n",
            "[INFO 23-11-05 08:04:24.5063 UTC gradient_boosted_trees.cc:1556] \tnum-trees:221 train-loss:6.691485 train-rmse:6.691485 valid-loss:7.880099 valid-rmse:7.880099\n",
            "[INFO 23-11-05 08:04:55.5441 UTC gradient_boosted_trees.cc:1556] \tnum-trees:207 train-loss:7.360445 train-rmse:7.360445 valid-loss:8.092504 valid-rmse:8.092504\n",
            "[INFO 23-11-05 08:05:32.8476 UTC gradient_boosted_trees.cc:1556] \tnum-trees:248 train-loss:6.219827 train-rmse:6.219827 valid-loss:7.839559 valid-rmse:7.839559\n",
            "[INFO 23-11-05 08:06:06.6353 UTC gradient_boosted_trees.cc:1556] \tnum-trees:249 train-loss:6.216958 train-rmse:6.216958 valid-loss:7.838218 valid-rmse:7.838218\n",
            "[INFO 23-11-05 08:06:38.4818 UTC gradient_boosted_trees.cc:1556] \tnum-trees:211 train-loss:7.350997 train-rmse:7.350997 valid-loss:8.088647 valid-rmse:8.088647\n",
            "[INFO 23-11-05 08:07:12.3423 UTC gradient_boosted_trees.cc:1556] \tnum-trees:253 train-loss:6.269238 train-rmse:6.269238 valid-loss:7.888936 valid-rmse:7.888936\n",
            "[INFO 23-11-05 08:07:46.5639 UTC gradient_boosted_trees.cc:1556] \tnum-trees:252 train-loss:6.202825 train-rmse:6.202825 valid-loss:7.837812 valid-rmse:7.837812\n",
            "[INFO 23-11-05 08:08:19.4480 UTC gradient_boosted_trees.cc:1556] \tnum-trees:215 train-loss:7.336285 train-rmse:7.336285 valid-loss:8.083400 valid-rmse:8.083400\n",
            "[INFO 23-11-05 08:08:52.3455 UTC gradient_boosted_trees.cc:1556] \tnum-trees:254 train-loss:6.190774 train-rmse:6.190774 valid-loss:7.839819 valid-rmse:7.839819\n",
            "[INFO 23-11-05 08:09:25.7308 UTC gradient_boosted_trees.cc:1556] \tnum-trees:255 train-loss:6.187209 train-rmse:6.187209 valid-loss:7.838996 valid-rmse:7.838996\n",
            "[INFO 23-11-05 08:09:58.0084 UTC gradient_boosted_trees.cc:1556] \tnum-trees:234 train-loss:6.629320 train-rmse:6.629320 valid-loss:7.866060 valid-rmse:7.866060\n",
            "[INFO 23-11-05 08:10:31.9670 UTC gradient_boosted_trees.cc:1556] \tnum-trees:257 train-loss:6.177132 train-rmse:6.177132 valid-loss:7.840936 valid-rmse:7.840936\n",
            "[INFO 23-11-05 08:11:01.9834 UTC gradient_boosted_trees.cc:1556] \tnum-trees:261 train-loss:6.214302 train-rmse:6.214302 valid-loss:7.884072 valid-rmse:7.884072\n",
            "[INFO 23-11-05 08:11:37.6079 UTC gradient_boosted_trees.cc:1556] \tnum-trees:259 train-loss:6.162689 train-rmse:6.162689 valid-loss:7.839845 valid-rmse:7.839845\n",
            "[INFO 23-11-05 08:12:08.4918 UTC gradient_boosted_trees.cc:1556] \tnum-trees:224 train-loss:7.307900 train-rmse:7.307900 valid-loss:8.081149 valid-rmse:8.081149\n",
            "[INFO 23-11-05 08:12:44.6163 UTC gradient_boosted_trees.cc:1556] \tnum-trees:261 train-loss:6.154661 train-rmse:6.154661 valid-loss:7.838141 valid-rmse:7.838141\n",
            "[INFO 23-11-05 08:13:18.0155 UTC gradient_boosted_trees.cc:1556] \tnum-trees:262 train-loss:6.147073 train-rmse:6.147073 valid-loss:7.835088 valid-rmse:7.835088\n",
            "[INFO 23-11-05 08:13:48.9210 UTC gradient_boosted_trees.cc:1556] \tnum-trees:243 train-loss:6.594045 train-rmse:6.594045 valid-loss:7.848538 valid-rmse:7.848538\n",
            "[INFO 23-11-05 08:14:19.9419 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:6.188348 train-rmse:6.188348 valid-loss:7.885896 valid-rmse:7.885896\n",
            "[INFO 23-11-05 08:14:57.4140 UTC gradient_boosted_trees.cc:1556] \tnum-trees:265 train-loss:6.130027 train-rmse:6.130027 valid-loss:7.828662 valid-rmse:7.828662\n",
            "[INFO 23-11-05 08:15:30.3515 UTC gradient_boosted_trees.cc:1556] \tnum-trees:266 train-loss:6.124857 train-rmse:6.124857 valid-loss:7.827500 valid-rmse:7.827500\n",
            "[INFO 23-11-05 08:16:03.8127 UTC gradient_boosted_trees.cc:1556] \tnum-trees:267 train-loss:6.121378 train-rmse:6.121378 valid-loss:7.827008 valid-rmse:7.827008\n",
            "[INFO 23-11-05 08:16:36.8133 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:6.116344 train-rmse:6.116344 valid-loss:7.827219 valid-rmse:7.827219\n",
            "[INFO 23-11-05 08:17:10.0260 UTC gradient_boosted_trees.cc:1556] \tnum-trees:251 train-loss:6.554801 train-rmse:6.554801 valid-loss:7.848248 valid-rmse:7.848248\n",
            "[INFO 23-11-05 08:17:40.9253 UTC gradient_boosted_trees.cc:1556] \tnum-trees:275 train-loss:6.160281 train-rmse:6.160281 valid-loss:7.885889 valid-rmse:7.885889\n",
            "[INFO 23-11-05 08:18:17.6829 UTC gradient_boosted_trees.cc:1556] \tnum-trees:271 train-loss:6.107684 train-rmse:6.107684 valid-loss:7.824965 valid-rmse:7.824965\n",
            "[INFO 23-11-05 08:18:49.5608 UTC gradient_boosted_trees.cc:1556] \tnum-trees:272 train-loss:6.096297 train-rmse:6.096297 valid-loss:7.821963 valid-rmse:7.821963\n",
            "[INFO 23-11-05 08:19:23.0243 UTC gradient_boosted_trees.cc:1556] \tnum-trees:241 train-loss:7.255210 train-rmse:7.255210 valid-loss:8.064854 valid-rmse:8.064854\n",
            "[INFO 23-11-05 08:19:56.6442 UTC gradient_boosted_trees.cc:1556] \tnum-trees:274 train-loss:6.089508 train-rmse:6.089508 valid-loss:7.819539 valid-rmse:7.819539\n",
            "[INFO 23-11-05 08:20:29.3074 UTC gradient_boosted_trees.cc:1556] \tnum-trees:275 train-loss:6.082146 train-rmse:6.082146 valid-loss:7.816546 valid-rmse:7.816546\n",
            "[INFO 23-11-05 08:21:01.1328 UTC gradient_boosted_trees.cc:1556] \tnum-trees:282 train-loss:6.126659 train-rmse:6.126659 valid-loss:7.888696 valid-rmse:7.888696\n",
            "[INFO 23-11-05 08:21:31.5230 UTC gradient_boosted_trees.cc:1556] \tnum-trees:246 train-loss:7.240825 train-rmse:7.240825 valid-loss:8.064194 valid-rmse:8.064194\n",
            "[INFO 23-11-05 08:22:09.5924 UTC gradient_boosted_trees.cc:1556] \tnum-trees:278 train-loss:6.070955 train-rmse:6.070955 valid-loss:7.814819 valid-rmse:7.814819\n",
            "[INFO 23-11-05 08:22:42.6505 UTC gradient_boosted_trees.cc:1556] \tnum-trees:279 train-loss:6.058773 train-rmse:6.058773 valid-loss:7.812850 valid-rmse:7.812850\n",
            "[INFO 23-11-05 08:23:14.3491 UTC gradient_boosted_trees.cc:1556] \tnum-trees:250 train-loss:7.230274 train-rmse:7.230274 valid-loss:8.063460 valid-rmse:8.063460\n",
            "[INFO 23-11-05 08:23:49.0384 UTC gradient_boosted_trees.cc:1556] \tnum-trees:281 train-loss:6.047212 train-rmse:6.047212 valid-loss:7.812780 valid-rmse:7.812780\n",
            "[INFO 23-11-05 08:24:19.8957 UTC gradient_boosted_trees.cc:1556] \tnum-trees:268 train-loss:6.471385 train-rmse:6.471385 valid-loss:7.828292 valid-rmse:7.828292\n",
            "[INFO 23-11-05 08:24:51.9080 UTC gradient_boosted_trees.cc:1556] \tnum-trees:290 train-loss:6.096887 train-rmse:6.096887 valid-loss:7.882696 valid-rmse:7.882696\n",
            "[INFO 23-11-05 08:25:22.5201 UTC gradient_boosted_trees.cc:1556] \tnum-trees:255 train-loss:7.218428 train-rmse:7.218428 valid-loss:8.061156 valid-rmse:8.061156\n",
            "[INFO 23-11-05 08:26:00.3959 UTC gradient_boosted_trees.cc:1556] \tnum-trees:272 train-loss:6.447595 train-rmse:6.447595 valid-loss:7.822343 valid-rmse:7.822343\n",
            "[INFO 23-11-05 08:26:34.3723 UTC gradient_boosted_trees.cc:1556] \tnum-trees:286 train-loss:6.023711 train-rmse:6.023711 valid-loss:7.814008 valid-rmse:7.814008\n",
            "[INFO 23-11-05 08:27:07.5523 UTC gradient_boosted_trees.cc:1556] \tnum-trees:287 train-loss:6.018834 train-rmse:6.018834 valid-loss:7.813818 valid-rmse:7.813818\n",
            "[INFO 23-11-05 08:27:41.6681 UTC gradient_boosted_trees.cc:1556] \tnum-trees:288 train-loss:6.016012 train-rmse:6.016012 valid-loss:7.814184 valid-rmse:7.814184\n",
            "[INFO 23-11-05 08:28:13.2675 UTC gradient_boosted_trees.cc:1556] \tnum-trees:297 train-loss:6.067943 train-rmse:6.067943 valid-loss:7.876851 valid-rmse:7.876851\n",
            "[INFO 23-11-05 08:28:46.4283 UTC gradient_boosted_trees.cc:1556] \tnum-trees:263 train-loss:7.194606 train-rmse:7.194606 valid-loss:8.056974 valid-rmse:8.056974\n",
            "[INFO 23-11-05 08:29:20.8149 UTC gradient_boosted_trees.cc:1556] \tnum-trees:291 train-loss:6.003946 train-rmse:6.003946 valid-loss:7.809582 valid-rmse:7.809582\n",
            "[INFO 23-11-05 08:29:39.5326 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.047782 train-rmse:6.047782 valid-loss:7.871277 valid-rmse:7.871277\n",
            "[INFO 23-11-05 08:29:39.5326 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 08:29:39.5326 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.871277 valid-rmse:7.871277\n",
            "[INFO 23-11-05 08:29:39.5358 UTC hyperparameters_optimizer.cc:582] [97/100] Score: -7.87128 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"CART\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 512 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.1 } } fields { name: \"min_examples\" value { integer: 20 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 08:29:54.2267 UTC gradient_boosted_trees.cc:1556] \tnum-trees:292 train-loss:6.000471 train-rmse:6.000471 valid-loss:7.809512 valid-rmse:7.809512\n",
            "[INFO 23-11-05 08:30:27.5643 UTC gradient_boosted_trees.cc:1556] \tnum-trees:293 train-loss:5.996933 train-rmse:5.996933 valid-loss:7.806934 valid-rmse:7.806934\n",
            "[INFO 23-11-05 08:30:59.6090 UTC gradient_boosted_trees.cc:1556] \tnum-trees:294 train-loss:5.994095 train-rmse:5.994095 valid-loss:7.806443 valid-rmse:7.806443\n",
            "[INFO 23-11-05 08:31:33.2260 UTC gradient_boosted_trees.cc:1556] \tnum-trees:295 train-loss:5.991298 train-rmse:5.991298 valid-loss:7.805785 valid-rmse:7.805785\n",
            "[INFO 23-11-05 08:32:04.8347 UTC gradient_boosted_trees.cc:1556] \tnum-trees:296 train-loss:5.987748 train-rmse:5.987748 valid-loss:7.805165 valid-rmse:7.805165\n",
            "[INFO 23-11-05 08:32:35.3042 UTC gradient_boosted_trees.cc:1556] \tnum-trees:272 train-loss:7.167313 train-rmse:7.167313 valid-loss:8.046675 valid-rmse:8.046675\n",
            "[INFO 23-11-05 08:33:07.4698 UTC gradient_boosted_trees.cc:1556] \tnum-trees:289 train-loss:6.386644 train-rmse:6.386644 valid-loss:7.810827 valid-rmse:7.810827\n",
            "[INFO 23-11-05 08:33:42.9009 UTC gradient_boosted_trees.cc:1556] \tnum-trees:299 train-loss:5.969845 train-rmse:5.969845 valid-loss:7.801034 valid-rmse:7.801034\n",
            "[INFO 23-11-05 08:34:15.4132 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:5.967024 train-rmse:5.967024 valid-loss:7.802622 valid-rmse:7.802622\n",
            "[INFO 23-11-05 08:34:15.4132 UTC gradient_boosted_trees.cc:249] Truncates the model to 299 tree(s) i.e. 299  iteration(s).\n",
            "[INFO 23-11-05 08:34:15.4132 UTC gradient_boosted_trees.cc:312] Final model num-trees:299 valid-loss:7.801034 valid-rmse:7.801034\n",
            "[INFO 23-11-05 08:34:15.4236 UTC hyperparameters_optimizer.cc:582] [98/100] Score: -7.80103 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"MIN_MAX\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"LOCAL\" } } fields { name: \"max_depth\" value { integer: 8 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 1 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.5 } }\n",
            "[INFO 23-11-05 08:34:16.3017 UTC gradient_boosted_trees.cc:1556] \tnum-trees:276 train-loss:7.154211 train-rmse:7.154211 valid-loss:8.042197 valid-rmse:8.042197\n",
            "[INFO 23-11-05 08:35:03.3997 UTC gradient_boosted_trees.cc:1556] \tnum-trees:278 train-loss:7.150176 train-rmse:7.150176 valid-loss:8.041982 valid-rmse:8.041982\n",
            "[INFO 23-11-05 08:35:48.7012 UTC gradient_boosted_trees.cc:1556] \tnum-trees:280 train-loss:7.142431 train-rmse:7.142431 valid-loss:8.039518 valid-rmse:8.039518\n",
            "[INFO 23-11-05 08:36:36.7148 UTC gradient_boosted_trees.cc:1556] \tnum-trees:282 train-loss:7.136727 train-rmse:7.136727 valid-loss:8.039752 valid-rmse:8.039752\n",
            "[INFO 23-11-05 08:37:23.9098 UTC gradient_boosted_trees.cc:1556] \tnum-trees:284 train-loss:7.133361 train-rmse:7.133361 valid-loss:8.039282 valid-rmse:8.039282\n",
            "[INFO 23-11-05 08:37:28.7554 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:6.329392 train-rmse:6.329392 valid-loss:7.799703 valid-rmse:7.799703\n",
            "[INFO 23-11-05 08:37:28.7554 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 08:37:28.7554 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:7.799703 valid-rmse:7.799703\n",
            "[INFO 23-11-05 08:37:28.7600 UTC hyperparameters_optimizer.cc:582] [99/100] Score: -7.7997 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 64 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.05 } } fields { name: \"min_examples\" value { integer: 10 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 1 } }\n",
            "[INFO 23-11-05 08:38:10.7484 UTC gradient_boosted_trees.cc:1556] \tnum-trees:286 train-loss:7.127136 train-rmse:7.127136 valid-loss:8.036821 valid-rmse:8.036821\n",
            "[INFO 23-11-05 08:38:57.4848 UTC gradient_boosted_trees.cc:1556] \tnum-trees:288 train-loss:7.121889 train-rmse:7.121889 valid-loss:8.035521 valid-rmse:8.035521\n",
            "[INFO 23-11-05 08:39:42.9986 UTC gradient_boosted_trees.cc:1556] \tnum-trees:290 train-loss:7.117258 train-rmse:7.117258 valid-loss:8.034808 valid-rmse:8.034808\n",
            "[INFO 23-11-05 08:40:28.2798 UTC gradient_boosted_trees.cc:1556] \tnum-trees:292 train-loss:7.109822 train-rmse:7.109822 valid-loss:8.034364 valid-rmse:8.034364\n",
            "[INFO 23-11-05 08:41:15.4196 UTC gradient_boosted_trees.cc:1556] \tnum-trees:294 train-loss:7.106212 train-rmse:7.106212 valid-loss:8.032327 valid-rmse:8.032327\n",
            "[INFO 23-11-05 08:42:00.8291 UTC gradient_boosted_trees.cc:1556] \tnum-trees:296 train-loss:7.101520 train-rmse:7.101520 valid-loss:8.031504 valid-rmse:8.031504\n",
            "[INFO 23-11-05 08:42:47.9043 UTC gradient_boosted_trees.cc:1556] \tnum-trees:298 train-loss:7.097484 train-rmse:7.097484 valid-loss:8.032146 valid-rmse:8.032146\n",
            "[INFO 23-11-05 08:43:34.3958 UTC gradient_boosted_trees.cc:1554] \tnum-trees:300 train-loss:7.091059 train-rmse:7.091059 valid-loss:8.029217 valid-rmse:8.029217\n",
            "[INFO 23-11-05 08:43:34.3958 UTC gradient_boosted_trees.cc:249] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
            "[INFO 23-11-05 08:43:34.3958 UTC gradient_boosted_trees.cc:312] Final model num-trees:300 valid-loss:8.029217 valid-rmse:8.029217\n",
            "[INFO 23-11-05 08:43:34.4006 UTC hyperparameters_optimizer.cc:582] [100/100] Score: -8.02922 / -7.70017 HParams: fields { name: \"split_axis\" value { categorical: \"SPARSE_OBLIQUE\" } } fields { name: \"sparse_oblique_projection_density_factor\" value { real: 5 } } fields { name: \"sparse_oblique_normalization\" value { categorical: \"STANDARD_DEVIATION\" } } fields { name: \"sparse_oblique_weights\" value { categorical: \"BINARY\" } } fields { name: \"categorical_algorithm\" value { categorical: \"RANDOM\" } } fields { name: \"growing_strategy\" value { categorical: \"BEST_FIRST_GLOBAL\" } } fields { name: \"max_num_nodes\" value { integer: 64 } } fields { name: \"sampling_method\" value { categorical: \"RANDOM\" } } fields { name: \"subsample\" value { real: 0.9 } } fields { name: \"shrinkage\" value { real: 0.02 } } fields { name: \"min_examples\" value { integer: 5 } } fields { name: \"num_candidate_attributes_ratio\" value { real: 0.9 } }\n",
            "[INFO 23-11-05 08:43:34.4347 UTC hyperparameters_optimizer.cc:219] Best hyperparameters:\n",
            "fields {\n",
            "  name: \"split_axis\"\n",
            "  value {\n",
            "    categorical: \"AXIS_ALIGNED\"\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"categorical_algorithm\"\n",
            "  value {\n",
            "    categorical: \"CART\"\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"growing_strategy\"\n",
            "  value {\n",
            "    categorical: \"LOCAL\"\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"max_depth\"\n",
            "  value {\n",
            "    integer: 8\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"sampling_method\"\n",
            "  value {\n",
            "    categorical: \"RANDOM\"\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"subsample\"\n",
            "  value {\n",
            "    real: 0.9\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"shrinkage\"\n",
            "  value {\n",
            "    real: 0.1\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"min_examples\"\n",
            "  value {\n",
            "    integer: 10\n",
            "  }\n",
            "}\n",
            "fields {\n",
            "  name: \"num_candidate_attributes_ratio\"\n",
            "  value {\n",
            "    real: 0.9\n",
            "  }\n",
            "}\n",
            "\n",
            "[INFO 23-11-05 08:43:34.4358 UTC kernel.cc:926] Export model in log directory: /tmp/tmpgcwqak7x with prefix 482dd34e134f430d\n",
            "[INFO 23-11-05 08:43:34.4592 UTC kernel.cc:944] Save model in resources\n",
            "[INFO 23-11-05 08:43:34.4946 UTC abstract_model.cc:881] Model self evaluation:\n",
            "Number of predictions (with weights): 1\n",
            "Task: REGRESSION\n",
            "Loss (SQUARED_ERROR): 7.70017\n",
            "\n",
            "RMSE: 2.77492\n",
            "Default RMSE: : 0\n",
            "\n",
            "[INFO 23-11-05 08:43:34.5251 UTC kernel.cc:1233] Loading model from path /tmp/tmpgcwqak7x/model/ with prefix 482dd34e134f430d\n",
            "[INFO 23-11-05 08:43:34.5543 UTC decision_forest.cc:660] Model loaded with 300 root(s), 26864 node(s), and 49 input feature(s).\n",
            "[INFO 23-11-05 08:43:34.5546 UTC abstract_model.cc:1343] Engine \"GradientBoostedTreesOptPred\" built\n",
            "[INFO 23-11-05 08:43:34.5546 UTC kernel.cc:1061] Use fast generic engine\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained in 15:11:36.096382\n",
            "Compiling model...\n",
            "Model compiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_test_accuracy = tuned_model.evaluate(X[index%N_fold==0][0:20000, : ], Y[index%N_fold==0][0:20000], return_dict=True, verbose=0)[\"mae\"]\n",
        "print(tuned_test_accuracy)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T06:45:40.031176Z",
          "iopub.status.idle": "2023-11-04T06:45:40.031704Z",
          "shell.execute_reply.started": "2023-11-04T06:45:40.031434Z",
          "shell.execute_reply": "2023-11-04T06:45:40.031470Z"
        },
        "trusted": true,
        "id": "KzXWsqroreL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1d5ede-fd69-4696-fafb-f27072be1e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.213750839233398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuning_logs = tuned_model.make_inspector().tuning_logs()\n",
        "tuning_logs.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "x5F7PNH0Ab52",
        "outputId": "351adcdb-645f-4af1-f270-0f5d538506d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      score  evaluation_time   best      split_axis  \\\n",
              "0 -8.218175      2467.965598  False  SPARSE_OBLIQUE   \n",
              "1 -8.143211      3189.592446  False  SPARSE_OBLIQUE   \n",
              "2 -7.892285      4346.781670  False  SPARSE_OBLIQUE   \n",
              "3 -7.911223      4396.937751  False  SPARSE_OBLIQUE   \n",
              "4 -7.945287      4816.981907  False  SPARSE_OBLIQUE   \n",
              "\n",
              "   sparse_oblique_projection_density_factor sparse_oblique_normalization  \\\n",
              "0                                       4.0           STANDARD_DEVIATION   \n",
              "1                                       5.0                      MIN_MAX   \n",
              "2                                       5.0                         NONE   \n",
              "3                                       1.0                         NONE   \n",
              "4                                       2.0                      MIN_MAX   \n",
              "\n",
              "  sparse_oblique_weights categorical_algorithm   growing_strategy  max_depth  \\\n",
              "0             CONTINUOUS                  CART              LOCAL        3.0   \n",
              "1                 BINARY                  CART  BEST_FIRST_GLOBAL        NaN   \n",
              "2             CONTINUOUS                  CART  BEST_FIRST_GLOBAL        NaN   \n",
              "3             CONTINUOUS                RANDOM  BEST_FIRST_GLOBAL        NaN   \n",
              "4                 BINARY                RANDOM  BEST_FIRST_GLOBAL        NaN   \n",
              "\n",
              "  sampling_method  subsample  shrinkage  min_examples  \\\n",
              "0          RANDOM        0.8       0.05             7   \n",
              "1          RANDOM        0.6       0.10            20   \n",
              "2          RANDOM        0.6       0.05            10   \n",
              "3          RANDOM        0.6       0.10            10   \n",
              "4          RANDOM        0.6       0.05             7   \n",
              "\n",
              "   num_candidate_attributes_ratio  max_num_nodes  \n",
              "0                             1.0            NaN  \n",
              "1                             0.5           16.0  \n",
              "2                             0.2           64.0  \n",
              "3                             0.9          256.0  \n",
              "4                             0.9          512.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e3b7cbf-a2b9-417c-9d8c-34fee317d9b2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>evaluation_time</th>\n",
              "      <th>best</th>\n",
              "      <th>split_axis</th>\n",
              "      <th>sparse_oblique_projection_density_factor</th>\n",
              "      <th>sparse_oblique_normalization</th>\n",
              "      <th>sparse_oblique_weights</th>\n",
              "      <th>categorical_algorithm</th>\n",
              "      <th>growing_strategy</th>\n",
              "      <th>max_depth</th>\n",
              "      <th>sampling_method</th>\n",
              "      <th>subsample</th>\n",
              "      <th>shrinkage</th>\n",
              "      <th>min_examples</th>\n",
              "      <th>num_candidate_attributes_ratio</th>\n",
              "      <th>max_num_nodes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-8.218175</td>\n",
              "      <td>2467.965598</td>\n",
              "      <td>False</td>\n",
              "      <td>SPARSE_OBLIQUE</td>\n",
              "      <td>4.0</td>\n",
              "      <td>STANDARD_DEVIATION</td>\n",
              "      <td>CONTINUOUS</td>\n",
              "      <td>CART</td>\n",
              "      <td>LOCAL</td>\n",
              "      <td>3.0</td>\n",
              "      <td>RANDOM</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.05</td>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-8.143211</td>\n",
              "      <td>3189.592446</td>\n",
              "      <td>False</td>\n",
              "      <td>SPARSE_OBLIQUE</td>\n",
              "      <td>5.0</td>\n",
              "      <td>MIN_MAX</td>\n",
              "      <td>BINARY</td>\n",
              "      <td>CART</td>\n",
              "      <td>BEST_FIRST_GLOBAL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RANDOM</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.10</td>\n",
              "      <td>20</td>\n",
              "      <td>0.5</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-7.892285</td>\n",
              "      <td>4346.781670</td>\n",
              "      <td>False</td>\n",
              "      <td>SPARSE_OBLIQUE</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NONE</td>\n",
              "      <td>CONTINUOUS</td>\n",
              "      <td>CART</td>\n",
              "      <td>BEST_FIRST_GLOBAL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RANDOM</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.05</td>\n",
              "      <td>10</td>\n",
              "      <td>0.2</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-7.911223</td>\n",
              "      <td>4396.937751</td>\n",
              "      <td>False</td>\n",
              "      <td>SPARSE_OBLIQUE</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NONE</td>\n",
              "      <td>CONTINUOUS</td>\n",
              "      <td>RANDOM</td>\n",
              "      <td>BEST_FIRST_GLOBAL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RANDOM</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.10</td>\n",
              "      <td>10</td>\n",
              "      <td>0.9</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-7.945287</td>\n",
              "      <td>4816.981907</td>\n",
              "      <td>False</td>\n",
              "      <td>SPARSE_OBLIQUE</td>\n",
              "      <td>2.0</td>\n",
              "      <td>MIN_MAX</td>\n",
              "      <td>BINARY</td>\n",
              "      <td>RANDOM</td>\n",
              "      <td>BEST_FIRST_GLOBAL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RANDOM</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.05</td>\n",
              "      <td>7</td>\n",
              "      <td>0.9</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e3b7cbf-a2b9-417c-9d8c-34fee317d9b2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e3b7cbf-a2b9-417c-9d8c-34fee317d9b2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e3b7cbf-a2b9-417c-9d8c-34fee317d9b2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-182df2d7-3ef8-4835-b1cd-db8c3ddd2cc5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-182df2d7-3ef8-4835-b1cd-db8c3ddd2cc5')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-182df2d7-3ef8-4835-b1cd-db8c3ddd2cc5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuning_logs = tuned_model.make_inspector().tuning_logs()\n",
        "tuning_logs[tuning_logs.best].iloc[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T06:45:40.033099Z",
          "iopub.status.idle": "2023-11-04T06:45:40.033499Z",
          "shell.execute_reply.started": "2023-11-04T06:45:40.033315Z",
          "shell.execute_reply": "2023-11-04T06:45:40.033332Z"
        },
        "trusted": true,
        "id": "psXPtfAsreL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757cdf26-2e52-4592-90e2-89dce90426a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "score                                          -7.700172\n",
              "evaluation_time                              27488.98287\n",
              "best                                                True\n",
              "split_axis                                  AXIS_ALIGNED\n",
              "sparse_oblique_projection_density_factor             NaN\n",
              "sparse_oblique_normalization                         NaN\n",
              "sparse_oblique_weights                               NaN\n",
              "categorical_algorithm                               CART\n",
              "growing_strategy                                   LOCAL\n",
              "max_depth                                            8.0\n",
              "sampling_method                                   RANDOM\n",
              "subsample                                            0.9\n",
              "shrinkage                                            0.1\n",
              "min_examples                                          10\n",
              "num_candidate_attributes_ratio                       0.9\n",
              "max_num_nodes                                        NaN\n",
              "Name: 46, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(tuning_logs[\"score\"], label=\"current trial\")\n",
        "plt.plot(tuning_logs[\"score\"].cummax(), label=\"best trial\")\n",
        "plt.xlabel(\"Tuning step\")\n",
        "plt.ylabel(\"Tuning score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "PHbHNfELAgXV",
        "outputId": "4efe4c64-ba67-4ca2-be65-48a933017e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfdf.model_plotter.plot_model_in_colab(model=tuned_model, tree_idx=0, max_depth=8)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_58r2KDGreL9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78ca1a1c-71f4-478a-e9ce-aace5abc708a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
              "<div id=\"tree_plot_addce304f04f4734b0b0cff15cd8d42c\"></div>\n",
              "<script>\n",
              "/*\n",
              " * Copyright 2021 Google LLC.\n",
              " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              " * you may not use this file except in compliance with the License.\n",
              " * You may obtain a copy of the License at\n",
              " *\n",
              " *     https://www.apache.org/licenses/LICENSE-2.0\n",
              " *\n",
              " * Unless required by applicable law or agreed to in writing, software\n",
              " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              " * See the License for the specific language governing permissions and\n",
              " * limitations under the License.\n",
              " */\n",
              "\n",
              "/**\n",
              " *  Plotting of decision trees generated by TF-DF.\n",
              " *\n",
              " *  A tree is a recursive structure of node objects.\n",
              " *  A node contains one or more of the following components:\n",
              " *\n",
              " *    - A value: Representing the output of the node. If the node is not a leaf,\n",
              " *      the value is only present for analysis i.e. it is not used for\n",
              " *      predictions.\n",
              " *\n",
              " *    - A condition : For non-leaf nodes, the condition (also known as split)\n",
              " *      defines a binary test to branch to the positive or negative child.\n",
              " *\n",
              " *    - An explanation: Generally a plot showing the relation between the label\n",
              " *      and the condition to give insights about the effect of the condition.\n",
              " *\n",
              " *    - Two children : For non-leaf nodes, the children nodes. The first\n",
              " *      children (i.e. \"node.children[0]\") is the negative children (drawn in\n",
              " *      red). The second children is the positive one (drawn in green).\n",
              " *\n",
              " */\n",
              "\n",
              "/**\n",
              " * Plots a single decision tree into a DOM element.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!tree} raw_tree Recursive tree structure.\n",
              " * @param {string} canvas_id Id of the output dom element.\n",
              " */\n",
              "function display_tree(options, raw_tree, canvas_id) {\n",
              "  console.log(options);\n",
              "\n",
              "  // Determine the node placement.\n",
              "  const tree_struct = d3.tree().nodeSize(\n",
              "      [options.node_y_offset, options.node_x_offset])(d3.hierarchy(raw_tree));\n",
              "\n",
              "  // Boundaries of the node placement.\n",
              "  let x_min = Infinity;\n",
              "  let x_max = -x_min;\n",
              "  let y_min = Infinity;\n",
              "  let y_max = -x_min;\n",
              "\n",
              "  tree_struct.each(d => {\n",
              "    if (d.x > x_max) x_max = d.x;\n",
              "    if (d.x < x_min) x_min = d.x;\n",
              "    if (d.y > y_max) y_max = d.y;\n",
              "    if (d.y < y_min) y_min = d.y;\n",
              "  });\n",
              "\n",
              "  // Size of the plot.\n",
              "  const width = y_max - y_min + options.node_x_size + options.margin * 2;\n",
              "  const height = x_max - x_min + options.node_y_size + options.margin * 2 +\n",
              "      options.node_y_offset - options.node_y_size;\n",
              "\n",
              "  const plot = d3.select(canvas_id);\n",
              "\n",
              "  // Tool tip\n",
              "  options.tooltip = plot.append('div')\n",
              "                        .attr('width', 100)\n",
              "                        .attr('height', 100)\n",
              "                        .style('padding', '4px')\n",
              "                        .style('background', '#fff')\n",
              "                        .style('box-shadow', '4px 4px 0px rgba(0,0,0,0.1)')\n",
              "                        .style('border', '1px solid black')\n",
              "                        .style('font-family', 'sans-serif')\n",
              "                        .style('font-size', options.font_size)\n",
              "                        .style('position', 'absolute')\n",
              "                        .style('z-index', '10')\n",
              "                        .attr('pointer-events', 'none')\n",
              "                        .style('display', 'none');\n",
              "\n",
              "  // Create canvas\n",
              "  const svg = plot.append('svg').attr('width', width).attr('height', height);\n",
              "  const graph =\n",
              "      svg.style('overflow', 'visible')\n",
              "          .append('g')\n",
              "          .attr('font-family', 'sans-serif')\n",
              "          .attr('font-size', options.font_size)\n",
              "          .attr(\n",
              "              'transform',\n",
              "              () => `translate(${options.margin},${\n",
              "                  - x_min + options.node_y_offset / 2 + options.margin})`);\n",
              "\n",
              "  // Plot bounding box.\n",
              "  if (options.show_plot_bounding_box) {\n",
              "    svg.append('rect')\n",
              "        .attr('width', width)\n",
              "        .attr('height', height)\n",
              "        .attr('fill', 'none')\n",
              "        .attr('stroke-width', 1.0)\n",
              "        .attr('stroke', 'black');\n",
              "  }\n",
              "\n",
              "  // Draw the edges.\n",
              "  display_edges(options, graph, tree_struct);\n",
              "\n",
              "  // Draw the nodes.\n",
              "  display_nodes(options, graph, tree_struct);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Draw the nodes of the tree.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!graph} graph D3 search handle containing the graph.\n",
              " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
              " *     data, etc.).\n",
              " */\n",
              "function display_nodes(options, graph, tree_struct) {\n",
              "  const nodes = graph.append('g')\n",
              "                    .selectAll('g')\n",
              "                    .data(tree_struct.descendants())\n",
              "                    .join('g')\n",
              "                    .attr('transform', d => `translate(${d.y},${d.x})`);\n",
              "\n",
              "  nodes.append('rect')\n",
              "      .attr('x', 0.5)\n",
              "      .attr('y', 0.5)\n",
              "      .attr('width', options.node_x_size)\n",
              "      .attr('height', options.node_y_size)\n",
              "      .attr('stroke', 'lightgrey')\n",
              "      .attr('stroke-width', 1)\n",
              "      .attr('fill', 'white')\n",
              "      .attr('y', -options.node_y_size / 2);\n",
              "\n",
              "  // Brackets on the right of condition nodes without children.\n",
              "  non_leaf_node_without_children =\n",
              "      nodes.filter(node => node.data.condition != null && node.children == null)\n",
              "          .append('g')\n",
              "          .attr('transform', `translate(${options.node_x_size},0)`);\n",
              "\n",
              "  non_leaf_node_without_children.append('path')\n",
              "      .attr('d', 'M0,0 C 10,0 0,10 10,10')\n",
              "      .attr('fill', 'none')\n",
              "      .attr('stroke-width', 1.0)\n",
              "      .attr('stroke', '#F00');\n",
              "\n",
              "  non_leaf_node_without_children.append('path')\n",
              "      .attr('d', 'M0,0 C 10,0 0,-10 10,-10')\n",
              "      .attr('fill', 'none')\n",
              "      .attr('stroke-width', 1.0)\n",
              "      .attr('stroke', '#0F0');\n",
              "\n",
              "  const node_content = nodes.append('g').attr(\n",
              "      'transform',\n",
              "      `translate(0,${options.node_padding - options.node_y_size / 2})`);\n",
              "\n",
              "  node_content.append(node => create_node_element(options, node));\n",
              "}\n",
              "\n",
              "/**\n",
              " * Creates the D3 content for a single node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!node} node Node to draw.\n",
              " * @return {!d3} D3 content.\n",
              " */\n",
              "function create_node_element(options, node) {\n",
              "  // Output accumulator.\n",
              "  let output = {\n",
              "    // Content to draw.\n",
              "    content: d3.create('svg:g'),\n",
              "    // Vertical offset to the next element to draw.\n",
              "    vertical_offset: 0\n",
              "  };\n",
              "\n",
              "  // Conditions.\n",
              "  if (node.data.condition != null) {\n",
              "    display_condition(options, node.data.condition, output);\n",
              "  }\n",
              "\n",
              "  // Values.\n",
              "  if (node.data.value != null) {\n",
              "    display_value(options, node.data.value, output);\n",
              "  }\n",
              "\n",
              "  // Explanations.\n",
              "  if (node.data.explanation != null) {\n",
              "    display_explanation(options, node.data.explanation, output);\n",
              "  }\n",
              "\n",
              "  return output.content.node();\n",
              "}\n",
              "\n",
              "\n",
              "/**\n",
              " * Adds a single line of text inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {string} text Text to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_node_text(options, text, output) {\n",
              "  output.content.append('text')\n",
              "      .attr('x', options.node_padding)\n",
              "      .attr('y', output.vertical_offset)\n",
              "      .attr('alignment-baseline', 'hanging')\n",
              "      .text(text);\n",
              "  output.vertical_offset += 10;\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a single line of text inside of a node with a tooltip.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {string} text Text to display.\n",
              " * @param {string} tooltip Text in the Tooltip.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_node_text_with_tooltip(options, text, tooltip, output) {\n",
              "  const item = output.content.append('text')\n",
              "                   .attr('x', options.node_padding)\n",
              "                   .attr('alignment-baseline', 'hanging')\n",
              "                   .text(text);\n",
              "\n",
              "  add_tooltip(options, item, () => tooltip);\n",
              "  output.vertical_offset += 10;\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a tooltip to a dom element.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!dom} target Dom element to equip with a tooltip.\n",
              " * @param {!func} get_content Generates the html content of the tooltip.\n",
              " */\n",
              "function add_tooltip(options, target, get_content) {\n",
              "  function show(d) {\n",
              "    options.tooltip.style('display', 'block');\n",
              "    options.tooltip.html(get_content());\n",
              "  }\n",
              "\n",
              "  function hide(d) {\n",
              "    options.tooltip.style('display', 'none');\n",
              "  }\n",
              "\n",
              "  function move(d) {\n",
              "    options.tooltip.style('display', 'block');\n",
              "    options.tooltip.style('left', (d.pageX + 5) + 'px');\n",
              "    options.tooltip.style('top', d.pageY + 'px');\n",
              "  }\n",
              "\n",
              "  target.on('mouseover', show);\n",
              "  target.on('mouseout', hide);\n",
              "  target.on('mousemove', move);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a condition inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!condition} condition Condition to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_condition(options, condition, output) {\n",
              "  threshold_format = d3.format('r');\n",
              "\n",
              "  if (condition.type === 'IS_MISSING') {\n",
              "    display_node_text(options, `${condition.attribute} is missing`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'IS_TRUE') {\n",
              "    display_node_text(options, `${condition.attribute} is true`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'NUMERICAL_IS_HIGHER_THAN') {\n",
              "    format = d3.format('r');\n",
              "    display_node_text(\n",
              "        options,\n",
              "        `${condition.attribute} >= ${threshold_format(condition.threshold)}`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'CATEGORICAL_IS_IN') {\n",
              "    display_node_text_with_tooltip(\n",
              "        options, `${condition.attribute} in [...]`,\n",
              "        `${condition.attribute} in [${condition.mask}]`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'CATEGORICAL_SET_CONTAINS') {\n",
              "    display_node_text_with_tooltip(\n",
              "        options, `${condition.attribute} intersect [...]`,\n",
              "        `${condition.attribute} intersect [${condition.mask}]`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'NUMERICAL_SPARSE_OBLIQUE') {\n",
              "    display_node_text_with_tooltip(\n",
              "        options, `Sparse oblique split...`,\n",
              "        `[${condition.attributes}]*[${condition.weights}]>=${\n",
              "            threshold_format(condition.threshold)}`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  display_node_text(\n",
              "      options, `Non supported condition ${condition.type}`, output);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a value inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!value} value Value to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_value(options, value, output) {\n",
              "  if (value.type === 'PROBABILITY') {\n",
              "    const left_margin = 0;\n",
              "    const right_margin = 50;\n",
              "    const plot_width = options.node_x_size - options.node_padding * 2 -\n",
              "        left_margin - right_margin;\n",
              "\n",
              "    let cusum = Array.from(d3.cumsum(value.distribution));\n",
              "    cusum.unshift(0);\n",
              "    const distribution_plot = output.content.append('g').attr(\n",
              "        'transform', `translate(0,${output.vertical_offset + 0.5})`);\n",
              "\n",
              "    distribution_plot.selectAll('rect')\n",
              "        .data(value.distribution)\n",
              "        .join('rect')\n",
              "        .attr('height', 10)\n",
              "        .attr(\n",
              "            'x',\n",
              "            (d, i) =>\n",
              "                (cusum[i] * plot_width + left_margin + options.node_padding))\n",
              "        .attr('width', (d, i) => d * plot_width)\n",
              "        .style('fill', (d, i) => d3.schemeSet1[i]);\n",
              "\n",
              "    const num_examples =\n",
              "        output.content.append('g')\n",
              "            .attr('transform', `translate(0,${output.vertical_offset})`)\n",
              "            .append('text')\n",
              "            .attr('x', options.node_x_size - options.node_padding)\n",
              "            .attr('alignment-baseline', 'hanging')\n",
              "            .attr('text-anchor', 'end')\n",
              "            .text(`(${value.num_examples})`);\n",
              "\n",
              "    const distribution_details = d3.create('ul');\n",
              "    distribution_details.selectAll('li')\n",
              "        .data(value.distribution)\n",
              "        .join('li')\n",
              "        .append('span')\n",
              "        .text(\n",
              "            (d, i) =>\n",
              "                'class ' + i + ': ' + d3.format('.3%')(value.distribution[i]));\n",
              "\n",
              "    add_tooltip(options, distribution_plot, () => distribution_details.html());\n",
              "    add_tooltip(options, num_examples, () => 'Number of examples');\n",
              "\n",
              "    output.vertical_offset += 10;\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (value.type === 'REGRESSION') {\n",
              "    display_node_text(\n",
              "        options,\n",
              "        'value: ' + d3.format('r')(value.value) + ` (` +\n",
              "            d3.format('.6')(value.num_examples) + `)`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (value.type === 'UPLIFT') {\n",
              "    display_node_text(\n",
              "        options,\n",
              "        'effect: ' + d3.format('r')(value.treatment_effect) + ` (` +\n",
              "            d3.format('.6')(value.num_examples) + `)`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  display_node_text(options, `Non supported value ${value.type}`, output);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds an explanation inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!explanation} explanation Explanation to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_explanation(options, explanation, output) {\n",
              "  // Margin before the explanation.\n",
              "  output.vertical_offset += 10;\n",
              "\n",
              "  display_node_text(\n",
              "      options, `Non supported explanation ${explanation.type}`, output);\n",
              "}\n",
              "\n",
              "\n",
              "/**\n",
              " * Draw the edges of the tree.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!graph} graph D3 search handle containing the graph.\n",
              " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
              " *     data, etc.).\n",
              " */\n",
              "function display_edges(options, graph, tree_struct) {\n",
              "  // Draw an edge between a parent and a child node with a bezier.\n",
              "  function draw_single_edge(d) {\n",
              "    return 'M' + (d.source.y + options.node_x_size) + ',' + d.source.x + ' C' +\n",
              "        (d.source.y + options.node_x_size + options.edge_rounding) + ',' +\n",
              "        d.source.x + ' ' + (d.target.y - options.edge_rounding) + ',' +\n",
              "        d.target.x + ' ' + d.target.y + ',' + d.target.x;\n",
              "  }\n",
              "\n",
              "  graph.append('g')\n",
              "      .attr('fill', 'none')\n",
              "      .attr('stroke-width', 1.2)\n",
              "      .selectAll('path')\n",
              "      .data(tree_struct.links())\n",
              "      .join('path')\n",
              "      .attr('d', draw_single_edge)\n",
              "      .attr(\n",
              "          'stroke', d => (d.target === d.source.children[0]) ? '#0F0' : '#F00');\n",
              "}\n",
              "\n",
              "display_tree({\"margin\": 10, \"node_x_size\": 160, \"node_y_size\": 28, \"node_x_offset\": 180, \"node_y_offset\": 33, \"font_size\": 10, \"edge_rounding\": 20, \"node_padding\": 2, \"show_plot_bounding_box\": false}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.0013848933158442378, \"num_examples\": 32355.0, \"standard_deviation\": 8.645177062142574}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.9\", \"threshold\": 1.008579969406128}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -2.4682846069335938, \"num_examples\": 70.0, \"standard_deviation\": 29.777791918625677}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.0\", \"threshold\": 210.0}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -1.6186740398406982, \"num_examples\": 55.0, \"standard_deviation\": 20.834364476582234}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.36\", \"threshold\": 0.1181073784828186}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -2.0824944972991943, \"num_examples\": 45.0, \"standard_deviation\": 19.341564064542684}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.1\", \"threshold\": -0.5}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -2.562410354614258, \"num_examples\": 35.0, \"standard_deviation\": 18.34512104119465}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.5\", \"threshold\": 8494.544921875}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -2.0177013874053955, \"num_examples\": 25.0, \"standard_deviation\": 15.034265501187116}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.20\", \"threshold\": -0.0006475540576502681}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -2.8681817054748535, \"num_examples\": 15.0, \"standard_deviation\": 9.018024279614561}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.7419806718826294, \"num_examples\": 10.0, \"standard_deviation\": 13.108846630929273}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -3.9241833686828613, \"num_examples\": 10.0, \"standard_deviation\": 18.79387500592713}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.40278851985931396, \"num_examples\": 10.0, \"standard_deviation\": 11.948947588290595}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.4685181677341461, \"num_examples\": 10.0, \"standard_deviation\": 13.096231715534385}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -5.0, \"num_examples\": 15.0, \"standard_deviation\": 36.2115234942448}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.003963812720030546, \"num_examples\": 32285.0, \"standard_deviation\": 8.4650003979147}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.27\", \"threshold\": -0.00032748395460657775}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.02207721397280693, \"num_examples\": 30121.0, \"standard_deviation\": 7.838944264217658}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.28\", \"threshold\": 0.0003593452274799347}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.38166406750679016, \"num_examples\": 1379.0, \"standard_deviation\": 13.254380591543391}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.6\", \"threshold\": 0.9932390451431274}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.3599463403224945, \"num_examples\": 1365.0, \"standard_deviation\": 12.883062323484198}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.3\", \"threshold\": 66631384.0}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 1.3815852403640747, \"num_examples\": 16.0, \"standard_deviation\": 20.37188904663036}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.38060200214385986, \"num_examples\": 1349.0, \"standard_deviation\": 12.624563286252233}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.13\", \"threshold\": 0.3843581974506378}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -1.3047806024551392, \"num_examples\": 39.0, \"standard_deviation\": 12.420794110931874}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.6\", \"threshold\": 1.0013000965118408}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -2.234548807144165, \"num_examples\": 13.0, \"standard_deviation\": 13.820148109165116}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.8398964405059814, \"num_examples\": 26.0, \"standard_deviation\": 8.430940747333846}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.35308828949928284, \"num_examples\": 1310.0, \"standard_deviation\": 12.526494911621898}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.19\", \"threshold\": 0.0003698158834595233}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.7600439190864563, \"num_examples\": 175.0, \"standard_deviation\": 13.217944287604455}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.2903418242931366, \"num_examples\": 1135.0, \"standard_deviation\": 12.29720359230973}}]}]}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -2.4991445541381836, \"num_examples\": 14.0, \"standard_deviation\": 25.864557773763817}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.004824751988053322, \"num_examples\": 28742.0, \"standard_deviation\": 7.4376297273890914}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.12\", \"threshold\": -0.1599915325641632}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.05484021082520485, \"num_examples\": 17287.0, \"standard_deviation\": 7.385230055474086}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.24\", \"threshold\": 7.902888319222257e-05}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.14811772108078003, \"num_examples\": 3804.0, \"standard_deviation\": 8.584243502037905}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.9\", \"threshold\": 0.991724967956543}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.1523807942867279, \"num_examples\": 3794.0, \"standard_deviation\": 8.469249812392558}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.5\", \"threshold\": 10994.0}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.08299374580383301, \"num_examples\": 1891.0, \"standard_deviation\": 8.822843409592112}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.22133029997348785, \"num_examples\": 1903.0, \"standard_deviation\": 8.043540982910557}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 1.4692920446395874, \"num_examples\": 10.0, \"standard_deviation\": 23.567316944727434}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.02852354198694229, \"num_examples\": 13483.0, \"standard_deviation\": 6.987468513962677}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.0\", \"threshold\": 505.0}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.1332019418478012, \"num_examples\": 976.0, \"standard_deviation\": 8.421187531806975}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.10\", \"threshold\": 0.997186541557312}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.10341963917016983, \"num_examples\": 915.0, \"standard_deviation\": 7.683896151266496}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.5799364447593689, \"num_examples\": 61.0, \"standard_deviation\": 15.091084852965366}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.041143998503685, \"num_examples\": 12507.0, \"standard_deviation\": 6.846950450155795}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.12\", \"threshold\": 0.580353319644928}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.09294479340314865, \"num_examples\": 3631.0, \"standard_deviation\": 6.770751762882548}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.019953297451138496, \"num_examples\": 8876.0, \"standard_deviation\": 6.866625033590645}}]}]}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.07065470516681671, \"num_examples\": 11455.0, \"standard_deviation\": 7.452735222028742}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.10\", \"threshold\": 0.9924114942550659}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.06795703619718552, \"num_examples\": 11437.0, \"standard_deviation\": 7.37625146372866}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.20\", \"threshold\": -0.00010289571946486831}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.03475556895136833, \"num_examples\": 7766.0, \"standard_deviation\": 7.01517550034598}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.12\", \"threshold\": -0.7895114421844482}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.004659893456846476, \"num_examples\": 5647.0, \"standard_deviation\": 6.9531307889941285}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.11495862901210785, \"num_examples\": 2119.0, \"standard_deviation\": 7.1160136437277695}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.138194739818573, \"num_examples\": 3671.0, \"standard_deviation\": 8.042109964656278}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.9\", \"threshold\": 0.9969489574432373}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.14737462997436523, \"num_examples\": 3595.0, \"standard_deviation\": 7.860746848812481}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.2960380017757416, \"num_examples\": 76.0, \"standard_deviation\": 13.485212684042532}}]}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 1.7847238779067993, \"num_examples\": 18.0, \"standard_deviation\": 21.95269480826171}}]}]}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.3664322793483734, \"num_examples\": 2164.0, \"standard_deviation\": 14.1297249998161}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.27\", \"threshold\": -0.0007063921075314283}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.2735421657562256, \"num_examples\": 1732.0, \"standard_deviation\": 13.457026152738775}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.23\", \"threshold\": -0.0014428021386265755}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.29854533076286316, \"num_examples\": 1691.0, \"standard_deviation\": 13.38185687209453}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.36\", \"threshold\": 12330855301120.0}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.5322080850601196, \"num_examples\": 54.0, \"standard_deviation\": 15.705528788987902}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.11\", \"threshold\": 0.9959070086479187}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.18482814729213715, \"num_examples\": 44.0, \"standard_deviation\": 12.98435134760481}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.4\", \"threshold\": 870.989990234375}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.6434105038642883, \"num_examples\": 27.0, \"standard_deviation\": 13.34951596206004}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.5435085296630859, \"num_examples\": 17.0, \"standard_deviation\": 8.177587504354829}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -2.0606799125671387, \"num_examples\": 10.0, \"standard_deviation\": 17.41993459300738}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.3259495198726654, \"num_examples\": 1637.0, \"standard_deviation\": 13.209572014873052}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.6\", \"threshold\": 1.0048189163208008}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 1.0315158367156982, \"num_examples\": 60.0, \"standard_deviation\": 20.10095695872055}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.44\", \"threshold\": -1.000773549079895}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.6987529993057251, \"num_examples\": 50.0, \"standard_deviation\": 14.94983265526135}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 2.6953296661376953, \"num_examples\": 10.0, \"standard_deviation\": 31.21874685975538}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.2991048991680145, \"num_examples\": 1577.0, \"standard_deviation\": 12.798155167872439}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.13\", \"threshold\": 0.29904329776763916}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.9863770008087158, \"num_examples\": 81.0, \"standard_deviation\": 14.931104316597695}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.26189297437667847, \"num_examples\": 1496.0, \"standard_deviation\": 12.56560608413082}}]}]}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.7576851844787598, \"num_examples\": 41.0, \"standard_deviation\": 12.46507439345211}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.9\", \"threshold\": 1.0026044845581055}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -1.4532428979873657, \"num_examples\": 15.0, \"standard_deviation\": 11.321780246761508}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.35640186071395874, \"num_examples\": 26.0, \"standard_deviation\": 11.271792974431614}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.6\", \"threshold\": 0.9997569918632507}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.19220294058322906, \"num_examples\": 10.0, \"standard_deviation\": 8.772067583092237}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.6992799043655396, \"num_examples\": 16.0, \"standard_deviation\": 11.304929195739328}}]}]}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.738852858543396, \"num_examples\": 432.0, \"standard_deviation\": 16.022550854567527}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.6\", \"threshold\": 1.0047199726104736}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.31358852982521057, \"num_examples\": 29.0, \"standard_deviation\": 21.99650707698825}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.47\", \"threshold\": 0.6306324005126953}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.92876136302948, \"num_examples\": 16.0, \"standard_deviation\": 14.94306135274901}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -1.8426345586776733, \"num_examples\": 13.0, \"standard_deviation\": 19.513215444375838}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.8145868182182312, \"num_examples\": 403.0, \"standard_deviation\": 15.226098945488356}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.24\", \"threshold\": -0.000977422227151692}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.7346378564834595, \"num_examples\": 359.0, \"standard_deviation\": 15.423597041922903}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.3\", \"threshold\": 13941409.0}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 1.554546594619751, \"num_examples\": 30.0, \"standard_deviation\": 22.996866328196557}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.36\", \"threshold\": 3.3175978660583496}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 3.1671302318573, \"num_examples\": 10.0, \"standard_deviation\": 30.52665520084309}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.7482547163963318, \"num_examples\": 20.0, \"standard_deviation\": 11.502759967245485}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.6598741412162781, \"num_examples\": 329.0, \"standard_deviation\": 14.306165087313321}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.10\", \"threshold\": 0.9979659914970398}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.8100326657295227, \"num_examples\": 240.0, \"standard_deviation\": 14.016247068660032}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.2549523115158081, \"num_examples\": 89.0, \"standard_deviation\": 14.294490039422483}}]}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 1.4668978452682495, \"num_examples\": 44.0, \"standard_deviation\": 11.605123504828695}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.5\", \"threshold\": 57431.69921875}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.9826009273529053, \"num_examples\": 18.0, \"standard_deviation\": 10.47180448927837}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 1.802180290222168, \"num_examples\": 26.0, \"standard_deviation\": 11.158920631533034}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"data:0.45\", \"threshold\": 16.83568572998047}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 1.238054871559143, \"num_examples\": 12.0, \"standard_deviation\": 8.892890720241025}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 2.2857165336608887, \"num_examples\": 14.0, \"standard_deviation\": 10.621212259300922}}]}]}]}]}]}]}]}, \"#tree_plot_addce304f04f4734b0b0cff15cd8d42c\")\n",
              "</script>\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import optiver2023\n",
        "env = optiver2023.make_env()\n",
        "iter_test = env.iter_test()\n",
        "'''"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T06:45:40.035427Z",
          "iopub.status.idle": "2023-11-04T06:45:40.035755Z",
          "shell.execute_reply.started": "2023-11-04T06:45:40.035594Z",
          "shell.execute_reply": "2023-11-04T06:45:40.035609Z"
        },
        "trusted": true,
        "id": "U40-7m79reL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "counter = 0\n",
        "for (test, revealed_targets, sample_prediction) in iter_test:\n",
        "    if counter == 0:\n",
        "        print(test.head(3))\n",
        "        print(revealed_targets.head(3))\n",
        "        print(sample_prediction.head(3))\n",
        "    sample_prediction['target'] = model.predict(test)\n",
        "    env.predict(sample_prediction)\n",
        "    counter += 1\n",
        "'''"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T06:45:40.036790Z",
          "iopub.status.idle": "2023-11-04T06:45:40.037136Z",
          "shell.execute_reply.started": "2023-11-04T06:45:40.036950Z",
          "shell.execute_reply": "2023-11-04T06:45:40.036965Z"
        },
        "trusted": true,
        "id": "hRvm6nMSreL-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}